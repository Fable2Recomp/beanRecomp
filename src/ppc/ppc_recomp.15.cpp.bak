#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_8219B0D0"))) PPC_WEAK_FUNC(sub_8219B0D0);
PPC_FUNC_IMPL(__imp__sub_8219B0D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// clrlwi r11,r10,28
	r11.u64 = ctx.r10.u32 & 0xF;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x8219b12c
	if (!cr6.getEQ()) goto loc_8219B12C;
	// lwz r10,48(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// rlwinm r10,r10,23,30,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 23) & 0x3;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// bne cr6,0x8219b0f8
	if (!cr6.getEQ()) goto loc_8219B0F8;
	// li r11,18
	r11.s64 = 18;
	// b 0x8219b154
	goto loc_8219B154;
loc_8219B0F8:
	// cmplwi cr6,r10,2
	cr6.compare<uint32_t>(ctx.r10.u32, 2, xer);
	// bne cr6,0x8219b108
	if (!cr6.getEQ()) goto loc_8219B108;
	// li r11,17
	r11.s64 = 17;
	// b 0x8219b154
	goto loc_8219B154;
loc_8219B108:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8219b118
	if (!cr6.getEQ()) goto loc_8219B118;
	// li r11,20
	r11.s64 = 20;
	// b 0x8219b154
	goto loc_8219B154;
loc_8219B118:
	// lwz r10,32(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// rlwinm. r10,r10,0,21,21
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x400;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8219b154
	if (cr0.getEQ()) goto loc_8219B154;
	// li r11,19
	r11.s64 = 19;
	// b 0x8219b154
	goto loc_8219B154;
loc_8219B12C:
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bne cr6,0x8219b154
	if (!cr6.getEQ()) goto loc_8219B154;
	// rlwinm. r10,r10,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8219b154
	if (cr0.getEQ()) goto loc_8219B154;
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,48(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// rlwinm r10,r10,0,21,22
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x600;
	// cmplwi cr6,r10,1024
	cr6.compare<uint32_t>(ctx.r10.u32, 1024, xer);
	// bne cr6,0x8219b154
	if (!cr6.getEQ()) goto loc_8219B154;
	// li r11,16
	r11.s64 = 16;
loc_8219B154:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219B160"))) PPC_WEAK_FUNC(sub_8219B160);
PPC_FUNC_IMPL(__imp__sub_8219B160) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r10,1768(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 1768);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// clrlwi r10,r11,28
	ctx.r10.u64 = r11.u32 & 0xF;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bgt cr6,0x8219b330
	if (cr6.getGT()) goto loc_8219B330;
	// lis r12,-32254
	r12.s64 = -2113798144;
	// addi r12,r12,-17608
	r12.s64 = r12.s64 + -17608;
	// lbzx r0,r12,r10
	r0.u64 = PPC_LOAD_U8(r12.u32 + ctx.r10.u32);
	// rlwinm r0,r0,2,0,29
	r0.u64 = __builtin_rotateleft64(r0.u32 | (r0.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r12,-32230
	r12.s64 = -2112225280;
	// addi r12,r12,-20040
	r12.s64 = r12.s64 + -20040;
	// add r12,r12,r0
	r12.u64 = r12.u64 + r0.u64;
	// mtctr r12
	ctr.u64 = r12.u64;
	// nop 
	// bctr 
	switch (ctx.r10.u64) {
	case 0:
		goto loc_8219B248;
	case 1:
		goto loc_8219B278;
	case 2:
		goto loc_8219B1FC;
	case 3:
		goto loc_8219B1B8;
	case 4:
		goto loc_8219B330;
	case 5:
		goto loc_8219B2A4;
	case 6:
		goto loc_8219B2D0;
	case 7:
		goto loc_8219B2EC;
	case 8:
		goto loc_8219B308;
	default:
		__builtin_unreachable();
	}
loc_8219B1B8:
	// rlwinm. r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x8219b330
	if (!cr0.getEQ()) goto loc_8219B330;
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219b1e0
	if (cr0.getEQ()) goto loc_8219B1E0;
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// li r9,5120
	ctx.r9.s64 = 5120;
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// divwu r4,r11,r9
	ctx.r4.u32 = r11.u32 / ctx.r9.u32;
	// clrlwi r3,r10,20
	ctx.r3.u64 = ctx.r10.u32 & 0xFFF;
	// bl 0x8219c0f8
	sub_8219C0F8(ctx, base);
loc_8219B1E0:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219b330
	if (cr0.getEQ()) goto loc_8219B330;
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// li r11,0
	r11.s64 = 0;
	// stw r11,-31440(r10)
	PPC_STORE_U32(ctx.r10.u32 + -31440, r11.u32);
	// b 0x8219b330
	goto loc_8219B330;
loc_8219B1FC:
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8219b21c
	if (cr0.getEQ()) goto loc_8219B21C;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8219b21c
	if (cr6.getEQ()) goto loc_8219B21C;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,13
	ctx.r5.s64 = 13;
	// bl 0x8219d060
	sub_8219D060(ctx, base);
loc_8219B21C:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lis r29,-20096
	r29.s64 = -1317011456;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rlwinm r3,r11,0,0,19
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFF000;
	// rlwinm r30,r10,0,0,19
	r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFF000;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_8219B240:
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// b 0x8219b330
	goto loc_8219B330;
loc_8219B248:
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8219b268
	if (cr0.getEQ()) goto loc_8219B268;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8219b268
	if (cr6.getEQ()) goto loc_8219B268;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,9
	ctx.r5.s64 = 9;
	// bl 0x8219d060
	sub_8219D060(ctx, base);
loc_8219B268:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lis r4,-20096
	ctx.r4.s64 = -1317011456;
	// rlwinm r3,r11,0,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// b 0x8219b240
	goto loc_8219B240;
loc_8219B278:
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8219b298
	if (cr0.getEQ()) goto loc_8219B298;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8219b298
	if (cr6.getEQ()) goto loc_8219B298;
	// li r5,11
	ctx.r5.s64 = 11;
loc_8219B290:
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// bl 0x8219d060
	sub_8219D060(ctx, base);
loc_8219B298:
	// lwz r3,24(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lis r4,-20096
	ctx.r4.s64 = -1317011456;
	// b 0x8219b240
	goto loc_8219B240;
loc_8219B2A4:
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8219b2c4
	if (cr0.getEQ()) goto loc_8219B2C4;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8219b2c4
	if (cr6.getEQ()) goto loc_8219B2C4;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,7
	ctx.r5.s64 = 7;
	// bl 0x8219d060
	sub_8219D060(ctx, base);
loc_8219B2C4:
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lis r4,-20096
	ctx.r4.s64 = -1317011456;
	// b 0x8219b240
	goto loc_8219B240;
loc_8219B2D0:
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8219b298
	if (cr0.getEQ()) goto loc_8219B298;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8219b298
	if (cr6.getEQ()) goto loc_8219B298;
	// li r5,8
	ctx.r5.s64 = 8;
	// b 0x8219b290
	goto loc_8219B290;
loc_8219B2EC:
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8219b298
	if (cr0.getEQ()) goto loc_8219B298;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8219b298
	if (cr6.getEQ()) goto loc_8219B298;
	// li r5,17
	ctx.r5.s64 = 17;
	// b 0x8219b290
	goto loc_8219B290;
loc_8219B308:
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8219b328
	if (cr0.getEQ()) goto loc_8219B328;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x8219b328
	if (cr6.getEQ()) goto loc_8219B328;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,15
	ctx.r5.s64 = 15;
	// bl 0x8219d060
	sub_8219D060(ctx, base);
loc_8219B328:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219bd60
	sub_8219BD60(ctx, base);
loc_8219B330:
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219B348"))) PPC_WEAK_FUNC(sub_8219B348);
PPC_FUNC_IMPL(__imp__sub_8219B348) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed120
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r26,r9
	r26.u64 = ctx.r9.u64;
	// mr r25,r10
	r25.u64 = ctx.r10.u64;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// mr r23,r6
	r23.u64 = ctx.r6.u64;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// clrlwi r9,r11,28
	ctx.r9.u64 = r11.u32 & 0xF;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// cmplwi cr6,r9,4
	cr6.compare<uint32_t>(ctx.r9.u32, 4, xer);
	// bne cr6,0x8219b39c
	if (!cr6.getEQ()) goto loc_8219B39C;
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219b39c
	if (cr0.getEQ()) goto loc_8219B39C;
	// lwz r11,24(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 24);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8219b39c
	if (cr0.getEQ()) goto loc_8219B39C;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_8219B39C:
	// andi. r11,r25,4112
	r11.u64 = r25.u64 & 4112;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8219b3b0
	if (cr0.getEQ()) goto loc_8219B3B0;
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// b 0x8219b3b4
	goto loc_8219B3B4;
loc_8219B3B0:
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
loc_8219B3B4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lis r31,-32256
	r31.s64 = -2113929216;
	// beq cr6,0x8219b3d8
	if (cr6.getEQ()) goto loc_8219B3D8;
	// lwz r10,1768(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// lwz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// bl 0x8219d060
	sub_8219D060(ctx, base);
loc_8219B3D8:
	// andi. r11,r25,18
	r11.u64 = r25.u64 & 18;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x8219b4b0
	if (!cr0.getEQ()) goto loc_8219B4B0;
	// lwz r11,1768(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1768);
	// rlwinm r9,r28,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r28,3
	ctx.r10.u64 = r28.u32 & 0x1FFFFFFF;
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// add r29,r30,r26
	r29.u64 = r30.u64 + r26.u64;
	// bl 0x8235eaa8
	sub_8235EAA8(ctx, base);
	// lwz r11,10888(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10888);
	// cmplw cr6,r11,r3
	cr6.compare<uint32_t>(r11.u32, ctx.r3.u32, xer);
	// beq cr6,0x8219b428
	if (cr6.getEQ()) goto loc_8219B428;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r31,11816
	ctx.r3.s64 = r31.s64 + 11816;
	// bl 0x8219c570
	sub_8219C570(ctx, base);
	// b 0x8219b4b0
	goto loc_8219B4B0;
loc_8219B428:
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8219b444
	if (!cr6.getGT()) goto loc_8219B444;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_8219B444:
	// li r9,2609
	ctx.r9.s64 = 2609;
	// lwz r8,260(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lis r7,1
	ctx.r7.s64 = 65536;
	// addi r6,r29,4095
	ctx.r6.s64 = r29.s64 + 4095;
	// ori r7,r7,2607
	ctx.r7.u64 = ctx.r7.u64 | 2607;
	// rlwinm r10,r30,0,0,19
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFF000;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// rlwinm r9,r6,0,0,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFF000;
	// lis r6,-16380
	ctx.r6.s64 = -1073479680;
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// ori r6,r6,15360
	ctx.r6.u64 = ctx.r6.u64 | 15360;
	// li r5,3
	ctx.r5.s64 = 3;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// li r4,2609
	ctx.r4.s64 = 2609;
	// li r3,0
	ctx.r3.s64 = 0;
	// lis r30,-32768
	r30.s64 = -2147483648;
	// li r29,8
	r29.s64 = 8;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r6,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	r11.u32 = ea;
	// stwu r5,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	r11.u32 = ea;
	// stwu r4,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	r11.u32 = ea;
	// stwu r3,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	r11.u32 = ea;
	// stwu r30,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r30.u32);
	r11.u32 = ea;
	// stwu r29,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r29.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
loc_8219B4B0:
	// rlwinm. r6,r25,0,27,27
	ctx.r6.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne 0x8219b4c4
	if (!cr0.getEQ()) goto loc_8219B4C4;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm. r11,r11,0,10,10
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x200000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219b550
	if (cr0.getEQ()) goto loc_8219B550;
loc_8219B4C4:
	// clrlwi. r11,r25,31
	r11.u64 = r25.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8219b530
	if (!cr0.getEQ()) goto loc_8219B530;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x8219b4e8
	if (cr6.getEQ()) goto loc_8219B4E8;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x8219b4e8
	if (cr6.getEQ()) goto loc_8219B4E8;
	// subf r11,r24,r28
	r11.s64 = r28.s64 - r24.s64;
	// addi r8,r27,24
	ctx.r8.s64 = r27.s64 + 24;
	// b 0x8219b4f0
	goto loc_8219B4F0;
loc_8219B4E8:
	// subf r11,r23,r28
	r11.s64 = r28.s64 - r23.s64;
	// addi r8,r27,20
	ctx.r8.s64 = r27.s64 + 20;
loc_8219B4F0:
	// add r9,r11,r26
	ctx.r9.u64 = r11.u64 + r26.u64;
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// rlwinm r7,r11,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1FFFFFF;
	// addi r11,r9,127
	r11.s64 = ctx.r9.s64 + 127;
	// clrlwi r9,r10,16
	ctx.r9.u64 = ctx.r10.u32 & 0xFFFF;
	// rlwinm r11,r11,25,7,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 25) & 0x1FFFFFF;
	// rlwinm r10,r10,16,16,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bgt cr6,0x8219b518
	if (cr6.getGT()) goto loc_8219B518;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_8219B518:
	// cmplw cr6,r7,r10
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r10.u32, xer);
	// bge cr6,0x8219b524
	if (!cr6.getLT()) goto loc_8219B524;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
loc_8219B524:
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
loc_8219B530:
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x8219b550
	if (cr6.getEQ()) goto loc_8219B550;
	// rlwinm r11,r28,12,20,31
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r28,3
	ctx.r10.u64 = r28.u32 & 0x1FFFFFFF;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addis r28,r11,-16384
	r28.s64 = r11.s64 + -1073741824;
loc_8219B550:
	// li r11,256
	r11.s64 = 256;
loc_8219B554:
	// mfmsr r8
	// mtmsrd r13,1
	// lwarx r10,0,r27
	reserved.u32 = *(uint32_t*)(base + r27.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// add r9,r11,r10
	ctx.r9.u64 = r11.u64 + ctx.r10.u64;
	// stwcx. r9,0,r27
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r27.u32), reserved.s32, __builtin_bswap32(ctx.r9.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r8,1
	// bne 0x8219b554
	if (!cr0.getEQ()) goto loc_8219B554;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x823ed170
	return;
}

__attribute__((alias("__imp__sub_8219B580"))) PPC_WEAK_FUNC(sub_8219B580);
PPC_FUNC_IMPL(__imp__sub_8219B580) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// li r10,-256
	ctx.r10.s64 = -256;
loc_8219B59C:
	// mfmsr r7
	// mtmsrd r13,1
	// lwarx r9,0,r31
	reserved.u32 = *(uint32_t*)(base + r31.u32);
	ctx.r9.u64 = __builtin_bswap32(reserved.u32);
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwcx. r8,0,r31
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r31.u32), reserved.s32, __builtin_bswap32(ctx.r8.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r7,1
	// bne 0x8219b59c
	if (!cr0.getEQ()) goto loc_8219B59C;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// rlwinm r10,r10,0,20,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xF00;
	// cmplwi cr6,r10,256
	cr6.compare<uint32_t>(ctx.r10.u32, 256, xer);
	// bne cr6,0x8219b680
	if (!cr6.getEQ()) goto loc_8219B680;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lis r30,-1
	r30.s64 = -65536;
	// lis r28,16384
	r28.s64 = 1073741824;
	// cmplw cr6,r10,r30
	cr6.compare<uint32_t>(ctx.r10.u32, r30.u32, xer);
	// beq cr6,0x8219b624
	if (cr6.getEQ()) goto loc_8219B624;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r8,r10,16,16,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF;
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
	// clrlwi r7,r10,16
	ctx.r7.u64 = ctx.r10.u32 & 0xFFFF;
	// rlwinm. r9,r9,0,10,10
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x200000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x8219b60c
	if (!cr0.getEQ()) goto loc_8219B60C;
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
loc_8219B60C:
	// rlwinm r9,r7,7,0,24
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 7) & 0xFFFFFF80;
	// rlwinm r10,r8,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
	// li r5,0
	ctx.r5.s64 = 0;
	// add r4,r9,r11
	ctx.r4.u64 = ctx.r9.u64 + r11.u64;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
loc_8219B624:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x8219b680
	if (cr6.getEQ()) goto loc_8219B680;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// beq cr6,0x8219b680
	if (cr6.getEQ()) goto loc_8219B680;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r9,r11,16,16,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF;
	// stw r30,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r30.u32);
	// clrlwi r8,r11,16
	ctx.r8.u64 = r11.u32 & 0xFFFF;
	// rlwinm. r10,r10,0,10,10
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x200000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x8219b668
	if (!cr0.getEQ()) goto loc_8219B668;
	// rlwinm r11,r29,12,20,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r29,3
	ctx.r10.u64 = r29.u32 & 0x1FFFFFFF;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r29,r28,r11
	r29.s64 = r11.s64 - r28.s64;
loc_8219B668:
	// rlwinm r10,r8,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
	// rlwinm r11,r9,7,0,24
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
	// li r5,0
	ctx.r5.s64 = 0;
	// add r4,r10,r29
	ctx.r4.u64 = ctx.r10.u64 + r29.u64;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
loc_8219B680:
	// sync 
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_8219B690"))) PPC_WEAK_FUNC(sub_8219B690);
PPC_FUNC_IMPL(__imp__sub_8219B690) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
loc_8219B6A4:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r9,1
	// bne 0x8219b6a4
	if (!cr0.getEQ()) goto loc_8219B6A4;
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x8219b6ec
	if (!cr6.getEQ()) goto loc_8219B6EC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// clrlwi r10,r11,28
	ctx.r10.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// bne cr6,0x8219b6ec
	if (!cr6.getEQ()) goto loc_8219B6EC;
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219b6ec
	if (cr0.getEQ()) goto loc_8219B6EC;
	// lwz r3,24(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// bl 0x8219b690
	sub_8219B690(ctx, base);
loc_8219B6EC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219B708"))) PPC_WEAK_FUNC(sub_8219B708);
PPC_FUNC_IMPL(__imp__sub_8219B708) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
loc_8219B720:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r11
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r9,1
	// bne 0x8219b720
	if (!cr0.getEQ()) goto loc_8219B720;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x8219b774
	if (!cr6.getEQ()) goto loc_8219B774;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// clrlwi r10,r11,28
	ctx.r10.u64 = r11.u32 & 0xF;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// bne cr6,0x8219b768
	if (!cr6.getEQ()) goto loc_8219B768;
	// rlwinm. r11,r11,0,1,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219b768
	if (cr0.getEQ()) goto loc_8219B768;
	// lwz r3,24(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// bl 0x8219b708
	sub_8219B708(ctx, base);
loc_8219B768:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219b160
	sub_8219B160(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_8219B774:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219B788"))) PPC_WEAK_FUNC(sub_8219B788);
PPC_FUNC_IMPL(__imp__sub_8219B788) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lis r4,25728
	ctx.r4.s64 = 1686110208;
	// li r3,32
	ctx.r3.s64 = 32;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8219b7b8
	if (!cr0.getEQ()) goto loc_8219B7B8;
loc_8219B7B0:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8219b848
	goto loc_8219B848;
loc_8219B7B8:
	// lis r30,16
	r30.s64 = 1048576;
	// rlwinm. r10,r29,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ori r30,r30,1
	r30.u64 = r30.u64 | 1;
	// li r11,3
	r11.s64 = 3;
	// beq 0x8219b7d8
	if (cr0.getEQ()) goto loc_8219B7D8;
	// lis r30,48
	r30.s64 = 3145728;
	// li r11,2
	r11.s64 = 2;
	// ori r30,r30,1
	r30.u64 = r30.u64 | 1;
loc_8219B7D8:
	// rlwinm. r10,r29,0,22,22
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8219b7e4
	if (cr0.getEQ()) goto loc_8219B7E4;
	// oris r30,r30,64
	r30.u64 = r30.u64 | 4194304;
loc_8219B7E4:
	// lis r4,-32128
	ctx.r4.s64 = -2105540608;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// rlwimi r4,r11,28,1,3
	ctx.r4.u64 = (__builtin_rotateleft32(r11.u32, 28) & 0x70000000) | (ctx.r4.u64 & 0xFFFFFFFF8FFFFFFF);
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x8219b80c
	if (!cr0.getEQ()) goto loc_8219B80C;
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// b 0x8219b7b0
	goto loc_8219B7B0;
loc_8219B80C:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// rlwinm r10,r28,0,6,29
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x3FFFFFC;
	// ori r8,r3,3
	ctx.r8.u64 = ctx.r3.u64 | 3;
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// rlwinm r11,r11,0,4,4
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8000000;
	// lis r9,-1
	ctx.r9.s64 = -65536;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// oris r11,r11,4096
	r11.u64 = r11.u64 | 268435456;
	// stw r8,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r8.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// stw r9,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r9.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
loc_8219B848:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_8219B850"))) PPC_WEAK_FUNC(sub_8219B850);
PPC_FUNC_IMPL(__imp__sub_8219B850) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lis r6,768
	ctx.r6.s64 = 50331648;
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// rlwinm r8,r8,0,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFC;
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// rlwinm r9,r11,0,6,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3FFFFFC;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// li r4,10
	ctx.r4.s64 = 10;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// bl 0x8219b348
	sub_8219B348(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219B8A0"))) PPC_WEAK_FUNC(sub_8219B8A0);
PPC_FUNC_IMPL(__imp__sub_8219B8A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm r4,r11,0,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// b 0x8219b580
	sub_8219B580(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8219B8B0"))) PPC_WEAK_FUNC(sub_8219B8B0);
PPC_FUNC_IMPL(__imp__sub_8219B8B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lis r4,25728
	ctx.r4.s64 = 1686110208;
	// li r3,32
	ctx.r3.s64 = 32;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8219b8e4
	if (!cr0.getEQ()) goto loc_8219B8E4;
loc_8219B8DC:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8219b954
	goto loc_8219B954;
loc_8219B8E4:
	// rlwinm r10,r30,29,0,2
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 29) & 0xE0000000;
	// rlwinm. r9,r29,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// oris r30,r10,16
	r30.u64 = ctx.r10.u64 | 1048576;
	// li r11,3
	r11.s64 = 3;
	// ori r30,r30,2
	r30.u64 = r30.u64 | 2;
	// beq 0x8219b904
	if (cr0.getEQ()) goto loc_8219B904;
	// li r11,2
	r11.s64 = 2;
	// oris r30,r30,32
	r30.u64 = r30.u64 | 2097152;
loc_8219B904:
	// rlwinm. r10,r29,0,22,22
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8219b910
	if (cr0.getEQ()) goto loc_8219B910;
	// oris r30,r30,64
	r30.u64 = r30.u64 | 4194304;
loc_8219B910:
	// lis r4,-32128
	ctx.r4.s64 = -2105540608;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// rlwimi r4,r11,28,1,3
	ctx.r4.u64 = (__builtin_rotateleft32(r11.u32, 28) & 0x70000000) | (ctx.r4.u64 & 0xFFFFFFFF8FFFFFFF);
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// mr. r11,r3
	r11.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bne 0x8219b938
	if (!cr0.getEQ()) goto loc_8219B938;
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// b 0x8219b8dc
	goto loc_8219B8DC;
loc_8219B938:
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r28,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r28.u32);
	// lis r9,-1
	ctx.r9.s64 = -65536;
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// stw r9,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r9.u32);
loc_8219B954:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_8219B960"))) PPC_WEAK_FUNC(sub_8219B960);
PPC_FUNC_IMPL(__imp__sub_8219B960) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// ori r10,r6,2
	ctx.r10.u64 = ctx.r6.u64 | 2;
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,12
	ctx.r4.s64 = 12;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x8219b348
	sub_8219B348(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219B9A8"))) PPC_WEAK_FUNC(sub_8219B9A8);
PPC_FUNC_IMPL(__imp__sub_8219B9A8) {
	PPC_FUNC_PROLOGUE();
	// lwz r4,24(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// li r5,0
	ctx.r5.s64 = 0;
	// b 0x8219b580
	sub_8219B580(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8219B9B8"))) PPC_WEAK_FUNC(sub_8219B9B8);
PPC_FUNC_IMPL(__imp__sub_8219B9B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// clrlwi. r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r9,13500(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 13500);
	// beq 0x8219ba40
	if (cr0.getEQ()) goto loc_8219BA40;
	// lwz r10,14900(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14900);
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// rlwinm r8,r8,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// bne 0x8219b9ec
	if (!cr0.getEQ()) goto loc_8219B9EC;
	// lwz r10,156(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 156);
	// lwz r9,152(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 152);
	// rlwinm r10,r10,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_8219B9EC:
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// addi r7,r6,-1
	ctx.r7.s64 = ctx.r6.s64 + -1;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// addi r9,r9,164
	ctx.r9.s64 = ctx.r9.s64 + 164;
	// andc r3,r8,r7
	ctx.r3.u64 = ctx.r8.u64 & ~ctx.r7.u64;
	// cmplw cr6,r3,r9
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r9.u32, xer);
	// bge cr6,0x8219ba14
	if (!cr6.getLT()) goto loc_8219BA14;
loc_8219BA0C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8219BA14:
	// subf r10,r3,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r3.s64;
	// lwz r9,52(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// lwz r8,56(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// stw r3,14900(r11)
	PPC_STORE_U32(r11.u32 + 14900, ctx.r3.u32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// stw r9,52(r11)
	PPC_STORE_U32(r11.u32 + 52, ctx.r9.u32);
	// stw r10,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r10.u32);
	// blr 
	return;
loc_8219BA40:
	// lwz r10,56(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8219ba0c
	if (!cr6.getEQ()) goto loc_8219BA0C;
	// lwz r10,152(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 152);
	// stw r10,14912(r11)
	PPC_STORE_U32(r11.u32 + 14912, ctx.r10.u32);
	// lwz r11,156(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 156);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// lwz r3,152(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 152);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219BA68"))) PPC_WEAK_FUNC(sub_8219BA68);
PPC_FUNC_IMPL(__imp__sub_8219BA68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// lwz r9,156(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beqlr 
	if (cr0.getEQ()) return;
	// lis r10,16384
	ctx.r10.s64 = 1073741824;
	// lwz r11,152(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r8,r11,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r4,r8,512
	ctx.r4.s64 = ctx.r8.s64 + 512;
	// rlwinm r9,r10,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r8,r10,3
	ctx.r8.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r9,r9,512
	ctx.r9.s64 = ctx.r9.s64 + 512;
	// rlwinm r10,r4,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x1000;
	// rlwinm r9,r9,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1000;
	// clrlwi r11,r11,3
	r11.u64 = r11.u32 & 0x1FFFFFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// subf r4,r7,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r7.s64;
	// subf r3,r6,r11
	ctx.r3.s64 = r11.s64 - ctx.r6.s64;
	// b 0x821a39a0
	sub_821A39A0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8219BAC4"))) PPC_WEAK_FUNC(sub_8219BAC4);
PPC_FUNC_IMPL(__imp__sub_8219BAC4) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219BAC8"))) PPC_WEAK_FUNC(sub_8219BAC8);
PPC_FUNC_IMPL(__imp__sub_8219BAC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r5,128
	ctx.r5.s64 = 128;
	// li r4,502
	ctx.r4.s64 = 502;
	// lwz r29,13504(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 13504);
	// bl 0x8219ca10
	sub_8219CA10(ctx, base);
	// lbz r11,10941(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10941);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219bb00
	if (cr0.getEQ()) goto loc_8219BB00;
	// lwz r31,16712(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 16712);
	// b 0x8219bb5c
	goto loc_8219BB5C;
loc_8219BB00:
	// rlwinm r11,r31,12,20,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 12) & 0xFFF;
	// stw r31,13504(r30)
	PPC_STORE_U32(r30.u32 + 13504, r31.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// clrlwi r10,r31,3
	ctx.r10.u64 = r31.u32 & 0x1FFFFFFF;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// addis r3,r11,-16384
	ctx.r3.s64 = r11.s64 + -1073741824;
	// bne cr6,0x8219bb38
	if (!cr6.getEQ()) goto loc_8219BB38;
	// lwz r11,13500(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13500);
	// stw r3,112(r11)
	PPC_STORE_U32(r11.u32 + 112, ctx.r3.u32);
	// b 0x8219bb50
	goto loc_8219BB50;
loc_8219BB38:
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// lwz r11,13508(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13508);
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// stw r11,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r11.u32);
loc_8219BB50:
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r3,2008
	ctx.r4.s64 = ctx.r3.s64 + 2008;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
loc_8219BB5C:
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// addi r11,r31,2008
	r11.s64 = r31.s64 + 2008;
	// stw r3,13508(r30)
	PPC_STORE_U32(r30.u32 + 13508, ctx.r3.u32);
	// stw r11,13512(r30)
	PPC_STORE_U32(r30.u32 + 13512, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219BB78"))) PPC_WEAK_FUNC(sub_8219BB78);
PPC_FUNC_IMPL(__imp__sub_8219BB78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r5,128
	ctx.r5.s64 = 128;
	// li r4,136
	ctx.r4.s64 = 136;
	// lwz r29,13516(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 13516);
	// bl 0x8219ca10
	sub_8219CA10(ctx, base);
	// lbz r11,10941(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10941);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219bbb0
	if (cr0.getEQ()) goto loc_8219BBB0;
	// lwz r31,16712(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 16712);
	// b 0x8219bc0c
	goto loc_8219BC0C;
loc_8219BBB0:
	// rlwinm r11,r31,12,20,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 12) & 0xFFF;
	// stw r31,13516(r30)
	PPC_STORE_U32(r30.u32 + 13516, r31.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// clrlwi r10,r31,3
	ctx.r10.u64 = r31.u32 & 0x1FFFFFFF;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// addis r3,r11,-16384
	ctx.r3.s64 = r11.s64 + -1073741824;
	// bne cr6,0x8219bbe8
	if (!cr6.getEQ()) goto loc_8219BBE8;
	// lwz r11,13500(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13500);
	// stw r3,116(r11)
	PPC_STORE_U32(r11.u32 + 116, ctx.r3.u32);
	// b 0x8219bc00
	goto loc_8219BC00;
loc_8219BBE8:
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// lwz r11,13520(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13520);
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// srawi r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	// stw r11,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r11.u32);
loc_8219BC00:
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r3,136
	ctx.r4.s64 = ctx.r3.s64 + 136;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
loc_8219BC0C:
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// addi r11,r31,136
	r11.s64 = r31.s64 + 136;
	// stw r3,13520(r30)
	PPC_STORE_U32(r30.u32 + 13520, ctx.r3.u32);
	// stw r11,13524(r30)
	PPC_STORE_U32(r30.u32 + 13524, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219BC28"))) PPC_WEAK_FUNC(sub_8219BC28);
PPC_FUNC_IMPL(__imp__sub_8219BC28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed134
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// lwz r10,1768(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 1768);
	// lwz r11,168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 168);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// bge cr6,0x8219bc64
	if (!cr6.getLT()) goto loc_8219BC64;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
loc_8219BC64:
	// addi r29,r10,14956
	r29.s64 = ctx.r10.s64 + 14956;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8240f8bc
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r3,164(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// lwz r11,172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8240f8ac
	__imp__RtlLeaveCriticalSection(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed184
	return;
}

__attribute__((alias("__imp__sub_8219BCA8"))) PPC_WEAK_FUNC(sub_8219BCA8);
PPC_FUNC_IMPL(__imp__sub_8219BCA8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r30,r11,14956
	r30.s64 = r11.s64 + 14956;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8240f8bc
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r3,164(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// lwz r11,176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 176);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8240f8ac
	__imp__RtlLeaveCriticalSection(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219BD08"))) PPC_WEAK_FUNC(sub_8219BD08);
PPC_FUNC_IMPL(__imp__sub_8219BD08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r30,r11,14956
	r30.s64 = r11.s64 + 14956;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8240f8bc
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r3,164(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// lwz r11,180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 180);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8240f8ac
	__imp__RtlLeaveCriticalSection(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_8219BD60"))) PPC_WEAK_FUNC(sub_8219BD60);
PPC_FUNC_IMPL(__imp__sub_8219BD60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,152(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 152);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8219bd88
	if (!cr6.getEQ()) goto loc_8219BD88;
	// bl 0x8219bca8
	sub_8219BCA8(ctx, base);
	// b 0x8219bd98
	goto loc_8219BD98;
loc_8219BD88:
	// bl 0x8219ba68
	sub_8219BA68(ctx, base);
	// lis r4,-20096
	ctx.r4.s64 = -1317011456;
	// lwz r3,152(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 152);
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_8219BD98:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219BDB0"))) PPC_WEAK_FUNC(sub_8219BDB0);
PPC_FUNC_IMPL(__imp__sub_8219BDB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r8,13216(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 13216);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq 0x8219be54
	if (cr0.getEQ()) goto loc_8219BE54;
	// lbz r7,10941(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 10941);
	// lwz r9,14888(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 14888);
	// lwz r10,14892(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14892);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// rlwinm. r9,r7,0,26,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// beq 0x8219be18
	if (cr0.getEQ()) goto loc_8219BE18;
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x8219bebc
	goto loc_8219BEBC;
loc_8219BE18:
	// lwz r10,14904(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 14904);
	// lwz r9,13220(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 13220);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x8219be40
	if (!cr6.getEQ()) goto loc_8219BE40;
	// subf r11,r11,r8
	r11.s64 = ctx.r8.s64 - r11.s64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r10,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r10.s64;
	// b 0x8219bebc
	goto loc_8219BEBC;
loc_8219BE40:
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r9,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r9.s64;
	// b 0x8219bebc
	goto loc_8219BEBC;
loc_8219BE54:
	// lbz r8,10940(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 10940);
	// rlwinm. r8,r8,0,0,24
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x8219bebc
	if (cr0.getEQ()) goto loc_8219BEBC;
	// lwz r3,13500(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 13500);
	// lwz r10,152(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x8219be88
	if (!cr6.getEQ()) goto loc_8219BE88;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// bl 0x8219bd08
	sub_8219BD08(ctx, base);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x8219bebc
	goto loc_8219BEBC;
loc_8219BE88:
	// lbz r10,10941(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 10941);
	// rlwinm. r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8219be9c
	if (cr0.getEQ()) goto loc_8219BE9C;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x8219beb0
	goto loc_8219BEB0;
loc_8219BE9C:
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_8219BEB0:
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r11,156(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
loc_8219BEBC:
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219BEE0"))) PPC_WEAK_FUNC(sub_8219BEE0);
PPC_FUNC_IMPL(__imp__sub_8219BEE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r7,r3,32
	ctx.r7.s64 = ctx.r3.s64 + 32;
	// addi r6,r3,800
	ctx.r6.s64 = ctx.r3.s64 + 800;
	// rlwinm r11,r10,29,3,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFC;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// bne cr6,0x8219bf04
	if (!cr6.getEQ()) goto loc_8219BF04;
loc_8219BEFC:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_8219BF04:
	// clrlwi r8,r10,27
	ctx.r8.u64 = ctx.r10.u32 & 0x1F;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// li r10,-1
	ctx.r10.s64 = -1;
	// srw r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r8.u8 & 0x3F));
	// andc r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ~ctx.r9.u64;
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// cmplwi cr6,r8,32
	cr6.compare<uint32_t>(ctx.r8.u32, 32, xer);
	// bne cr6,0x8219bf48
	if (!cr6.getEQ()) goto loc_8219BF48;
loc_8219BF24:
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r9,-1
	cr6.compare<int32_t>(ctx.r9.s32, -1, xer);
	// beq cr6,0x8219bf24
	if (cr6.getEQ()) goto loc_8219BF24;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// beq cr6,0x8219befc
	if (cr6.getEQ()) goto loc_8219BEFC;
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// not r8,r9
	ctx.r8.u64 = ~ctx.r9.u64;
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
loc_8219BF48:
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// srw r5,r5,r8
	ctx.r5.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r5.u32 >> (ctx.r8.u8 & 0x3F));
	// and r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 & ctx.r9.u64;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// cmplwi cr6,r9,32
	cr6.compare<uint32_t>(ctx.r9.u32, 32, xer);
	// bne cr6,0x8219bf94
	if (!cr6.getEQ()) goto loc_8219BF94;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// b 0x8219bf70
	goto loc_8219BF70;
loc_8219BF6C:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
loc_8219BF70:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8219bf6c
	if (cr6.getEQ()) goto loc_8219BF6C;
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// bne cr6,0x8219bf8c
	if (!cr6.getEQ()) goto loc_8219BF8C;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x8219bf94
	goto loc_8219BF94;
loc_8219BF8C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
loc_8219BF94:
	// subf r11,r7,r11
	r11.s64 = r11.s64 - ctx.r7.s64;
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// rlwinm r11,r11,5,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219BFC0"))) PPC_WEAK_FUNC(sub_8219BFC0);
PPC_FUNC_IMPL(__imp__sub_8219BFC0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// rlwinm r8,r5,27,5,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x7FFFFFF;
	// rlwinm r10,r4,27,5,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x7FFFFFF;
	// addi r9,r8,8
	ctx.r9.s64 = ctx.r8.s64 + 8;
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r7,r4,27
	ctx.r7.u64 = ctx.r4.u32 & 0x1F;
	// clrlwi r5,r5,27
	ctx.r5.u64 = ctx.r5.u32 & 0x1F;
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// beq cr6,0x8219c070
	if (cr6.getEQ()) goto loc_8219C070;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// li r10,-1
	ctx.r10.s64 = -1;
	// bne cr6,0x8219c014
	if (!cr6.getEQ()) goto loc_8219C014;
	// srw r8,r10,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r5.u8 & 0x3F));
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srw r10,r10,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r7.u8 & 0x3F));
	// andc r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r8.u64;
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// b 0x8219c0f0
	goto loc_8219C0F0;
loc_8219C014:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srw r7,r10,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r7.u8 & 0x3F));
	// or r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 | ctx.r8.u64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x8219c060
	if (!cr6.getLT()) goto loc_8219C060;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addic. r9,r9,1
	xer.ca = ctx.r9.u32 > 4294967294;
	ctx.r9.s64 = ctx.r9.s64 + 1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x8219c058
	if (cr0.getEQ()) goto loc_8219C058;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8219C04C:
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bdnz 0x8219c04c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8219C04C;
loc_8219C058:
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
loc_8219C060:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srw r10,r10,r5
	ctx.r10.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r5.u8 & 0x3F));
	// orc r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ~ctx.r10.u64;
	// b 0x8219c0f0
	goto loc_8219C0F0;
loc_8219C070:
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// li r10,-1
	ctx.r10.s64 = -1;
	// bne cr6,0x8219c094
	if (!cr6.getEQ()) goto loc_8219C094;
	// srw r8,r10,r5
	ctx.r8.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r5.u8 & 0x3F));
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srw r10,r10,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r7.u8 & 0x3F));
	// andc r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r8.u64;
	// andc r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ~ctx.r10.u64;
	// b 0x8219c0f0
	goto loc_8219C0F0;
loc_8219C094:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srw r7,r10,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r7.u8 & 0x3F));
	// andc r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 & ~ctx.r7.u64;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x8219c0e4
	if (!cr6.getLT()) goto loc_8219C0E4;
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// li r7,0
	ctx.r7.s64 = 0;
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// addic. r9,r9,1
	xer.ca = ctx.r9.u32 > 4294967294;
	ctx.r9.s64 = ctx.r9.s64 + 1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x8219c0dc
	if (cr0.getEQ()) goto loc_8219C0DC;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_8219C0D0:
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bdnz 0x8219c0d0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8219C0D0;
loc_8219C0DC:
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
loc_8219C0E4:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// srw r10,r10,r5
	ctx.r10.u64 = ctx.r5.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r5.u8 & 0x3F));
	// and r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ctx.r9.u64;
loc_8219C0F0:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219C0F8"))) PPC_WEAK_FUNC(sub_8219C0F8);
PPC_FUNC_IMPL(__imp__sub_8219C0F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r31,-31400(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + -31400);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8240f8bc
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// li r6,0
	ctx.r6.s64 = 0;
	// add r11,r29,r11
	r11.u64 = r29.u64 + r11.u64;
	// add r5,r30,r29
	ctx.r5.u64 = r30.u64 + r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// bl 0x8219bfc0
	sub_8219BFC0(ctx, base);
	// bl 0x8240f8ac
	__imp__RtlLeaveCriticalSection(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219C148"))) PPC_WEAK_FUNC(sub_8219C148);
PPC_FUNC_IMPL(__imp__sub_8219C148) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed12c
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// li r27,-1
	r27.s64 = -1;
	// addi r30,r11,-31400
	r30.s64 = r11.s64 + -31400;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r26,r27
	r26.u64 = r27.u64;
	// li r29,0
	r29.s64 = 0;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x8219c200
	if (!cr0.getEQ()) goto loc_8219C200;
	// lis r4,25728
	ctx.r4.s64 = 1686110208;
	// li r3,804
	ctx.r3.s64 = 804;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8219c198
	if (!cr0.getEQ()) goto loc_8219C198;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8219c2b4
	goto loc_8219C2B4;
loc_8219C198:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8240f94c
	__imp__RtlInitializeCriticalSection(ctx, base);
	// lis r10,21845
	ctx.r10.s64 = 1431633920;
	// li r11,6144
	r11.s64 = 6144;
	// ori r10,r10,21845
	ctx.r10.u64 = ctx.r10.u64 | 21845;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// stw r10,800(r31)
	PPC_STORE_U32(r31.u32 + 800, ctx.r10.u32);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_8219C1B8:
	// mfmsr r8
	// mtmsrd r13,1
	// lwarx r9,0,r10
	reserved.u32 = *(uint32_t*)(base + ctx.r10.u32);
	ctx.r9.u64 = __builtin_bswap32(reserved.u32);
	// cmpw cr6,r9,r29
	cr6.compare<int32_t>(ctx.r9.s32, r29.s32, xer);
	// bne cr6,0x8219c1dc
	if (!cr6.getEQ()) goto loc_8219C1DC;
	// stwcx. r31,0,r10
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r10.u32), reserved.s32, __builtin_bswap32(r31.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r8,1
	// bne 0x8219c1b8
	if (!cr0.getEQ()) goto loc_8219C1B8;
	// b 0x8219c1e4
	goto loc_8219C1E4;
loc_8219C1DC:
	// stwcx. r9,0,r10
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r10.u32), reserved.s32, __builtin_bswap32(ctx.r9.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r8,1
loc_8219C1E4:
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8219c200
	if (cr6.getEQ()) goto loc_8219C200;
	// lis r4,25728
	ctx.r4.s64 = 1686110208;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
loc_8219C200:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8240f8bc
	__imp__RtlEnterCriticalSection(ctx, base);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bgt cr6,0x8219c298
	if (cr6.getGT()) goto loc_8219C298;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r30,r29
	r30.u64 = r29.u64;
	// mr r29,r27
	r29.u64 = r27.u64;
	// bl 0x8219bee0
	sub_8219BEE0(ctx, base);
	// mr. r11,r3
	r11.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219c298
	if (cr0.getEQ()) goto loc_8219C298;
loc_8219C234:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// blt cr6,0x8219c250
	if (cr6.getLT()) goto loc_8219C250;
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// bge cr6,0x8219c250
	if (!cr6.getLT()) goto loc_8219C250;
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
	// mr r29,r11
	r29.u64 = r11.u64;
loc_8219C250:
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x8219bee0
	sub_8219BEE0(ctx, base);
	// mr. r11,r3
	r11.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8219c234
	if (!cr0.getEQ()) goto loc_8219C234;
	// cmpwi cr6,r29,-1
	cr6.compare<int32_t>(r29.s32, -1, xer);
	// beq cr6,0x8219c298
	if (cr6.getEQ()) goto loc_8219C298;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// li r6,1
	ctx.r6.s64 = 1;
	// add r5,r30,r28
	ctx.r5.u64 = r30.u64 + r28.u64;
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// bl 0x8219bfc0
	sub_8219BFC0(ctx, base);
	// mr r26,r30
	r26.u64 = r30.u64;
loc_8219C298:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8240f8ac
	__imp__RtlLeaveCriticalSection(ctx, base);
	// subf r11,r26,r27
	r11.s64 = r27.s64 - r26.s64;
	// stw r26,0(r25)
	PPC_STORE_U32(r25.u32 + 0, r26.u32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = r11.u64 ^ 1;
loc_8219C2B4:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed17c
	return;
}

__attribute__((alias("__imp__sub_8219C2C0"))) PPC_WEAK_FUNC(sub_8219C2C0);
PPC_FUNC_IMPL(__imp__sub_8219C2C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// lwz r10,52(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// lwz r9,14912(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14912);
	// lwz r8,14908(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14908);
	// lwz r7,14900(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14900);
	// lwz r6,14904(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14904);
	// stw r11,13392(r3)
	PPC_STORE_U32(ctx.r3.u32 + 13392, r11.u32);
	// stw r10,13396(r3)
	PPC_STORE_U32(ctx.r3.u32 + 13396, ctx.r10.u32);
	// stw r9,13400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 13400, ctx.r9.u32);
	// stw r8,13404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 13404, ctx.r8.u32);
	// stw r7,13408(r3)
	PPC_STORE_U32(ctx.r3.u32 + 13408, ctx.r7.u32);
	// stw r6,13412(r3)
	PPC_STORE_U32(ctx.r3.u32 + 13412, ctx.r6.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219C2F8"))) PPC_WEAK_FUNC(sub_8219C2F8);
PPC_FUNC_IMPL(__imp__sub_8219C2F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r10,13392(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13392);
	// lwz r11,13396(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13396);
	// lwz r9,13400(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13400);
	// lwz r8,13404(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13404);
	// lwz r7,13408(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13408);
	// stw r10,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, ctx.r10.u32);
	// addi r10,r11,-160
	ctx.r10.s64 = r11.s64 + -160;
	// lwz r6,13412(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13412);
	// stw r9,14912(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14912, ctx.r9.u32);
	// stw r8,14908(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14908, ctx.r8.u32);
	// stw r7,14900(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14900, ctx.r7.u32);
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, r11.u32);
	// stw r6,14904(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14904, ctx.r6.u32);
	// stw r10,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219C338"))) PPC_WEAK_FUNC(sub_8219C338);
PPC_FUNC_IMPL(__imp__sub_8219C338) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r11,10896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// subf r10,r11,r30
	ctx.r10.s64 = r30.s64 - r11.s64;
	// clrlwi. r10,r10,30
	ctx.r10.u64 = ctx.r10.u32 & 0x3;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8219c3c8
	if (cr0.getEQ()) goto loc_8219C3C8;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bne cr6,0x8219c378
	if (!cr6.getEQ()) goto loc_8219C378;
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// ble cr6,0x8219c3c8
	if (!cr6.getGT()) goto loc_8219C3C8;
loc_8219C378:
	// li r5,2
	ctx.r5.s64 = 2;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198a90
	sub_82198A90(ctx, base);
loc_8219C388:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198c28
	sub_82198C28(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x8219c3c0
	if (cr0.getEQ()) goto loc_8219C3C0;
	// lwz r11,10896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// subf r10,r11,r30
	ctx.r10.s64 = r30.s64 - r11.s64;
	// clrlwi. r10,r10,30
	ctx.r10.u64 = ctx.r10.u32 & 0x3;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8219c3c0
	if (cr0.getEQ()) goto loc_8219C3C0;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bne cr6,0x8219c388
	if (!cr6.getEQ()) goto loc_8219C388;
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bgt cr6,0x8219c388
	if (cr6.getGT()) goto loc_8219C388;
loc_8219C3C0:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198ac0
	sub_82198AC0(ctx, base);
loc_8219C3C8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219C3D0"))) PPC_WEAK_FUNC(sub_8219C3D0);
PPC_FUNC_IMPL(__imp__sub_8219C3D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// add r10,r31,r5
	ctx.r10.u64 = r31.u64 + ctx.r5.u64;
	// lwz r11,14884(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14884);
	// lwz r9,10896(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 10896);
	// and r30,r10,r11
	r30.u64 = ctx.r10.u64 & r11.u64;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// lwz r11,60(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 60);
	// bge cr6,0x8219c414
	if (!cr6.getLT()) goto loc_8219C414;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bge cr6,0x8219c470
	if (!cr6.getLT()) goto loc_8219C470;
loc_8219C408:
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// ble cr6,0x8219c41c
	if (!cr6.getGT()) goto loc_8219C41C;
	// b 0x8219c470
	goto loc_8219C470;
loc_8219C414:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bge cr6,0x8219c408
	if (!cr6.getLT()) goto loc_8219C408;
loc_8219C41C:
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198a90
	sub_82198A90(ctx, base);
loc_8219C42C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198c28
	sub_82198C28(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x8219c468
	if (cr0.getEQ()) goto loc_8219C468;
	// lwz r11,10896(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 10896);
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// lwz r11,60(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// bge cr6,0x8219c458
	if (!cr6.getLT()) goto loc_8219C458;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bge cr6,0x8219c468
	if (!cr6.getLT()) goto loc_8219C468;
	// b 0x8219c460
	goto loc_8219C460;
loc_8219C458:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x8219c42c
	if (cr6.getLT()) goto loc_8219C42C;
loc_8219C460:
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// ble cr6,0x8219c42c
	if (!cr6.getGT()) goto loc_8219C42C;
loc_8219C468:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198ac0
	sub_82198AC0(ctx, base);
loc_8219C470:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219C480"))) PPC_WEAK_FUNC(sub_8219C480);
PPC_FUNC_IMPL(__imp__sub_8219C480) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r3,21516(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 21516);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x8219c4b8
	if (cr0.getEQ()) goto loc_8219C4B8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8219C4B8:
	// lwz r11,21656(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21656);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8219c4dc
	if (cr0.getEQ()) goto loc_8219C4DC;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8219C4DC:
	// lwz r31,10952(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 10952);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r27,14884(r30)
	r27.u64 = PPC_LOAD_U32(r30.u32 + 14884);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r26,14880(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 14880);
	// bl 0x8219c3d0
	sub_8219C3D0(ctx, base);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x8219c524
	if (cr6.getEQ()) goto loc_8219C524;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_8219C504:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm r9,r31,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r31,1
	ctx.r8.s64 = r31.s64 + 1;
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// and r31,r8,r27
	r31.u64 = ctx.r8.u64 & r27.u64;
	// stwx r10,r9,r26
	PPC_STORE_U32(ctx.r9.u32 + r26.u32, ctx.r10.u32);
	// bne 0x8219c504
	if (!cr0.getEQ()) goto loc_8219C504;
loc_8219C524:
	// lwz r3,21516(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 21516);
	// stw r31,10952(r30)
	PPC_STORE_U32(r30.u32 + 10952, r31.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x8219c544
	if (cr0.getEQ()) goto loc_8219C544;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8219C544:
	// lwz r11,21656(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21656);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8219c568
	if (cr0.getEQ()) goto loc_8219C568;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,2
	ctx.r3.s64 = 2;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8219C568:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_8219C570"))) PPC_WEAK_FUNC(sub_8219C570);
PPC_FUNC_IMPL(__imp__sub_8219C570) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
loc_8219C570:
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// cmplw cr6,r4,r10
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r10.u32, xer);
	// bge cr6,0x8219c584
	if (!cr6.getLT()) goto loc_8219C584;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
loc_8219C584:
	// rldicl r9,r11,32,32
	ctx.r9.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFFFFFFFF;
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// cmplw cr6,r5,r9
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r9.u32, xer);
	// ble cr6,0x8219c598
	if (!cr6.getGT()) goto loc_8219C598;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
loc_8219C598:
	// rldicr r9,r9,32,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// or r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 | ctx.r10.u64;
loc_8219C5A4:
	// mfmsr r7
	// mtmsrd r13,1
	// ldarx r8,0,r3
	reserved.u64 = *(uint64_t*)(base + ctx.r3.u32);
	ctx.r8.u64 = __builtin_bswap64(reserved.u64);
	// cmpd cr6,r8,r11
	cr6.compare<int64_t>(ctx.r8.s64, r11.s64, xer);
	// bne cr6,0x8219c5c8
	if (!cr6.getEQ()) goto loc_8219C5C8;
	// stdcx. r10,0,r3
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint64_t*>(base + ctx.r3.u32), reserved.s64, __builtin_bswap64(ctx.r10.s64));
	cr0.getSO() = xer.so;
	// mtmsrd r7,1
	// bne 0x8219c5a4
	if (!cr0.getEQ()) goto loc_8219C5A4;
	// b 0x8219c5d0
	goto loc_8219C5D0;
loc_8219C5C8:
	// stdcx. r8,0,r3
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint64_t*>(base + ctx.r3.u32), reserved.s64, __builtin_bswap64(ctx.r8.s64));
	cr0.getSO() = xer.so;
	// mtmsrd r7,1
loc_8219C5D0:
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// cmpld cr6,r10,r11
	cr6.compare<uint64_t>(ctx.r10.u64, r11.u64, xer);
	// bne cr6,0x8219c570
	if (!cr6.getEQ()) goto loc_8219C570;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219C5E0"))) PPC_WEAK_FUNC(sub_8219C5E0);
PPC_FUNC_IMPL(__imp__sub_8219C5E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
loc_8219C5E0:
	// li r10,-1
	ctx.r10.s64 = -1;
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
loc_8219C5EC:
	// mfmsr r8
	// mtmsrd r13,1
	// ldarx r9,0,r3
	reserved.u64 = *(uint64_t*)(base + ctx.r3.u32);
	ctx.r9.u64 = __builtin_bswap64(reserved.u64);
	// cmpd cr6,r9,r11
	cr6.compare<int64_t>(ctx.r9.s64, r11.s64, xer);
	// bne cr6,0x8219c610
	if (!cr6.getEQ()) goto loc_8219C610;
	// stdcx. r10,0,r3
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint64_t*>(base + ctx.r3.u32), reserved.s64, __builtin_bswap64(ctx.r10.s64));
	cr0.getSO() = xer.so;
	// mtmsrd r8,1
	// bne 0x8219c5ec
	if (!cr0.getEQ()) goto loc_8219C5EC;
	// b 0x8219c618
	goto loc_8219C618;
loc_8219C610:
	// stdcx. r9,0,r3
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint64_t*>(base + ctx.r3.u32), reserved.s64, __builtin_bswap64(ctx.r9.s64));
	cr0.getSO() = xer.so;
	// mtmsrd r8,1
loc_8219C618:
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// cmpld cr6,r10,r11
	cr6.compare<uint64_t>(ctx.r10.u64, r11.u64, xer);
	// bne cr6,0x8219c5e0
	if (!cr6.getEQ()) goto loc_8219C5E0;
	// rldicl r10,r11,32,32
	ctx.r10.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFFFFFFFF;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219C638"))) PPC_WEAK_FUNC(sub_8219C638);
PPC_FUNC_IMPL(__imp__sub_8219C638) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// bne cr6,0x8219c6c8
	if (!cr6.getEQ()) goto loc_8219C6C8;
	// lwz r11,10900(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 10900);
	// lis r10,2989
	ctx.r10.s64 = 195887104;
	// ori r10,r10,61453
	ctx.r10.u64 = ctx.r10.u64 | 61453;
	// lwz r31,16(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x8219c678
	if (!cr6.getEQ()) goto loc_8219C678;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,-17560
	ctx.r3.s64 = r11.s64 + -17560;
	// bl 0x821a2690
	sub_821A2690(ctx, base);
	// twi 31,r0,22
loc_8219C678:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8219c690
	if (cr6.getEQ()) goto loc_8219C690;
	// lwz r11,10900(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 10900);
	// lwz r3,20(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8219C690:
	// lbz r10,268(r13)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r13.u32 + 268);
	// li r11,1
	r11.s64 = 1;
	// addi r31,r30,10904
	r31.s64 = r30.s64 + 10904;
	// lwz r30,10900(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 10900);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// slw r29,r11,r10
	r29.u64 = ctx.r10.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r10.u8 & 0x3F));
	// bl 0x8240f96c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// andc r11,r11,r29
	r11.u64 = r11.u64 & ~r29.u64;
	// clrlwi r11,r11,26
	r11.u64 = r11.u32 & 0x3F;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// bl 0x8240f95c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// b 0x8219c6e8
	goto loc_8219C6E8;
loc_8219C6C8:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x8219c6e8
	if (!cr6.getEQ()) goto loc_8219C6E8;
	// lis r11,32712
	r11.s64 = 2143813632;
	// lwz r11,25924(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 25924);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219c6e8
	if (cr0.getEQ()) goto loc_8219C6E8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82198fb8
	sub_82198FB8(ctx, base);
loc_8219C6E8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219C6F0"))) PPC_WEAK_FUNC(sub_8219C6F0);
PPC_FUNC_IMPL(__imp__sub_8219C6F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq 0x8219c728
	if (cr0.getEQ()) goto loc_8219C728;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lis r7,-16384
	ctx.r7.s64 = -1073741824;
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addis r11,r11,-16384
	r11.s64 = r11.s64 + -1073741824;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
loc_8219C728:
	// li r11,0
	r11.s64 = 0;
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219C738"))) PPC_WEAK_FUNC(sub_8219C738);
PPC_FUNC_IMPL(__imp__sub_8219C738) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// rlwinm r10,r5,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 12) & 0xFFF;
	// lwz r31,8(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// addi r9,r10,512
	ctx.r9.s64 = ctx.r10.s64 + 512;
	// clrlwi r7,r5,3
	ctx.r7.u64 = ctx.r5.u32 & 0x1FFFFFFF;
	// rlwinm r8,r9,0,19,19
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1000;
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// lis r10,16384
	ctx.r10.s64 = 1073741824;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r9,r9,512
	ctx.r9.s64 = ctx.r9.s64 + 512;
	// subf r4,r10,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r10.s64;
	// rlwinm r9,r9,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r11,3
	ctx.r8.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r7,r5,-4
	ctx.r7.s64 = ctx.r5.s64 + -4;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// subf r30,r10,r9
	r30.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// rlwinm r10,r31,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r31,3
	ctx.r9.u64 = r31.u32 & 0x1FFFFFFF;
	// stw r7,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r7.u32);
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r29,r8,r10
	r29.s64 = ctx.r10.s64 - ctx.r8.s64;
	// eieio 
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
	// stw r30,0(r31)
	PPC_MM_STORE_U32(r31.u32 + 0, r30.u32);
	// eieio 
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r29,16
	ctx.r4.s64 = r29.s64 + 16;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219C7E0"))) PPC_WEAK_FUNC(sub_8219C7E0);
PPC_FUNC_IMPL(__imp__sub_8219C7E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lbz r10,10941(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 10941);
	// lwz r11,16712(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16712);
	// ori r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 | 32;
	// stw r11,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, r11.u32);
	// stb r10,10941(r3)
	PPC_STORE_U8(ctx.r3.u32 + 10941, ctx.r10.u8);
	// addi r10,r11,4800
	ctx.r10.s64 = r11.s64 + 4800;
	// addi r11,r10,-160
	r11.s64 = ctx.r10.s64 + -160;
	// stw r10,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, ctx.r10.u32);
	// stw r11,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219C808"))) PPC_WEAK_FUNC(sub_8219C808);
PPC_FUNC_IMPL(__imp__sub_8219C808) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// clrlwi. r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// rlwinm r7,r11,30,2,31
	ctx.r7.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// lwz r11,14900(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14900);
	// beq 0x8219c908
	if (cr0.getEQ()) goto loc_8219C908;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x8219c83c
	if (!cr0.getEQ()) goto loc_8219C83C;
	// lwz r11,52(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// addi r11,r11,60
	r11.s64 = r11.s64 + 60;
loc_8219C83C:
	// addi r9,r6,-1
	ctx.r9.s64 = ctx.r6.s64 + -1;
	// lwz r8,14892(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14892);
	// add r10,r11,r6
	ctx.r10.u64 = r11.u64 + ctx.r6.u64;
	// lwz r5,14904(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14904);
	// not r11,r9
	r11.u64 = ~ctx.r9.u64;
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// and r31,r9,r11
	r31.u64 = ctx.r9.u64 & r11.u64;
	// add r4,r10,r31
	ctx.r4.u64 = ctx.r10.u64 + r31.u64;
	// cmplw cr6,r4,r8
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r8.u32, xer);
	// bgt cr6,0x8219c8a4
	if (cr6.getGT()) goto loc_8219C8A4;
	// lwz r10,13216(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13216);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x8219c89c
	if (cr0.getEQ()) goto loc_8219C89C;
	// lwz r11,13220(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13220);
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// beq cr6,0x8219c894
	if (cr6.getEQ()) goto loc_8219C894;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8219c89c
	if (!cr6.getEQ()) goto loc_8219C89C;
	// cmplw cr6,r10,r4
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r4.u32, xer);
	// bge cr6,0x8219c89c
	if (!cr6.getLT()) goto loc_8219C89C;
loc_8219C894:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8219ca04
	goto loc_8219CA04;
loc_8219C89C:
	// stw r4,14900(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14900, ctx.r4.u32);
	// b 0x8219c8fc
	goto loc_8219C8FC;
loc_8219C8A4:
	// lwz r9,14908(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14908);
	// lwz r8,13216(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13216);
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// and r31,r9,r11
	r31.u64 = ctx.r9.u64 & r11.u64;
	// add r4,r10,r31
	ctx.r4.u64 = ctx.r10.u64 + r31.u64;
	// beq 0x8219c8e8
	if (cr0.getEQ()) goto loc_8219C8E8;
	// lwz r11,13220(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13220);
	// subf r11,r5,r11
	r11.s64 = r11.s64 - ctx.r5.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// beq cr6,0x8219c894
	if (cr6.getEQ()) goto loc_8219C894;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8219c8e8
	if (!cr6.getEQ()) goto loc_8219C8E8;
	// cmplw cr6,r8,r4
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r4.u32, xer);
	// blt cr6,0x8219c894
	if (cr6.getLT()) goto loc_8219C894;
loc_8219C8E8:
	// lwz r11,14912(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14912);
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// bgt cr6,0x8219c894
	if (cr6.getGT()) goto loc_8219C894;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r4,14908(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14908, ctx.r4.u32);
loc_8219C8FC:
	// bl 0x8219c338
	sub_8219C338(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// b 0x8219ca04
	goto loc_8219CA04;
loc_8219C908:
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x8219c918
	if (!cr0.getEQ()) goto loc_8219C918;
	// lwz r11,48(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_8219C918:
	// addi r11,r11,31
	r11.s64 = r11.s64 + 31;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// rlwinm r30,r11,0,0,26
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// bne cr6,0x8219c92c
	if (!cr6.getEQ()) goto loc_8219C92C;
	// li r7,54
	ctx.r7.s64 = 54;
loc_8219C92C:
	// lwz r11,14896(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14896);
	// rlwinm r11,r11,30,2,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r7,r11
	cr6.compare<uint32_t>(ctx.r7.u32, r11.u32, xer);
	// ble cr6,0x8219c940
	if (!cr6.getGT()) goto loc_8219C940;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_8219C940:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,14892(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14892);
	// add r31,r10,r30
	r31.u64 = ctx.r10.u64 + r30.u64;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// ble cr6,0x8219c958
	if (!cr6.getGT()) goto loc_8219C958;
	// mr r31,r11
	r31.u64 = r11.u64;
loc_8219C958:
	// subf r11,r30,r31
	r11.s64 = r31.s64 - r30.s64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bge cr6,0x8219c998
	if (!cr6.getLT()) goto loc_8219C998;
	// lwz r9,14908(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14908);
	// lwz r11,14904(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14904);
	// addi r9,r9,31
	ctx.r9.s64 = ctx.r9.s64 + 31;
	// lbz r8,10942(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 10942);
	// lwz r6,14888(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14888);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r30,r9,0,0,26
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFE0;
	// ori r9,r8,4
	ctx.r9.u64 = ctx.r8.u64 | 4;
	// add r31,r10,r30
	r31.u64 = ctx.r10.u64 + r30.u64;
	// stw r11,14904(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14904, r11.u32);
	// stw r6,14908(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14908, ctx.r6.u32);
	// stb r9,10942(r3)
	PPC_STORE_U8(ctx.r3.u32 + 10942, ctx.r9.u8);
loc_8219C998:
	// lwz r8,13216(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13216);
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq 0x8219c9e4
	if (cr0.getEQ()) goto loc_8219C9E4;
	// lwz r11,13220(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 13220);
	// lwz r10,14904(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14904);
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// cmpwi cr6,r9,-1
	cr6.compare<int32_t>(ctx.r9.s32, -1, xer);
	// beq cr6,0x8219c9c8
	if (cr6.getEQ()) goto loc_8219C9C8;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x8219c9e4
	if (!cr6.getEQ()) goto loc_8219C9E4;
	// cmplw cr6,r8,r31
	cr6.compare<uint32_t>(ctx.r8.u32, r31.u32, xer);
	// bge cr6,0x8219c9e4
	if (!cr6.getLT()) goto loc_8219C9E4;
loc_8219C9C8:
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x8219c894
	if (!cr6.getEQ()) goto loc_8219C894;
	// subf r11,r30,r31
	r11.s64 = r31.s64 - r30.s64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// blt cr6,0x8219c894
	if (cr6.getLT()) goto loc_8219C894;
loc_8219C9E4:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r5,14904(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14904);
	// bl 0x8219c338
	sub_8219C338(ctx, base);
	// subf r11,r30,r31
	r11.s64 = r31.s64 - r30.s64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
loc_8219CA04:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219CA10"))) PPC_WEAK_FUNC(sub_8219CA10);
PPC_FUNC_IMPL(__imp__sub_8219CA10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219ca3c
	if (cr0.getEQ()) goto loc_8219CA3C;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8219cae4
	goto loc_8219CAE4;
loc_8219CA3C:
	// rlwinm r11,r4,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,13500(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 13500);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bne 0x8219ca64
	if (!cr0.getEQ()) goto loc_8219CA64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219c808
	sub_8219C808(ctx, base);
	// b 0x8219ca80
	goto loc_8219CA80;
loc_8219CA64:
	// lwz r11,152(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8219ca7c
	if (cr6.getEQ()) goto loc_8219CA7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219b9b8
	sub_8219B9B8(ctx, base);
	// b 0x8219ca80
	goto loc_8219CA80;
loc_8219CA7C:
	// bl 0x8219bc28
	sub_8219BC28(ctx, base);
loc_8219CA80:
	// lwz r11,14916(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14916);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r9,14920(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14920);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// stw r11,14916(r31)
	PPC_STORE_U32(r31.u32 + 14916, r11.u32);
	// ble cr6,0x8219cad0
	if (!cr6.getGT()) goto loc_8219CAD0;
	// lwz r11,13216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13216);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8219cad0
	if (!cr6.getEQ()) goto loc_8219CAD0;
	// lwz r10,14908(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14908);
	// lwz r9,14888(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 14888);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// addi r10,r11,160
	ctx.r10.s64 = r11.s64 + 160;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// stw r10,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r10.u32);
	// beq cr6,0x8219cad0
	if (cr6.getEQ()) goto loc_8219CAD0;
	// lwz r11,14892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14892);
	// stw r11,14900(r31)
	PPC_STORE_U32(r31.u32 + 14900, r11.u32);
loc_8219CAD0:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x8219cae4
	if (!cr6.getEQ()) goto loc_8219CAE4;
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// ori r11,r11,32
	r11.u64 = r11.u64 | 32;
	// stb r11,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, r11.u8);
loc_8219CAE4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219CAF8"))) PPC_WEAK_FUNC(sub_8219CAF8);
PPC_FUNC_IMPL(__imp__sub_8219CAF8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed11c
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// lbz r11,10941(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 10941);
	// lwz r25,14884(r26)
	r25.u64 = PPC_LOAD_U32(r26.u32 + 14884);
	// lwz r24,14880(r26)
	r24.u64 = PPC_LOAD_U32(r26.u32 + 14880);
	// rlwinm. r11,r11,0,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219cb9c
	if (cr0.getEQ()) goto loc_8219CB9C;
	// lwz r11,21516(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21516);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8219ccd4
	if (cr6.getEQ()) goto loc_8219CCD4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8219ccd4
	if (cr6.getEQ()) goto loc_8219CCD4;
	// lis r21,8192
	r21.s64 = 536870912;
	// lis r22,16384
	r22.s64 = 1073741824;
	// lis r23,16640
	r23.s64 = 1090519040;
loc_8219CB44:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplw cr6,r11,r21
	cr6.compare<uint32_t>(r11.u32, r21.u32, xer);
	// clrlwi r5,r10,8
	ctx.r5.u64 = ctx.r10.u32 & 0xFFFFFF;
	// subf r4,r22,r11
	ctx.r4.s64 = r11.s64 - r22.s64;
	// blt cr6,0x8219cb60
	if (cr6.getLT()) goto loc_8219CB60;
	// subf r4,r23,r11
	ctx.r4.s64 = r11.s64 - r23.s64;
loc_8219CB60:
	// lwz r3,21516(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 21516);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,21516(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 21516);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addic. r31,r31,-1
	xer.ca = r31.u32 > 0;
	r31.s64 = r31.s64 + -1;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// bne 0x8219cb44
	if (!cr0.getEQ()) goto loc_8219CB44;
	// b 0x8219ccd4
	goto loc_8219CCD4;
loc_8219CB9C:
	// lwz r29,10952(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + 10952);
	// mulli r5,r31,3
	ctx.r5.s64 = r31.s64 * 3;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8219c3d0
	sub_8219C3D0(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x8219cc7c
	if (cr6.getEQ()) goto loc_8219CC7C;
	// mr r27,r31
	r27.u64 = r31.u64;
	// lis r21,8192
	r21.s64 = 536870912;
	// lis r22,16384
	r22.s64 = 1073741824;
	// lis r23,16640
	r23.s64 = 1090519040;
loc_8219CBC8:
	// lwz r3,21516(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 21516);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// clrlwi r28,r11,8
	r28.u64 = r11.u32 & 0xFFFFFF;
	// beq 0x8219cc08
	if (cr0.getEQ()) goto loc_8219CC08;
	// cmplw cr6,r31,r21
	cr6.compare<uint32_t>(r31.u32, r21.u32, xer);
	// subf r4,r22,r31
	ctx.r4.s64 = r31.s64 - r22.s64;
	// blt cr6,0x8219cbf0
	if (cr6.getLT()) goto loc_8219CBF0;
	// subf r4,r23,r31
	ctx.r4.s64 = r31.s64 - r23.s64;
loc_8219CBF0:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8219CC08:
	// lwz r11,21656(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21656);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8219cc38
	if (cr0.getEQ()) goto loc_8219CC38;
	// cmplw cr6,r31,r21
	cr6.compare<uint32_t>(r31.u32, r21.u32, xer);
	// subf r4,r22,r31
	ctx.r4.s64 = r31.s64 - r22.s64;
	// blt cr6,0x8219cc24
	if (cr6.getLT()) goto loc_8219CC24;
	// subf r4,r23,r31
	ctx.r4.s64 = r31.s64 - r23.s64;
loc_8219CC24:
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8219CC38:
	// lis r9,-16383
	ctx.r9.s64 = -1073676288;
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// ori r9,r9,16128
	ctx.r9.u64 = ctx.r9.u64 | 16128;
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// addic. r27,r27,-1
	xer.ca = r27.u32 > 0;
	r27.s64 = r27.s64 + -1;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// and r11,r11,r25
	r11.u64 = r11.u64 & r25.u64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// stwx r9,r10,r24
	PPC_STORE_U32(ctx.r10.u32 + r24.u32, ctx.r9.u32);
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// and r11,r10,r25
	r11.u64 = ctx.r10.u64 & r25.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwx r31,r9,r24
	PPC_STORE_U32(ctx.r9.u32 + r24.u32, r31.u32);
	// and r29,r11,r25
	r29.u64 = r11.u64 & r25.u64;
	// stwx r28,r10,r24
	PPC_STORE_U32(ctx.r10.u32 + r24.u32, r28.u32);
	// bne 0x8219cbc8
	if (!cr0.getEQ()) goto loc_8219CBC8;
loc_8219CC7C:
	// stw r29,10952(r26)
	PPC_STORE_U32(r26.u32 + 10952, r29.u32);
	// sync 
	// lis r11,32712
	r11.s64 = 2143813632;
	// stw r29,1812(r11)
	PPC_MM_STORE_U32(r11.u32 + 1812, r29.u32);
	// eieio 
	// sync 
	// lwz r3,21516(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 21516);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x8219ccb0
	if (cr0.getEQ()) goto loc_8219CCB0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8219CCB0:
	// lwz r11,21656(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21656);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8219ccd4
	if (cr0.getEQ()) goto loc_8219CCD4;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,2
	ctx.r3.s64 = 2;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_8219CCD4:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x823ed16c
	return;
}

__attribute__((alias("__imp__sub_8219CCE0"))) PPC_WEAK_FUNC(sub_8219CCE0);
PPC_FUNC_IMPL(__imp__sub_8219CCE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed118
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r10,-1
	ctx.r10.s64 = -1;
	// mr r21,r4
	r21.u64 = ctx.r4.u64;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// ld r11,11816(r29)
	r11.u64 = PPC_LOAD_U64(r29.u32 + 11816);
	// addi r27,r29,11816
	r27.s64 = r29.s64 + 11816;
	// li r26,0
	r26.s64 = 0;
	// cmpld cr6,r11,r10
	cr6.compare<uint64_t>(r11.u64, ctx.r10.u64, xer);
	// li r28,1
	r28.s64 = 1;
	// bne cr6,0x8219cd1c
	if (!cr6.getEQ()) goto loc_8219CD1C;
	// mr r28,r26
	r28.u64 = r26.u64;
loc_8219CD1C:
	// lbz r11,10942(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 10942);
	// rlwinm r24,r11,30,31,31
	r24.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 30) & 0x1;
	// add r11,r24,r28
	r11.u64 = r24.u64 + r28.u64;
	// mulli r23,r11,11
	r23.s64 = r11.s64 * 11;
	// cmplwi r23,0
	cr0.compare<uint32_t>(r23.u32, 0, xer);
	// bne 0x8219cd3c
	if (!cr0.getEQ()) goto loc_8219CD3C;
loc_8219CD34:
	// stw r26,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r26.u32);
	// b 0x8219ce98
	goto loc_8219CE98;
loc_8219CD3C:
	// li r5,32
	ctx.r5.s64 = 32;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8219ca10
	sub_8219CA10(ctx, base);
	// mr. r25,r3
	r25.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r25.s32, 0, xer);
	// beq 0x8219cd34
	if (cr0.getEQ()) goto loc_8219CD34;
	// lis r11,1
	r11.s64 = 65536;
	// addi r7,r25,-4
	ctx.r7.s64 = r25.s64 + -4;
	// ori r30,r11,2607
	r30.u64 = r11.u64 | 2607;
	// lis r11,-16380
	r11.s64 = -1073479680;
	// lis r6,-32768
	ctx.r6.s64 = -2147483648;
	// ori r31,r11,15360
	r31.u64 = r11.u64 | 15360;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x8219cdf4
	if (cr6.getEQ()) goto loc_8219CDF4;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8219c5e0
	sub_8219C5E0(ctx, base);
	// li r11,2609
	r11.s64 = 2609;
	// lis r8,768
	ctx.r8.s64 = 50331648;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// rlwinm r10,r10,0,0,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFF000;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stwu r11,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r7.u32 = ea;
	// li r3,2609
	ctx.r3.s64 = 2609;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r28,r26
	r28.u64 = r26.u64;
	// mr r20,r6
	r20.u64 = ctx.r6.u64;
	// addi r9,r11,4095
	ctx.r9.s64 = r11.s64 + 4095;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// rlwinm r9,r9,0,0,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFF000;
	// li r7,3
	ctx.r7.s64 = 3;
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// li r27,8
	r27.s64 = 8;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r5,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r4,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	r11.u32 = ea;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r3,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	r11.u32 = ea;
	// stwu r28,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r28.u32);
	r11.u32 = ea;
	// stwu r20,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r20.u32);
	r11.u32 = ea;
	// stwu r27,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r27.u32);
	r11.u32 = ea;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
loc_8219CDF4:
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x8219ce7c
	if (cr6.getEQ()) goto loc_8219CE7C;
	// li r9,2609
	ctx.r9.s64 = 2609;
	// lwz r11,14888(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 14888);
	// lwz r10,14892(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 14892);
	// lis r5,256
	ctx.r5.s64 = 16777216;
	// rlwinm r8,r11,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// li r4,3
	ctx.r4.s64 = 3;
	// li r3,2609
	ctx.r3.s64 = 2609;
	// stwu r9,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r7.u32 = ea;
	// clrlwi r9,r11,3
	ctx.r9.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r8,512
	r11.s64 = ctx.r8.s64 + 512;
	// rlwinm r8,r10,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// addi r8,r8,512
	ctx.r8.s64 = ctx.r8.s64 + 512;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// stwu r5,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r7.u32 = ea;
	// clrlwi r10,r10,3
	ctx.r10.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// rlwinm r9,r8,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x1000;
	// rlwinm r11,r11,0,0,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFF000;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stwu r30,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, r30.u32);
	ctx.r7.u32 = ea;
	// li r29,8
	r29.s64 = 8;
	// addi r10,r10,4095
	ctx.r10.s64 = ctx.r10.s64 + 4095;
	// rlwinm r10,r10,0,0,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFF000;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// stwu r10,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r7.u32 = ea;
	// stwu r11,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r7.u32 = ea;
	// stwu r31,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, r31.u32);
	ctx.r7.u32 = ea;
	// stwu r4,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r7.u32 = ea;
	// stwu r3,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	ctx.r7.u32 = ea;
	// stwu r26,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r7.u32 = ea;
	// stwu r6,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r7.u32 = ea;
	// stwu r29,4(r7)
	ea = 4 + ctx.r7.u32;
	PPC_STORE_U32(ea, r29.u32);
	ctx.r7.u32 = ea;
loc_8219CE7C:
	// rlwinm r11,r25,12,20,31
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r25,3
	ctx.r10.u64 = r25.u32 & 0x1FFFFFFF;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// stw r23,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r23.u32);
loc_8219CE98:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x823ed168
	return;
}

__attribute__((alias("__imp__sub_8219CEA0"))) PPC_WEAK_FUNC(sub_8219CEA0);
PPC_FUNC_IMPL(__imp__sub_8219CEA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lis r9,-16384
	ctx.r9.s64 = -1073741824;
	// lis r8,-16382
	ctx.r8.s64 = -1073610752;
	// ori r31,r9,15104
	r31.u64 = ctx.r9.u64 | 15104;
	// addi r10,r4,-4
	ctx.r10.s64 = ctx.r4.s64 + -4;
	// lwz r9,10896(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 10896);
	// ori r8,r8,22528
	ctx.r8.u64 = ctx.r8.u64 | 22528;
	// lwz r7,14904(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 14904);
	// li r3,32767
	ctx.r3.s64 = 32767;
	// lwz r6,48(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm r4,r9,12,20,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// clrlwi r26,r7,30
	r26.u64 = ctx.r7.u32 & 0x3;
	// lwz r5,10908(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 10908);
	// addi r4,r4,512
	ctx.r4.s64 = ctx.r4.s64 + 512;
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// stw r7,13228(r11)
	PPC_STORE_U32(r11.u32 + 13228, ctx.r7.u32);
	// mr r27,r8
	r27.u64 = ctx.r8.u64;
	// stw r6,13224(r11)
	PPC_STORE_U32(r11.u32 + 13224, ctx.r6.u32);
	// or r7,r26,r6
	ctx.r7.u64 = r26.u64 | ctx.r6.u64;
	// addi r8,r9,4
	ctx.r8.s64 = ctx.r9.s64 + 4;
	// stwu r31,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r31.u32);
	ctx.r10.u32 = ea;
	// clrlwi r6,r9,3
	ctx.r6.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r9,r4,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x1000;
	// li r30,3
	r30.s64 = 3;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stwu r3,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	ctx.r10.u32 = ea;
	// li r28,3
	r28.s64 = 3;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r8,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// addi r9,r9,512
	ctx.r9.s64 = ctx.r9.s64 + 512;
	// stwu r29,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r29.u32);
	ctx.r10.u32 = ea;
	// rlwinm r9,r9,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1000;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r30,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r30.u32);
	ctx.r10.u32 = ea;
	// ori r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 2;
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// stwu r7,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r10.u32 = ea;
	// stwu r27,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r10.u32 = ea;
	// stwu r28,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r10.u32 = ea;
	// stwu r6,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r10.u32 = ea;
	// stwu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r10.u32 = ea;
	// lwz r9,21516(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 21516);
	// addi r3,r10,4
	ctx.r3.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8219cf7c
	if (!cr6.getEQ()) goto loc_8219CF7C;
	// lbz r10,10941(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 10941);
	// rlwinm. r10,r10,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8219cf7c
	if (cr0.getEQ()) goto loc_8219CF7C;
	// lwz r10,10896(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 10896);
	// stw r5,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r5.u32);
	// lwz r10,10896(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 10896);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
loc_8219CF7C:
	// addi r10,r5,2
	ctx.r10.s64 = ctx.r5.s64 + 2;
	// stw r10,10908(r11)
	PPC_STORE_U32(r11.u32 + 10908, ctx.r10.u32);
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_8219CF88"))) PPC_WEAK_FUNC(sub_8219CF88);
PPC_FUNC_IMPL(__imp__sub_8219CF88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed124
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// lwz r11,11000(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 11000);
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8219d024
	if (cr6.getEQ()) goto loc_8219D024;
	// addi r24,r30,11004
	r24.s64 = r30.s64 + 11004;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x8240f8dc
	__imp__KfAcquireSpinLock(ctx, base);
	// lwz r11,11000(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 11000);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8219d010
	if (cr6.getEQ()) goto loc_8219D010;
	// oris r11,r27,33024
	r11.u64 = r27.u64 | 2164260864;
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x8219c738
	sub_8219C738(ctx, base);
	// lwz r11,11000(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 11000);
	// li r29,1
	r29.s64 = 1;
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// stw r11,11000(r30)
	PPC_STORE_U32(r30.u32 + 11000, r11.u32);
loc_8219D010:
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x8240f8cc
	__imp__KfReleaseSpinLock(ctx, base);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x8219d050
	if (!cr6.getEQ()) goto loc_8219D050;
loc_8219D024:
	// lwz r11,11000(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 11000);
	// clrlwi r10,r27,8
	ctx.r10.u64 = r27.u32 & 0xFFFFFF;
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// oris r10,r10,33024
	ctx.r10.u64 = ctx.r10.u64 | 2164260864;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,11000(r30)
	PPC_STORE_U32(r30.u32 + 11000, r11.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// bl 0x8219caf8
	sub_8219CAF8(ctx, base);
loc_8219D050:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x823ed174
	return;
}

__attribute__((alias("__imp__sub_8219D060"))) PPC_WEAK_FUNC(sub_8219D060);
PPC_FUNC_IMPL(__imp__sub_8219D060) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8219d11c
	if (cr6.getEQ()) goto loc_8219D11C;
	// lwz r10,10896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// subf r9,r30,r11
	ctx.r9.s64 = r11.s64 - r30.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x8219d11c
	if (!cr6.getLT()) goto loc_8219D11C;
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x8219d0b8
	if (!cr6.getEQ()) goto loc_8219D0B8;
	// lwz r11,13216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13216);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8219d11c
	if (!cr6.getEQ()) goto loc_8219D11C;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_8219D0B8:
	// lwz r10,10896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// subf r9,r30,r11
	ctx.r9.s64 = r11.s64 - r30.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x8219d11c
	if (!cr6.getLT()) goto loc_8219D11C;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198a90
	sub_82198A90(ctx, base);
	// b 0x8219d0f8
	goto loc_8219D0F8;
loc_8219D0E8:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198c28
	sub_82198C28(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x8219d114
	if (cr0.getEQ()) goto loc_8219D114;
loc_8219D0F8:
	// lwz r10,10896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// subf r9,r30,r11
	ctx.r9.s64 = r11.s64 - r30.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x8219d0e8
	if (cr6.getLT()) goto loc_8219D0E8;
loc_8219D114:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198ac0
	sub_82198AC0(ctx, base);
loc_8219D11C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219D128"))) PPC_WEAK_FUNC(sub_8219D128);
PPC_FUNC_IMPL(__imp__sub_8219D128) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// rlwinm. r8,r5,0,29,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x6;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r10,r5,8,26,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0x3F;
	// bne 0x8219d148
	if (!cr0.getEQ()) goto loc_8219D148;
	// ori r5,r5,6
	ctx.r5.u64 = ctx.r5.u64 | 6;
loc_8219D148:
	// rlwinm. r8,r5,0,30,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// li r3,-1
	ctx.r3.s64 = -1;
	// beq 0x8219d210
	if (cr0.getEQ()) goto loc_8219D210;
	// lbz r8,10943(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 10943);
	// clrlwi. r4,r5,31
	ctx.r4.u64 = ctx.r5.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ori r8,r8,8
	ctx.r8.u64 = ctx.r8.u64 | 8;
	// stb r8,10943(r11)
	PPC_STORE_U8(r11.u32 + 10943, ctx.r8.u8);
	// bne 0x8219d178
	if (!cr0.getEQ()) goto loc_8219D178;
	// li r8,1480
	ctx.r8.s64 = 1480;
	// lis r4,2
	ctx.r4.s64 = 131072;
	// stwu r8,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r9.u32 = ea;
	// stwu r4,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r9.u32 = ea;
loc_8219D178:
	// lis r8,1
	ctx.r8.s64 = 65536;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ori r8,r8,1404
	ctx.r8.u64 = ctx.r8.u64 | 1404;
	// stwu r8,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r9.u32 = ea;
	// stwu r6,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r9.u32 = ea;
	// stwu r7,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r9.u32 = ea;
	// bne cr6,0x8219d198
	if (!cr6.getEQ()) goto loc_8219D198;
	// li r10,4
	ctx.r10.s64 = 4;
loc_8219D198:
	// li r8,1400
	ctx.r8.s64 = 1400;
	// li r7,19
	ctx.r7.s64 = 19;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r31,256
	r31.s64 = 256;
	// li r30,1118
	r30.s64 = 1118;
	// stwu r8,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r9.u32 = ea;
	// stwu r10,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r9.u32 = ea;
	// lwz r8,12708(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 12708);
	// oris r8,r8,49156
	ctx.r8.u64 = ctx.r8.u64 | 3221487616;
	// ori r8,r8,15360
	ctx.r8.u64 = ctx.r8.u64 | 15360;
	// stwu r8,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r9.u32 = ea;
	// stwu r7,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r9.u32 = ea;
	// lwz r8,10900(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 10900);
	// rlwinm r6,r8,12,20,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// clrlwi r7,r8,3
	ctx.r7.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// addi r8,r6,512
	ctx.r8.s64 = ctx.r6.s64 + 512;
	// rlwinm r8,r8,0,19,19
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x1000;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// ori r8,r8,2
	ctx.r8.u64 = ctx.r8.u64 | 2;
	// stwu r8,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r9.u32 = ea;
	// stwu r10,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r9.u32 = ea;
	// stwu r4,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r9.u32 = ea;
	// stwu r31,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, r31.u32);
	ctx.r9.u32 = ea;
	// lwz r8,12708(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 12708);
	// oris r8,r8,49152
	ctx.r8.u64 = ctx.r8.u64 | 3221225472;
	// ori r8,r8,21504
	ctx.r8.u64 = ctx.r8.u64 | 21504;
	// stwu r8,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r9.u32 = ea;
	// stwu r10,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r9.u32 = ea;
	// stwu r30,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, r30.u32);
	ctx.r9.u32 = ea;
	// stwu r10,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r9.u32 = ea;
loc_8219D210:
	// rlwinm. r10,r5,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x8219d288
	if (cr0.getEQ()) goto loc_8219D288;
	// lbz r10,10943(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 10943);
	// lis r8,-16380
	ctx.r8.s64 = -1073479680;
	// li r7,19
	ctx.r7.s64 = 19;
	// ori r8,r8,15361
	ctx.r8.u64 = ctx.r8.u64 | 15361;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,1404
	ctx.r4.s64 = 1404;
	// andi. r10,r10,247
	ctx.r10.u64 = ctx.r10.u64 & 247;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stb r10,10943(r11)
	PPC_STORE_U8(r11.u32 + 10943, ctx.r10.u8);
	// lis r10,2989
	ctx.r10.s64 = 195887104;
	// stwu r8,4(r9)
	ea = 4 + ctx.r9.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r9.u32 = ea;
	// ori r31,r10,61453
	r31.u64 = ctx.r10.u64 | 61453;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stwu r7,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r10.u32 = ea;
	// lwz r11,10900(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 10900);
	// rlwinm r8,r11,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r11,3
	ctx.r9.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r8,512
	r11.s64 = ctx.r8.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// stwu r11,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r10.u32 = ea;
	// stwu r6,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r10.u32 = ea;
	// stwu r3,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	ctx.r10.u32 = ea;
	// stwu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r10.u32 = ea;
	// stwu r4,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r10.u32 = ea;
	// stwu r31,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r31.u32);
	ctx.r10.u32 = ea;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
loc_8219D288:
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219D298"))) PPC_WEAK_FUNC(sub_8219D298);
PPC_FUNC_IMPL(__imp__sub_8219D298) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,6
	ctx.r5.s64 = 6;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x8219d060
	sub_8219D060(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8219D2B8"))) PPC_WEAK_FUNC(sub_8219D2B8);
PPC_FUNC_IMPL(__imp__sub_8219D2B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// li r5,5
	ctx.r5.s64 = 5;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lwz r4,8(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x8219d060
	sub_8219D060(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8219D2D8"))) PPC_WEAK_FUNC(sub_8219D2D8);
PPC_FUNC_IMPL(__imp__sub_8219D2D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lis r29,16384
	r29.s64 = 1073741824;
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq 0x8219d318
	if (cr0.getEQ()) goto loc_8219D318;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
loc_8219D318:
	// li r5,128
	ctx.r5.s64 = 128;
	// lwz r3,16(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// li r4,4224
	ctx.r4.s64 = 4224;
	// bl 0x8219ca10
	sub_8219CA10(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8219d344
	if (!cr0.getEQ()) goto loc_8219D344;
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r31,16712(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 16712);
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// b 0x8219d3b8
	goto loc_8219D3B8;
loc_8219D344:
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8219d390
	if (cr0.getEQ()) goto loc_8219D390;
	// rlwinm r10,r31,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r31,3
	ctx.r9.u64 = r31.u32 & 0x1FFFFFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - r29.s64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// b 0x8219d394
	goto loc_8219D394;
loc_8219D390:
	// stw r31,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r31.u32);
loc_8219D394:
	// rlwinm r11,r31,12,20,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r31,3
	ctx.r10.u64 = r31.u32 & 0x1FFFFFFF;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r3,r29,r11
	ctx.r3.s64 = r11.s64 - r29.s64;
	// addi r4,r3,4224
	ctx.r4.s64 = ctx.r3.s64 + 4224;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
loc_8219D3B8:
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// stw r31,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r31.u32);
	// addi r11,r31,4220
	r11.s64 = r31.s64 + 4220;
	// stw r3,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r3.u32);
	// stw r11,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219D3D8"))) PPC_WEAK_FUNC(sub_8219D3D8);
PPC_FUNC_IMPL(__imp__sub_8219D3D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lbz r10,10941(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// rlwinm. r11,r10,0,26,26
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219d41c
	if (cr0.getEQ()) goto loc_8219D41C;
	// ori r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 | 32;
	// lwz r11,16712(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16712);
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r10,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, ctx.r10.u8);
	// addi r10,r11,4800
	ctx.r10.s64 = r11.s64 + 4800;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// addi r11,r10,-160
	r11.s64 = ctx.r10.s64 + -160;
	// stw r10,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r10.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// b 0x8219d528
	goto loc_8219D528;
loc_8219D41C:
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x8219d430
	if (!cr6.getEQ()) goto loc_8219D430;
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x8219d438
	goto loc_8219D438;
loc_8219D430:
	// addi r11,r4,14
	r11.s64 = ctx.r4.s64 + 14;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_8219D438:
	// lwz r3,13500(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 13500);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x8219d460
	if (cr0.getEQ()) goto loc_8219D460;
	// lwz r10,172(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 172);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8219d460
	if (cr6.getEQ()) goto loc_8219D460;
	// lwz r10,14896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14896);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x8219d460
	if (cr6.getGT()) goto loc_8219D460;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_8219D460:
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,2
	ctx.r4.s64 = 2;
	// bne 0x8219d484
	if (!cr0.getEQ()) goto loc_8219D484;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219c808
	sub_8219C808(ctx, base);
	// b 0x8219d4a0
	goto loc_8219D4A0;
loc_8219D484:
	// lwz r11,152(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8219d49c
	if (cr6.getEQ()) goto loc_8219D49C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219b9b8
	sub_8219B9B8(ctx, base);
	// b 0x8219d4a0
	goto loc_8219D4A0;
loc_8219D49C:
	// bl 0x8219bc28
	sub_8219BC28(ctx, base);
loc_8219D4A0:
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8219d4c4
	if (!cr6.getEQ()) goto loc_8219D4C4;
	// lbz r10,10941(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// lwz r11,16712(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16712);
	// ori r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 | 32;
	// stb r10,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, ctx.r10.u8);
	// addi r10,r11,4800
	ctx.r10.s64 = r11.s64 + 4800;
	// b 0x8219d4e8
	goto loc_8219D4E8;
loc_8219D4C4:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r30,-4
	r11.s64 = r30.s64 + -4;
	// stw r30,14912(r31)
	PPC_STORE_U32(r31.u32 + 14912, r30.u32);
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// stw r29,14900(r31)
	PPC_STORE_U32(r31.u32 + 14900, r29.u32);
	// stw r29,14916(r31)
	PPC_STORE_U32(r31.u32 + 14916, r29.u32);
	// addi r10,r10,-14
	ctx.r10.s64 = ctx.r10.s64 + -14;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
loc_8219D4E8:
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// addi r11,r10,-160
	r11.s64 = ctx.r10.s64 + -160;
	// stw r10,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r10.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// lwz r11,22280(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 22280);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8219d524
	if (!cr0.getEQ()) goto loc_8219D524;
	// lwz r4,14924(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 14924);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x8219d524
	if (cr0.getEQ()) goto loc_8219D524;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,3
	ctx.r5.s64 = 3;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d060
	sub_8219D060(ctx, base);
	// stw r29,14924(r31)
	PPC_STORE_U32(r31.u32 + 14924, r29.u32);
loc_8219D524:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_8219D528:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219D530"))) PPC_WEAK_FUNC(sub_8219D530);
PPC_FUNC_IMPL(__imp__sub_8219D530) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// stw r4,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r4.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x8219d2d8
	sub_8219D2D8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8219D550"))) PPC_WEAK_FUNC(sub_8219D550);
PPC_FUNC_IMPL(__imp__sub_8219D550) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lbz r10,10941(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r30,14912(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 14912);
	// addi r28,r11,4
	r28.s64 = r11.s64 + 4;
	// rlwinm. r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x8219d690
	if (!cr0.getEQ()) goto loc_8219D690;
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219d5ec
	if (cr0.getEQ()) goto loc_8219D5EC;
	// lwz r11,13500(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13500);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8219d690
	if (cr0.getEQ()) goto loc_8219D690;
	// lwz r11,152(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 152);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8219d690
	if (!cr6.getEQ()) goto loc_8219D690;
	// subf r11,r30,r28
	r11.s64 = r28.s64 - r30.s64;
	// srawi. r29,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r29.s64 = r11.s32 >> 2;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x8219d690
	if (cr0.getEQ()) goto loc_8219D690;
	// lwz r3,13520(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 13520);
	// lwz r11,13524(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13524);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// blt cr6,0x8219d5c0
	if (cr6.getLT()) goto loc_8219D5C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219bb78
	sub_8219BB78(ctx, base);
loc_8219D5C0:
	// rlwinm r11,r30,12,20,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r30,3
	ctx.r10.u64 = r30.u32 & 0x1FFFFFFF;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// oris r9,r29,33024
	ctx.r9.u64 = r29.u64 | 2164260864;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// addi r8,r3,8
	ctx.r8.s64 = ctx.r3.s64 + 8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r8,13520(r31)
	PPC_STORE_U32(r31.u32 + 13520, ctx.r8.u32);
	// b 0x8219d690
	goto loc_8219D690;
loc_8219D5EC:
	// lwz r11,13216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13216);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8219d650
	if (cr6.getEQ()) goto loc_8219D650;
	// rlwinm r11,r30,12,20,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 12) & 0xFFF;
	// subf r10,r30,r28
	ctx.r10.s64 = r28.s64 - r30.s64;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// srawi. r29,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r29.s64 = ctx.r10.s32 >> 2;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// clrlwi r10,r30,3
	ctx.r10.u64 = r30.u32 & 0x1FFFFFFF;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// beq 0x8219d690
	if (cr0.getEQ()) goto loc_8219D690;
	// addi r3,r31,13352
	ctx.r3.s64 = r31.s64 + 13352;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r9,r11,8
	ctx.r9.s64 = r11.s64 + 8;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x8219d638
	if (!cr6.getGT()) goto loc_8219D638;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_8219D638:
	// oris r10,r29,33024
	ctx.r10.u64 = r29.u64 | 2164260864;
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// addi r9,r11,8
	ctx.r9.s64 = r11.s64 + 8;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r9,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r9.u32);
	// b 0x8219d690
	goto loc_8219D690;
loc_8219D650:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219cea0
	sub_8219CEA0(ctx, base);
	// rlwinm r11,r30,12,20,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r30,3
	ctx.r10.u64 = r30.u32 & 0x1FFFFFFF;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// addi r8,r31,13352
	ctx.r8.s64 = r31.s64 + 13352;
	// add r5,r11,r10
	ctx.r5.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r30,r4
	r11.s64 = ctx.r4.s64 - r30.s64;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// srawi r6,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r6.s64 = r11.s32 >> 2;
	// bl 0x8219cf88
	sub_8219CF88(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
loc_8219D690:
	// addi r11,r28,31
	r11.s64 = r28.s64 + 31;
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// rlwinm r9,r11,0,0,26
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x8219d6bc
	if (!cr6.getGT()) goto loc_8219D6BC;
	// addi r11,r28,-4
	r11.s64 = r28.s64 + -4;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x8219d3d8
	sub_8219D3D8(ctx, base);
	// b 0x8219d6cc
	goto loc_8219D6CC;
loc_8219D6BC:
	// rlwinm r11,r11,0,0,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFE0;
	// addi r10,r11,-4
	ctx.r10.s64 = r11.s64 + -4;
	// stw r11,14912(r31)
	PPC_STORE_U32(r31.u32 + 14912, r11.u32);
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
loc_8219D6CC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_8219D6D8"))) PPC_WEAK_FUNC(sub_8219D6D8);
PPC_FUNC_IMPL(__imp__sub_8219D6D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,10896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x8219d700
	if (cr0.getEQ()) goto loc_8219D700;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_8219D700:
	// lwz r11,13216(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13216);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8219d764
	if (!cr6.getEQ()) goto loc_8219D764;
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8219d764
	if (!cr0.getEQ()) goto loc_8219D764;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219cce0
	sub_8219CCE0(ctx, base);
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8219d764
	if (cr6.getEQ()) goto loc_8219D764;
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219ca10
	sub_8219CA10(ctx, base);
	// mr. r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq 0x8219d764
	if (cr0.getEQ()) goto loc_8219D764;
	// addi r8,r31,13352
	ctx.r8.s64 = r31.s64 + 13352;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219cf88
	sub_8219CF88(ctx, base);
loc_8219D764:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d550
	sub_8219D550(ctx, base);
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8219d7b8
	if (!cr0.getEQ()) goto loc_8219D7B8;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// lwz r11,-31396(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -31396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x8219d7b8
	if (cr6.getEQ()) goto loc_8219D7B8;
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// rlwinm. r11,r11,0,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8219d7b8
	if (!cr0.getEQ()) goto loc_8219D7B8;
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r11,-2
	ctx.r4.s64 = r11.s64 + -2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d060
	sub_8219D060(ctx, base);
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// stb r11,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, r11.u8);
loc_8219D7B8:
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219D7D8"))) PPC_WEAK_FUNC(sub_8219D7D8);
PPC_FUNC_IMPL(__imp__sub_8219D7D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,10908(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 10908);
	// stw r31,10928(r3)
	PPC_STORE_U32(ctx.r3.u32 + 10928, r31.u32);
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219D810"))) PPC_WEAK_FUNC(sub_8219D810);
PPC_FUNC_IMPL(__imp__sub_8219D810) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8219d870
	if (cr6.getEQ()) goto loc_8219D870;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x8219d850
	if (!cr6.getEQ()) goto loc_8219D850;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_8219D850:
	// lwz r10,10896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// subf r9,r30,r11
	ctx.r9.s64 = r11.s64 - r30.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// subfc r11,r11,r9
	xer.ca = ctx.r9.u32 >= r11.u32;
	r11.s64 = ctx.r9.s64 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// clrlwi r3,r11,31
	ctx.r3.u64 = r11.u32 & 0x1;
loc_8219D870:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219D888"))) PPC_WEAK_FUNC(sub_8219D888);
PPC_FUNC_IMPL(__imp__sub_8219D888) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x8219d8b4
	if (!cr6.getGT()) goto loc_8219D8B4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_8219D8B4:
	// li r11,1480
	r11.s64 = 1480;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// li r9,3332
	ctx.r9.s64 = 3332;
	// li r8,0
	ctx.r8.s64 = 0;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219D8F0"))) PPC_WEAK_FUNC(sub_8219D8F0);
PPC_FUNC_IMPL(__imp__sub_8219D8F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// rlwinm r30,r29,2,0,29
	r30.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,52(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8219d94c
	if (!cr6.getGT()) goto loc_8219D94C;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,52(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8219d94c
	if (!cr6.getGT()) goto loc_8219D94C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d3d8
	sub_8219D3D8(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// beq 0x8219d950
	if (cr0.getEQ()) goto loc_8219D950;
loc_8219D94C:
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
loc_8219D950:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219D958"))) PPC_WEAK_FUNC(sub_8219D958);
PPC_FUNC_IMPL(__imp__sub_8219D958) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8219d984
	if (!cr6.getGT()) goto loc_8219D984;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_8219D984:
	// li r10,1480
	ctx.r10.s64 = 1480;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,4
	ctx.r5.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// lwz r4,10908(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x8219d060
	sub_8219D060(ctx, base);
loc_8219D9AC:
	// lwz r11,11000(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 11000);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x8219d9ac
	if (!cr6.getEQ()) goto loc_8219D9AC;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219D9D0"))) PPC_WEAK_FUNC(sub_8219D9D0);
PPC_FUNC_IMPL(__imp__sub_8219D9D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// lwz r4,48(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// ble cr6,0x8219da04
	if (!cr6.getGT()) goto loc_8219DA04;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_8219DA04:
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d128
	sub_8219D128(ctx, base);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_8219DA28"))) PPC_WEAK_FUNC(sub_8219DA28);
PPC_FUNC_IMPL(__imp__sub_8219DA28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed128
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8219da58
	if (cr6.getEQ()) goto loc_8219DA58;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x8219da58
	if (cr6.getEQ()) goto loc_8219DA58;
	// bl 0x8219d958
	sub_8219D958(ctx, base);
loc_8219DA58:
	// lwz r10,14888(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 14888);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x8219daac
	if (cr0.getEQ()) goto loc_8219DAAC;
	// lwz r11,14892(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14892);
	// lis r9,16384
	ctx.r9.s64 = 1073741824;
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm r8,r11,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// rlwinm r9,r10,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// addi r8,r8,512
	ctx.r8.s64 = ctx.r8.s64 + 512;
	// addi r4,r9,512
	ctx.r4.s64 = ctx.r9.s64 + 512;
	// rlwinm r9,r8,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r11,3
	ctx.r8.u64 = r11.u32 & 0x1FFFFFFF;
	// rlwinm r11,r4,0,19,19
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x1000;
	// clrlwi r10,r10,3
	ctx.r10.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r4,r7,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r7.s64;
	// subf r3,r6,r11
	ctx.r3.s64 = r11.s64 - ctx.r6.s64;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
loc_8219DAAC:
	// lis r30,-20096
	r30.s64 = -1317011456;
	// lwz r3,14820(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14820);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,14824(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14824);
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// li r29,0
	r29.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r29,14820(r31)
	PPC_STORE_U32(r31.u32 + 14820, r29.u32);
	// stw r29,14824(r31)
	PPC_STORE_U32(r31.u32 + 14824, r29.u32);
	// stw r29,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r29.u32);
	// stw r29,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r29.u32);
	// bl 0x8240f9bc
	__imp__VdSetSystemCommandBufferGpuIdentifierAddress(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,10900(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 10900);
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,10896(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// stw r29,10900(r31)
	PPC_STORE_U32(r31.u32 + 10900, r29.u32);
	// stw r29,10896(r31)
	PPC_STORE_U32(r31.u32 + 10896, r29.u32);
	// bne cr6,0x8219db14
	if (!cr6.getEQ()) goto loc_8219DB14;
loc_8219DB0C:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8219de48
	goto loc_8219DE48;
loc_8219DB14:
	// lwz r26,4(r27)
	r26.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r28,8(r27)
	r28.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// lwz r25,12(r27)
	r25.u64 = PPC_LOAD_U32(r27.u32 + 12);
	// cmplwi r26,0
	cr0.compare<uint32_t>(r26.u32, 0, xer);
	// lwz r30,16(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// bne 0x8219db38
	if (!cr0.getEQ()) goto loc_8219DB38;
	// lis r26,0
	r26.s64 = 0;
	// ori r26,r26,32768
	r26.u64 = r26.u64 | 32768;
loc_8219DB38:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x8219db44
	if (!cr6.getEQ()) goto loc_8219DB44;
	// lis r25,32
	r25.s64 = 2097152;
loc_8219DB44:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x8219db50
	if (!cr6.getEQ()) goto loc_8219DB50;
	// li r11,32
	r11.s64 = 32;
loc_8219DB50:
	// divwu r27,r25,r11
	r27.u32 = r25.u32 / r11.u32;
	// twllei r11,0
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x8219db74
	if (!cr6.getEQ()) goto loc_8219DB74;
	// lis r4,-19072
	ctx.r4.s64 = -1249902592;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// stw r28,14820(r31)
	PPC_STORE_U32(r31.u32 + 14820, r28.u32);
loc_8219DB74:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x8219dba0
	if (!cr6.getEQ()) goto loc_8219DBA0;
	// lis r11,8
	r11.s64 = 524288;
	// lis r4,-30848
	ctx.r4.s64 = -2021654528;
	// subfc r11,r11,r25
	xer.ca = r25.u32 >= r11.u32;
	r11.s64 = r25.s64 - r11.s64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwimi r4,r11,28,2,3
	ctx.r4.u64 = (__builtin_rotateleft32(r11.u32, 28) & 0x30000000) | (ctx.r4.u64 & 0xFFFFFFFFCFFFFFFF);
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r30,14824(r31)
	PPC_STORE_U32(r31.u32 + 14824, r30.u32);
loc_8219DBA0:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x8219de40
	if (cr6.getEQ()) goto loc_8219DE40;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x8219de40
	if (cr6.getEQ()) goto loc_8219DE40;
	// lis r4,-23168
	ctx.r4.s64 = -1518338048;
	// li r3,96
	ctx.r3.s64 = 96;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,10896(r31)
	PPC_STORE_U32(r31.u32 + 10896, ctx.r3.u32);
	// beq 0x8219de40
	if (cr0.getEQ()) goto loc_8219DE40;
	// lis r4,-27264
	ctx.r4.s64 = -1786773504;
	// li r3,32
	ctx.r3.s64 = 32;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,10900(r31)
	PPC_STORE_U32(r31.u32 + 10900, ctx.r3.u32);
	// beq 0x8219de40
	if (cr0.getEQ()) goto loc_8219DE40;
	// li r5,96
	ctx.r5.s64 = 96;
	// lwz r3,10896(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,10900(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 10900);
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// cntlzw r11,r26
	r11.u64 = r26.u32 == 0 ? 32 : __builtin_clz(r26.u32);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// subfic r24,r11,28
	xer.ca = r11.u32 <= 28;
	r24.s64 = 28 - r11.s64;
	// bl 0x8240f9ac
	__imp__MmGetPhysicalAddress(ctx, base);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// bl 0x8240f99c
	__imp__VdInitializeRingBuffer(ctx, base);
	// rlwinm r11,r26,23,9,31
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 23) & 0x7FFFFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r4,r11,31
	xer.ca = r11.u32 <= 31;
	ctx.r4.s64 = 31 - r11.s64;
	// cmplwi cr6,r4,19
	cr6.compare<uint32_t>(ctx.r4.u32, 19, xer);
	// ble cr6,0x8219dc30
	if (!cr6.getGT()) goto loc_8219DC30;
	// li r4,19
	ctx.r4.s64 = 19;
loc_8219DC30:
	// lwz r11,10896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// addi r11,r11,60
	r11.s64 = r11.s64 + 60;
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x8240f98c
	__imp__VdEnableRingBufferRPtrWriteBack(ctx, base);
	// lis r9,2989
	ctx.r9.s64 = 195887104;
	// rlwinm r11,r25,30,2,31
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 30) & 0x3FFFFFFF;
	// ori r8,r9,61453
	ctx.r8.u64 = ctx.r9.u64 | 61453;
	// rlwinm r9,r26,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r10,r27,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFC;
	// addi r7,r9,-1
	ctx.r7.s64 = ctx.r9.s64 + -1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// add r9,r9,r30
	ctx.r9.u64 = ctx.r9.u64 + r30.u64;
	// addi r6,r30,-4
	ctx.r6.s64 = r30.s64 + -4;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// addi r5,r10,-160
	ctx.r5.s64 = ctx.r10.s64 + -160;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r8,10896(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// stw r11,14920(r31)
	PPC_STORE_U32(r31.u32 + 14920, r11.u32);
	// stw r28,14880(r31)
	PPC_STORE_U32(r31.u32 + 14880, r28.u32);
	// stw r30,14888(r31)
	PPC_STORE_U32(r31.u32 + 14888, r30.u32);
	// stw r30,14908(r31)
	PPC_STORE_U32(r31.u32 + 14908, r30.u32);
	// stw r9,14892(r31)
	PPC_STORE_U32(r31.u32 + 14892, ctx.r9.u32);
	// stw r30,14912(r31)
	PPC_STORE_U32(r31.u32 + 14912, r30.u32);
	// stw r27,14896(r31)
	PPC_STORE_U32(r31.u32 + 14896, r27.u32);
	// stw r7,14884(r31)
	PPC_STORE_U32(r31.u32 + 14884, ctx.r7.u32);
	// stw r29,60(r8)
	PPC_STORE_U32(ctx.r8.u32 + 60, r29.u32);
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// stw r29,10952(r31)
	PPC_STORE_U32(r31.u32 + 10952, r29.u32);
	// stw r6,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r6.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r10,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r10.u32);
	// stw r5,56(r31)
	PPC_STORE_U32(r31.u32 + 56, ctx.r5.u32);
	// stw r29,14900(r31)
	PPC_STORE_U32(r31.u32 + 14900, r29.u32);
	// bne cr6,0x8219dcd4
	if (!cr6.getEQ()) goto loc_8219DCD4;
	// li r11,3
	r11.s64 = 3;
	// stw r11,10908(r31)
	PPC_STORE_U32(r31.u32 + 10908, r11.u32);
loc_8219DCD4:
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// lwz r10,10896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,14904(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14904);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// lwz r9,10896(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// lwz r11,10896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// addi r3,r11,8
	ctx.r3.s64 = r11.s64 + 8;
	// bl 0x8240f9bc
	__imp__VdSetSystemCommandBufferGpuIdentifierAddress(ctx, base);
	// lis r10,-16367
	ctx.r10.s64 = -1072627712;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r29,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r29.u32);
	// ori r10,r10,18432
	ctx.r10.u64 = ctx.r10.u64 | 18432;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// addi r11,r11,-17484
	r11.s64 = r11.s64 + -17484;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// li r10,1023
	ctx.r10.s64 = 1023;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_8219DD38:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x8219dd38
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_8219DD38;
	// li r11,2048
	r11.s64 = 2048;
	// stw r29,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r29.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r29,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r29.u32);
	// li r4,19
	ctx.r4.s64 = 19;
	// stw r29,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r29.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r29,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r29.u32);
	// stw r29,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r29.u32);
	// sth r11,130(r1)
	PPC_STORE_U16(ctx.r1.u32 + 130, r11.u16);
	// sth r11,10926(r31)
	PPC_STORE_U16(r31.u32 + 10926, r11.u16);
	// li r11,7
	r11.s64 = 7;
	// sth r10,128(r1)
	PPC_STORE_U16(ctx.r1.u32 + 128, ctx.r10.u16);
	// sth r10,10924(r31)
	PPC_STORE_U16(r31.u32 + 10924, ctx.r10.u16);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// bl 0x8240f97c
	__imp__KiApcNormalRoutineNop(ctx, base);
	// li r5,19
	ctx.r5.s64 = 19;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219c480
	sub_8219C480(ctx, base);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8219ddb8
	if (!cr6.getGT()) goto loc_8219DDB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_8219DDB8:
	// li r10,3330
	ctx.r10.s64 = 3330;
	// lis r9,1
	ctx.r9.s64 = 65536;
	// lis r8,3
	ctx.r8.s64 = 196608;
	// ori r9,r9,2048
	ctx.r9.u64 = ctx.r9.u64 | 2048;
	// ori r8,r8,2562
	ctx.r8.u64 = ctx.r8.u64 | 2562;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lis r10,-16368
	ctx.r10.s64 = -1072693248;
	// lis r7,2032
	ctx.r7.s64 = 133169152;
	// lis r6,-16384
	ctx.r6.s64 = -1073741824;
	// lis r5,16
	ctx.r5.s64 = 1048576;
	// li r4,477
	ctx.r4.s64 = 477;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// li r30,476
	r30.s64 = 476;
	// lis r3,2
	ctx.r3.s64 = 131072;
	// ori r29,r3,51
	r29.u64 = ctx.r3.u64 | 51;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r6,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	r11.u32 = ea;
	// stwu r5,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	r11.u32 = ea;
	// stwu r4,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	r11.u32 = ea;
	// lwz r10,10900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10900);
	// rlwinm r8,r10,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r10,3
	ctx.r9.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r8,512
	ctx.r10.s64 = ctx.r8.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r30,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r30.u32);
	r11.u32 = ea;
	// stwu r29,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r29.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x8219d888
	sub_8219D888(ctx, base);
	// b 0x8219db0c
	goto loc_8219DB0C;
loc_8219DE40:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
loc_8219DE48:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x823ed178
	return;
}

__attribute__((alias("__imp__sub_8219DE50"))) PPC_WEAK_FUNC(sub_8219DE50);
PPC_FUNC_IMPL(__imp__sub_8219DE50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lhz r11,0(r4)
	r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// lhz r9,2(r4)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r4.u32 + 2);
	// rlwinm r10,r11,26,6,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 26) & 0x3FFFFFF;
	// rlwinm r11,r9,26,6,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 26) & 0x3FFFFFF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// clrlwi r9,r11,16
	ctx.r9.u64 = r11.u32 & 0xFFFF;
	// cmplwi cr6,r9,1023
	cr6.compare<uint32_t>(ctx.r9.u32, 1023, xer);
	// ble cr6,0x8219de74
	if (!cr6.getGT()) goto loc_8219DE74;
	// li r11,1023
	r11.s64 = 1023;
loc_8219DE74:
	// rlwinm r10,r10,1,15,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1FFFE;
	// rlwinm r11,r11,1,15,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0x1FFFE;
	// lhzx r9,r10,r5
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r5.u32);
	// rotlwi r9,r9,6
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 6);
	// sth r9,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, ctx.r9.u16);
	// lhzx r11,r11,r5
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r5.u32);
	// lhzx r10,r10,r5
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r5.u32);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// sth r11,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, r11.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219DEA0"))) PPC_WEAK_FUNC(sub_8219DEA0);
PPC_FUNC_IMPL(__imp__sub_8219DEA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x8219defc
	if (cr6.getEQ()) goto loc_8219DEFC;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-17400(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -17400);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// bgt cr6,0x8219ded0
	if (cr6.getGT()) goto loc_8219DED0;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-17404(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17404);
	f0.f64 = double(temp.f32);
	// b 0x8219df14
	goto loc_8219DF14;
loc_8219DED0:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-17408(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17408);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fadds f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 + f0.f64));
	// lfd f2,-17416(r11)
	ctx.f2.u64 = PPC_LOAD_U64(r11.u32 + -17416);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-17424(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17424);
	f0.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * f0.f64));
	// bl 0x823f0100
	sub_823F0100(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// b 0x8219df40
	goto loc_8219DF40;
loc_8219DEFC:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-17432(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -17432);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// bgt cr6,0x8219df1c
	if (cr6.getGT()) goto loc_8219DF1C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-17440(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17440);
	f0.f64 = double(temp.f32);
loc_8219DF14:
	// fmuls f1,f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64 * f0.f64));
	// b 0x8219df40
	goto loc_8219DF40;
loc_8219DF1C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f2,-17448(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f2.u64 = PPC_LOAD_U64(r11.u32 + -17448);
	// bl 0x823f0100
	sub_823F0100(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// lfs f0,-17452(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17452);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f13,-17408(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17408);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f1,f12,f0,f13
	ctx.f1.f64 = double(float(ctx.f12.f64 * f0.f64 - ctx.f13.f64));
loc_8219DF40:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219DF50"))) PPC_WEAK_FUNC(sub_8219DF50);
PPC_FUNC_IMPL(__imp__sub_8219DF50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x8219dfac
	if (cr6.getEQ()) goto loc_8219DFAC;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-17336(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -17336);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// bge cr6,0x8219df80
	if (!cr6.getLT()) goto loc_8219DF80;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-17340(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17340);
	f0.f64 = double(temp.f32);
	// b 0x8219dfc4
	goto loc_8219DFC4;
loc_8219DF80:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-17344(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17344);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fadds f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 + f0.f64));
	// lfd f2,-17352(r11)
	ctx.f2.u64 = PPC_LOAD_U64(r11.u32 + -17352);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-17360(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17360);
	f0.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * f0.f64));
	// bl 0x823f0100
	sub_823F0100(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// b 0x8219dff0
	goto loc_8219DFF0;
loc_8219DFAC:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-17368(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -17368);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// bge cr6,0x8219dfcc
	if (!cr6.getLT()) goto loc_8219DFCC;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-17376(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17376);
	f0.f64 = double(temp.f32);
loc_8219DFC4:
	// fmuls f1,f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64 * f0.f64));
	// b 0x8219dff0
	goto loc_8219DFF0;
loc_8219DFCC:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f2,-17384(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f2.u64 = PPC_LOAD_U64(r11.u32 + -17384);
	// bl 0x823f0100
	sub_823F0100(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// lfs f0,-17392(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17392);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f13,-17344(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17344);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f1,f12,f0,f13
	ctx.f1.f64 = double(float(ctx.f12.f64 * f0.f64 - ctx.f13.f64));
loc_8219DFF0:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219E000"))) PPC_WEAK_FUNC(sub_8219E000);
PPC_FUNC_IMPL(__imp__sub_8219E000) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// addi r12,r1,-40
	r12.s64 = ctx.r1.s64 + -40;
	// bl 0x823ed548
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r31,r11,1
	r31.u64 = r11.u64 ^ 1;
	// bl 0x8240f9cc
	__imp__VdGetCurrentDisplayGamma(ctx, base);
	// lis r10,-32019
	ctx.r10.s64 = -2098397184;
	// lfs f2,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f2.f64 = double(temp.f32);
	// lis r9,-31991
	ctx.r9.s64 = -2096562176;
	// rlwinm r11,r31,2,0,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,28920
	ctx.r10.s64 = ctx.r10.s64 + 28920;
	// addi r9,r9,-31384
	ctx.r9.s64 = ctx.r9.s64 + -31384;
	// rlwinm r8,r31,11,0,20
	ctx.r8.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 11) & 0xFFFFF800;
	// add r28,r8,r9
	r28.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// lis r9,-31991
	ctx.r9.s64 = -2096562176;
	// addi r9,r9,-31392
	ctx.r9.s64 = ctx.r9.s64 + -31392;
	// bne cr6,0x8219e074
	if (!cr6.getEQ()) goto loc_8219E074;
	// lfsx f0,r11,r9
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f2
	cr6.compare(f0.f64, ctx.f2.f64);
	// beq cr6,0x8219e178
	if (cr6.getEQ()) goto loc_8219E178;
loc_8219E074:
	// stfsx f2,r11,r9
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, temp.u32);
	// stwx r8,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r8.u32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r31,0
	r31.s64 = 0;
	// lfd f29,28168(r7)
	f29.u64 = PPC_LOAD_U64(ctx.r7.u32 + 28168);
	// mr r29,r28
	r29.u64 = r28.u64;
	// lfd f30,-17320(r9)
	f30.u64 = PPC_LOAD_U64(ctx.r9.u32 + -17320);
	// lfs f31,2776(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2776);
	f31.f64 = double(temp.f32);
	// lfs f28,-17328(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17328);
	f28.f64 = double(temp.f32);
	// b 0x8219e0b0
	goto loc_8219E0B0;
loc_8219E0A8:
	// lfs f2,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f2.f64 = double(temp.f32);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_8219E0B0:
	// extsw r11,r31
	r11.s64 = r31.s32;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f1,f0,f28
	ctx.f1.f64 = double(float(f0.f64 * f28.f64));
	// bne cr6,0x8219e0e0
	if (!cr6.getEQ()) goto loc_8219E0E0;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x8219dea0
	sub_8219DEA0(ctx, base);
	// lfs f2,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f2.f64 = double(temp.f32);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_8219E0E0:
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// beq cr6,0x8219e11c
	if (cr6.getEQ()) goto loc_8219E11C;
	// cmplwi cr6,r8,2
	cr6.compare<uint32_t>(ctx.r8.u32, 2, xer);
	// beq cr6,0x8219e110
	if (cr6.getEQ()) goto loc_8219E110;
	// cmplwi cr6,r8,3
	cr6.compare<uint32_t>(ctx.r8.u32, 3, xer);
	// bne cr6,0x8219e110
	if (!cr6.getEQ()) goto loc_8219E110;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x8219e104
	if (!cr6.getEQ()) goto loc_8219E104;
	// fdivs f2,f31,f2
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = double(float(f31.f64 / ctx.f2.f64));
loc_8219E104:
	// bl 0x823f0100
	sub_823F0100(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// b 0x8219e124
	goto loc_8219E124;
loc_8219E110:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8219df50
	sub_8219DF50(ctx, base);
	// b 0x8219e124
	goto loc_8219E124;
loc_8219E11C:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8219dea0
	sub_8219DEA0(ctx, base);
loc_8219E124:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x8219e134
	if (cr6.getEQ()) goto loc_8219E134;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8219dea0
	sub_8219DEA0(ctx, base);
loc_8219E134:
	// fmadd f0,f1,f30,f29
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f1.f64 * f30.f64 + f29.f64;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x8219e158
	if (!cr6.getLT()) goto loc_8219E158;
	// li r11,0
	r11.s64 = 0;
	// b 0x8219e164
	goto loc_8219E164;
loc_8219E158:
	// cmpwi cr6,r11,1023
	cr6.compare<int32_t>(r11.s32, 1023, xer);
	// ble cr6,0x8219e164
	if (!cr6.getGT()) goto loc_8219E164;
	// li r11,1023
	r11.s64 = 1023;
loc_8219E164:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// sth r11,0(r29)
	PPC_STORE_U16(r29.u32 + 0, r11.u16);
	// addi r29,r29,2
	r29.s64 = r29.s64 + 2;
	// cmpwi cr6,r31,1024
	cr6.compare<int32_t>(r31.s32, 1024, xer);
	// blt cr6,0x8219e0a8
	if (cr6.getLT()) goto loc_8219E0A8;
loc_8219E178:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// addi r12,r1,-40
	r12.s64 = ctx.r1.s64 + -40;
	// bl 0x823ed594
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_8219E190"))) PPC_WEAK_FUNC(sub_8219E190);
PPC_FUNC_IMPL(__imp__sub_8219E190) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x8219e000
	sub_8219E000(ctx, base);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// addi r11,r31,512
	r11.s64 = r31.s64 + 512;
	// subf r8,r31,r30
	ctx.r8.s64 = r30.s64 - r31.s64;
	// li r9,256
	ctx.r9.s64 = 256;
loc_8219E1C4:
	// lhz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r7,r7,27,5,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x7FFFFFE;
	// lhzx r7,r7,r3
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r3.u32);
	// rotlwi r7,r7,6
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 6);
	// sth r7,-512(r11)
	PPC_STORE_U16(r11.u32 + -512, ctx.r7.u16);
	// lhzx r7,r8,r11
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + r11.u32);
	// rlwinm r7,r7,27,5,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x7FFFFFE;
	// lhzx r7,r7,r3
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r3.u32);
	// rotlwi r7,r7,6
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 6);
	// sth r7,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r7.u16);
	// lhz r7,1024(r10)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r10.u32 + 1024);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rlwinm r7,r7,27,5,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x7FFFFFE;
	// lhzx r7,r7,r3
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r3.u32);
	// rotlwi r7,r7,6
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 6);
	// sth r7,512(r11)
	PPC_STORE_U16(r11.u32 + 512, ctx.r7.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne 0x8219e1c4
	if (!cr0.getEQ()) goto loc_8219E1C4;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219E228"))) PPC_WEAK_FUNC(sub_8219E228);
PPC_FUNC_IMPL(__imp__sub_8219E228) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x8219e000
	sub_8219E000(ctx, base);
	// addi r8,r31,512
	ctx.r8.s64 = r31.s64 + 512;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// subf r31,r31,r30
	r31.s64 = r30.s64 - r31.s64;
	// li r6,128
	ctx.r6.s64 = 128;
loc_8219E260:
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// addi r3,r8,-512
	ctx.r3.s64 = ctx.r8.s64 + -512;
	// bl 0x8219de50
	sub_8219DE50(ctx, base);
	// add r4,r31,r8
	ctx.r4.u64 = r31.u64 + ctx.r8.u64;
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// bl 0x8219de50
	sub_8219DE50(ctx, base);
	// addi r4,r7,1024
	ctx.r4.s64 = ctx.r7.s64 + 1024;
	// addi r3,r8,512
	ctx.r3.s64 = ctx.r8.s64 + 512;
	// bl 0x8219de50
	sub_8219DE50(ctx, base);
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bne 0x8219e260
	if (!cr0.getEQ()) goto loc_8219E260;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219E2B0"))) PPC_WEAK_FUNC(sub_8219E2B0);
PPC_FUNC_IMPL(__imp__sub_8219E2B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r4,2309
	ctx.r4.s64 = 2309;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// lis r11,1
	r11.s64 = 65536;
	// li r10,0
	ctx.r10.s64 = 0;
	// ori r8,r11,6433
	ctx.r8.u64 = r11.u64 | 6433;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,6439
	ctx.r6.s64 = 6439;
	// li r5,7
	ctx.r5.s64 = 7;
	// li r9,0
	ctx.r9.s64 = 0;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r11,r31,1024
	r11.s64 = r31.s64 + 1024;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_8219E30C:
	// lhz r8,-1024(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + -1024);
	// li r6,6437
	ctx.r6.s64 = 6437;
	// lhz r7,-512(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + -512);
	// lis r4,-16379
	ctx.r4.s64 = -1073414144;
	// lhz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// li r3,6434
	ctx.r3.s64 = 6434;
	// rlwimi r7,r8,10,6,15
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 10) & 0x3FF0000) | (ctx.r7.u64 & 0xFFFFFFFFFC00FFFF);
	// rlwinm r8,r5,26,6,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 26) & 0x3FFFFFF;
	// rlwinm r7,r7,4,2,21
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0x3FFFFC00;
	// stwu r6,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r10.u32 = ea;
	// ori r4,r4,17664
	ctx.r4.u64 = ctx.r4.u64 | 17664;
	// or r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 | ctx.r8.u64;
	// li r7,7
	ctx.r7.s64 = 7;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// cmplwi cr6,r9,256
	cr6.compare<uint32_t>(ctx.r9.u32, 256, xer);
	// stwu r4,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r10.u32 = ea;
	// stwu r7,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r10.u32 = ea;
	// stwu r6,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r10.u32 = ea;
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// stwu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r10.u32 = ea;
	// stwu r3,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	ctx.r10.u32 = ea;
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x8219e30c
	if (cr6.getLT()) goto loc_8219E30C;
	// stw r10,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219E390"))) PPC_WEAK_FUNC(sub_8219E390);
PPC_FUNC_IMPL(__imp__sub_8219E390) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r4,1413
	ctx.r4.s64 = 1413;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// lis r11,1
	r11.s64 = 65536;
	// li r10,1
	ctx.r10.s64 = 1;
	// ori r8,r11,6433
	ctx.r8.u64 = r11.u64 | 6433;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,6439
	ctx.r6.s64 = 6439;
	// li r5,7
	ctx.r5.s64 = 7;
	// li r9,0
	ctx.r9.s64 = 0;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r11,r31,514
	r11.s64 = r31.s64 + 514;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// stwu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r10.u32 = ea;
loc_8219E3E4:
	// lis r8,2
	ctx.r8.s64 = 131072;
	// lis r7,-16379
	ctx.r7.s64 = -1073414144;
	// ori r8,r8,39204
	ctx.r8.u64 = ctx.r8.u64 | 39204;
	// ori r7,r7,17664
	ctx.r7.u64 = ctx.r7.u64 | 17664;
	// li r6,7
	ctx.r6.s64 = 7;
	// li r5,6436
	ctx.r5.s64 = 6436;
	// li r4,0
	ctx.r4.s64 = 0;
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// li r3,-1
	ctx.r3.s64 = -1;
	// lhz r8,-512(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + -512);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lhz r31,-514(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + -514);
	// rotlwi r8,r8,16
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 16);
	// cmplwi cr6,r9,128
	cr6.compare<uint32_t>(ctx.r9.u32, 128, xer);
	// or r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 | r31.u64;
	// li r31,6434
	r31.s64 = 6434;
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r29,-2(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// rotlwi r8,r8,16
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 16);
	// or r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 | r29.u64;
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// lhz r8,512(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 512);
	// lhz r29,510(r11)
	r29.u64 = PPC_LOAD_U16(r11.u32 + 510);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// rotlwi r8,r8,16
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 16);
	// or r8,r8,r29
	ctx.r8.u64 = ctx.r8.u64 | r29.u64;
	// stwu r8,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r10.u32 = ea;
	// stwu r7,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r10.u32 = ea;
	// stwu r6,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r10.u32 = ea;
	// stwu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r10.u32 = ea;
	// stwu r4,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r10.u32 = ea;
	// stwu r3,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	ctx.r10.u32 = ea;
	// stwu r31,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r31.u32);
	ctx.r10.u32 = ea;
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// blt cr6,0x8219e3e4
	if (cr6.getLT()) goto loc_8219E3E4;
	// stw r10,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219E480"))) PPC_WEAK_FUNC(sub_8219E480);
PPC_FUNC_IMPL(__imp__sub_8219E480) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r3,1024
	r11.s64 = ctx.r3.s64 + 1024;
loc_8219E488:
	// li r9,255
	ctx.r9.s64 = 255;
	// lis r8,3
	ctx.r8.s64 = 196608;
	// divwu r9,r10,r9
	ctx.r9.u32 = ctx.r10.u32 / ctx.r9.u32;
	// ori r8,r8,65280
	ctx.r8.u64 = ctx.r8.u64 | 65280;
	// rlwinm r9,r9,6,10,25
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 6) & 0x3FFFC0;
	// addi r10,r10,1023
	ctx.r10.s64 = ctx.r10.s64 + 1023;
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// sth r9,-1024(r11)
	PPC_STORE_U16(r11.u32 + -1024, ctx.r9.u16);
	// sth r9,-512(r11)
	PPC_STORE_U16(r11.u32 + -512, ctx.r9.u16);
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// blt cr6,0x8219e488
	if (cr6.getLT()) goto loc_8219E488;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beqlr cr6
	if (cr6.getEQ()) return;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x8219e190
	sub_8219E190(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8219E4D0"))) PPC_WEAK_FUNC(sub_8219E4D0);
PPC_FUNC_IMPL(__imp__sub_8219E4D0) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219E4D8"))) PPC_WEAK_FUNC(sub_8219E4D8);
PPC_FUNC_IMPL(__imp__sub_8219E4D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r11,r3,512
	r11.s64 = ctx.r3.s64 + 512;
	// li r7,127
	ctx.r7.s64 = 127;
loc_8219E4E4:
	// divwu r10,r9,r7
	ctx.r10.u32 = ctx.r9.u32 / ctx.r7.u32;
	// addis r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 65536;
	// lis r8,127
	ctx.r8.s64 = 8323072;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// ori r6,r8,65408
	ctx.r6.u64 = ctx.r8.u64 | 65408;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// divwu r8,r9,r7
	ctx.r8.u32 = ctx.r9.u32 / ctx.r7.u32;
	// cmplw cr6,r9,r6
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, xer);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// clrlwi r8,r8,16
	ctx.r8.u64 = ctx.r8.u32 & 0xFFFF;
	// sth r10,-512(r11)
	PPC_STORE_U16(r11.u32 + -512, ctx.r10.u16);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// sth r10,512(r11)
	PPC_STORE_U16(r11.u32 + 512, ctx.r10.u16);
	// sth r8,-510(r11)
	PPC_STORE_U16(r11.u32 + -510, ctx.r8.u16);
	// sth r8,2(r11)
	PPC_STORE_U16(r11.u32 + 2, ctx.r8.u16);
	// sth r8,514(r11)
	PPC_STORE_U16(r11.u32 + 514, ctx.r8.u16);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// blt cr6,0x8219e4e4
	if (cr6.getLT()) goto loc_8219E4E4;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beqlr cr6
	if (cr6.getEQ()) return;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x8219e228
	sub_8219E228(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8219E540"))) PPC_WEAK_FUNC(sub_8219E540);
PPC_FUNC_IMPL(__imp__sub_8219E540) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219E548"))) PPC_WEAK_FUNC(sub_8219E548);
PPC_FUNC_IMPL(__imp__sub_8219E548) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed120
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// rlwinm. r11,r4,0,30,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// lwz r31,0(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// beq 0x8219e5f0
	if (cr0.getEQ()) goto loc_8219E5F0;
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219e578
	if (cr0.getEQ()) goto loc_8219E578;
	// lwz r11,12740(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// b 0x8219e57c
	goto loc_8219E57C;
loc_8219E578:
	// li r11,1
	r11.s64 = 1;
loc_8219E57C:
	// stw r11,148(r29)
	PPC_STORE_U32(r29.u32 + 148, r11.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x8219e5a0
	if (!cr6.getEQ()) goto loc_8219E5A0;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a50c8
	sub_821A50C8(ctx, base);
	// b 0x8219e5e4
	goto loc_8219E5E4;
loc_8219E5A0:
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x8219e5e4
	if (!cr6.getEQ()) goto loc_8219E5E4;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x8219e5c0
	if (!cr6.getGT()) goto loc_8219E5C0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_8219E5C0:
	// lis r11,-16384
	r11.s64 = -1073741824;
	// li r10,25
	ctx.r10.s64 = 25;
	// ori r11,r11,17920
	r11.u64 = r11.u64 | 17920;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// lbz r11,10943(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// ori r11,r11,128
	r11.u64 = r11.u64 | 128;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// stb r11,10943(r31)
	PPC_STORE_U8(r31.u32 + 10943, r11.u8);
loc_8219E5E4:
	// li r11,2
	r11.s64 = 2;
	// stb r11,16(r29)
	PPC_STORE_U8(r29.u32 + 16, r11.u8);
	// b 0x8219e7c8
	goto loc_8219E7C8;
loc_8219E5F0:
	// clrlwi. r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219e7c8
	if (cr0.getEQ()) goto loc_8219E7C8;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// beq cr6,0x8219e7c0
	if (cr6.getEQ()) goto loc_8219E7C0;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x8219e750
	if (!cr6.getEQ()) goto loc_8219E750;
	// lwz r11,148(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 148);
	// lwz r10,24(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r11,r29
	r30.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// beq cr6,0x8219e724
	if (cr6.getEQ()) goto loc_8219E724;
	// lbz r11,20(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 20);
	// rlwinm. r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x8219e724
	if (!cr0.getEQ()) goto loc_8219E724;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x8219e650
	if (!cr6.getGT()) goto loc_8219E650;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_8219E650:
	// lis r11,-16383
	r11.s64 = -1073676288;
	// addi r8,r30,16
	ctx.r8.s64 = r30.s64 + 16;
	// ori r9,r11,15616
	ctx.r9.u64 = r11.u64 | 15616;
	// li r11,-275
	r11.s64 = -275;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r27,r11
	r27.u64 = r11.u64;
	// mr r22,r11
	r22.u64 = r11.u64;
	// stwu r6,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r10.u32 = ea;
	// lis r9,-16380
	ctx.r9.s64 = -1073479680;
	// rlwinm r11,r8,12,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// ori r9,r9,15360
	ctx.r9.u64 = ctx.r9.u64 | 15360;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// addi r7,r30,20
	ctx.r7.s64 = r30.s64 + 20;
	// mr r30,r9
	r30.u64 = ctx.r9.u64;
	// mr r23,r9
	r23.u64 = ctx.r9.u64;
	// rlwinm r9,r11,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r11,r7,12,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r8,r11,512
	ctx.r8.s64 = r11.s64 + 512;
	// ori r11,r9,2
	r11.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r8,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r7,3
	ctx.r8.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// li r28,19
	r28.s64 = 19;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r11,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r10.u32 = ea;
	// li r6,-1
	ctx.r6.s64 = -1;
	// ori r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 2;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// li r25,256
	r25.s64 = 256;
	// li r24,19
	r24.s64 = 19;
	// stwu r5,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r10.u32 = ea;
	// stwu r3,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	ctx.r10.u32 = ea;
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// stwu r4,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r10.u32 = ea;
	// stwu r30,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r30.u32);
	ctx.r10.u32 = ea;
	// stwu r28,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r10.u32 = ea;
	// stwu r11,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r10.u32 = ea;
	// li r11,256
	r11.s64 = 256;
	// stwu r27,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r10.u32 = ea;
	// stwu r26,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r10.u32 = ea;
	// stwu r25,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r10.u32 = ea;
	// stwu r23,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r10.u32 = ea;
	// stwu r24,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r10.u32 = ea;
	// stwu r9,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r10.u32 = ea;
	// stwu r22,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r10.u32 = ea;
	// stwu r6,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r10.u32 = ea;
	// stwu r11,4(r10)
	ea = 4 + ctx.r10.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r10.u32 = ea;
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
	// b 0x8219e73c
	goto loc_8219E73C;
loc_8219E724:
	// lbz r10,20(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 20);
	// li r11,-275
	r11.s64 = -275;
	// clrlwi r10,r10,25
	ctx.r10.u64 = ctx.r10.u32 & 0x7F;
	// stb r10,20(r29)
	PPC_STORE_U8(r29.u32 + 20, ctx.r10.u8);
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// stw r11,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r11.u32);
loc_8219E73C:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a50c8
	sub_821A50C8(ctx, base);
	// b 0x8219e7b8
	goto loc_8219E7B8;
loc_8219E750:
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x8219e7b8
	if (!cr6.getEQ()) goto loc_8219E7B8;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8219e774
	if (!cr6.getGT()) goto loc_8219E774;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_8219E774:
	// lis r10,-16383
	ctx.r10.s64 = -1073676288;
	// li r9,26
	ctx.r9.s64 = 26;
	// ori r10,r10,23040
	ctx.r10.u64 = ctx.r10.u64 | 23040;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// lwz r10,28(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// rlwinm r8,r10,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// rlwinm r9,r10,0,3,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1FFFFFFE;
	// addi r10,r8,512
	ctx.r10.s64 = ctx.r8.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// ori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 | 1;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lbz r10,10943(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// clrlwi r10,r10,25
	ctx.r10.u64 = ctx.r10.u32 & 0x7F;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// stb r10,10943(r31)
	PPC_STORE_U8(r31.u32 + 10943, ctx.r10.u8);
loc_8219E7B8:
	// li r11,1
	r11.s64 = 1;
	// stb r11,16(r29)
	PPC_STORE_U8(r29.u32 + 16, r11.u8);
loc_8219E7C0:
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// stw r11,24(r29)
	PPC_STORE_U32(r29.u32 + 24, r11.u32);
loc_8219E7C8:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x823ed170
	return;
}

__attribute__((alias("__imp__sub_8219E7D0"))) PPC_WEAK_FUNC(sub_8219E7D0);
PPC_FUNC_IMPL(__imp__sub_8219E7D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,10916(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10916);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwimi r11,r4,12,18,19
	r11.u64 = (__builtin_rotateleft32(ctx.r4.u32, 12) & 0x3000) | (r11.u64 & 0xFFFFFFFFFFFFCFFF);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r3,r10
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r10.u32, xer);
	// stw r11,10916(r31)
	PPC_STORE_U32(r31.u32 + 10916, r11.u32);
	// ble cr6,0x8219e808
	if (!cr6.getGT()) goto loc_8219E808;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_8219E808:
	// li r11,1480
	r11.s64 = 1480;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// li r9,3841
	ctx.r9.s64 = 3841;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// lwz r11,10916(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10916);
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219E840"))) PPC_WEAK_FUNC(sub_8219E840);
PPC_FUNC_IMPL(__imp__sub_8219E840) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// beq cr6,0x8219e8f4
	if (cr6.getEQ()) goto loc_8219E8F4;
	// bl 0x8219d958
	sub_8219D958(ctx, base);
	// cntlzw r11,r30
	r11.u64 = r30.u32 == 0 ? 32 : __builtin_clz(r30.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x8240f9dc
	__imp__VdEnableDisableClockGating(ctx, base);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x8219e8f4
	if (!cr6.getEQ()) goto loc_8219E8F4;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x8219e8a0
	if (!cr6.getGT()) goto loc_8219E8A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_8219E8A0:
	// lis r10,-16382
	ctx.r10.s64 = -1073610752;
	// li r7,129
	ctx.r7.s64 = 129;
	// ori r10,r10,8448
	ctx.r10.u64 = ctx.r10.u64 | 8448;
	// li r9,-1
	ctx.r9.s64 = -1;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// li r5,130
	ctx.r5.s64 = 130;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r6,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	r11.u32 = ea;
	// lwz r10,-9512(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -9512);
	// oris r10,r10,32769
	ctx.r10.u64 = ctx.r10.u64 | 2147549184;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r5,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// lwz r10,-9508(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -9508);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
loc_8219E8F4:
	// addi r11,r30,-1
	r11.s64 = r30.s64 + -1;
	// lbz r10,10942(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// rlwimi r10,r11,0,31,31
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0x1) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFFE);
	// stb r10,10942(r31)
	PPC_STORE_U8(r31.u32 + 10942, ctx.r10.u8);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219E928"))) PPC_WEAK_FUNC(sub_8219E928);
PPC_FUNC_IMPL(__imp__sub_8219E928) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x8219e95c
	if (!cr0.getEQ()) goto loc_8219E95C;
loc_8219E954:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8219e9a8
	goto loc_8219E9A8;
loc_8219E95C:
	// lis r4,-23936
	ctx.r4.s64 = -1568669696;
	// li r3,480
	ctx.r3.s64 = 480;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// bne 0x8219e984
	if (!cr0.getEQ()) goto loc_8219E984;
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// b 0x8219e954
	goto loc_8219E954;
loc_8219E984:
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r3,480
	ctx.r4.s64 = ctx.r3.s64 + 480;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
loc_8219E9A8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8219E9C0"))) PPC_WEAK_FUNC(sub_8219E9C0);
PPC_FUNC_IMPL(__imp__sub_8219E9C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r4,120
	ctx.r4.s64 = 120;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// li r10,486
	ctx.r10.s64 = 486;
	// lbz r11,3(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 3);
	// li r9,917
	ctx.r9.s64 = 917;
	// li r8,918
	ctx.r8.s64 = 918;
	// rlwinm r29,r11,0,0,23
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r7,3528
	ctx.r7.s64 = 3528;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// li r6,3529
	ctx.r6.s64 = 3529;
	// li r5,3530
	ctx.r5.s64 = 3530;
	// li r4,3531
	ctx.r4.s64 = 3531;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,7(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 7);
	// or r11,r11,r29
	r11.u64 = r11.u64 | r29.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r9,3144
	ctx.r9.s64 = 3144;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,11(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 11);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r8,3146
	ctx.r8.s64 = 3146;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,15(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 15);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r7,3147
	ctx.r7.s64 = 3147;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,19(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 19);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r6,3659
	ctx.r6.s64 = 3659;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,23(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 23);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r5,3662
	ctx.r5.s64 = 3662;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,27(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 27);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r4,3665
	ctx.r4.s64 = 3665;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,31(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 31);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// li r10,3145
	ctx.r10.s64 = 3145;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// rlwinm r29,r11,0,0,23
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r9,3656
	ctx.r9.s64 = 3656;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,35(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 35);
	// or r11,r11,r29
	r11.u64 = r11.u64 | r29.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r29,r11,0,0,23
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,39(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 39);
	// or r11,r11,r29
	r11.u64 = r11.u64 | r29.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,43(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 43);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,47(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 47);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,51(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 51);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// li r8,3209
	ctx.r8.s64 = 3209;
	// li r7,3210
	ctx.r7.s64 = 3210;
	// li r6,3211
	ctx.r6.s64 = 3211;
	// li r29,3226
	r29.s64 = 3226;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,55(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 55);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r5,3224
	ctx.r5.s64 = 3224;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,59(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 59);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// li r10,3208
	ctx.r10.s64 = 3208;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r4,3225
	ctx.r4.s64 = 3225;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,63(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 63);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,67(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 67);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// li r9,3227
	ctx.r9.s64 = 3227;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r8,4100
	ctx.r8.s64 = 4100;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,71(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 71);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r7,3589
	ctx.r7.s64 = 3589;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,75(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 75);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r6,3592
	ctx.r6.s64 = 3592;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,79(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 79);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r5,3668
	ctx.r5.s64 = 3668;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,83(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 83);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r4,3671
	ctx.r4.s64 = 3671;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,87(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 87);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r29,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r29.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// rlwinm r11,r11,0,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// lbz r10,91(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 91);
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// li r10,4103
	ctx.r10.s64 = 4103;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,95(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 95);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,99(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 99);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,103(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 103);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,107(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 107);
	// li r9,3677
	ctx.r9.s64 = 3677;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// li r7,3680
	ctx.r7.s64 = 3680;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// li r6,3686
	ctx.r6.s64 = 3686;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r29,3695
	r29.s64 = 3695;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,111(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 111);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r5,3689
	ctx.r5.s64 = 3689;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,115(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 115);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r4,3692
	ctx.r4.s64 = 3692;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,119(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 119);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// li r10,3674
	ctx.r10.s64 = 3674;
	// rlwinm r8,r11,0,0,23
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// li r10,3683
	ctx.r10.s64 = 3683;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,123(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 123);
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// rlwinm r8,r11,0,0,23
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,127(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 127);
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// li r8,3701
	ctx.r8.s64 = 3701;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r7,3704
	ctx.r7.s64 = 3704;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,131(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 131);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,135(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 135);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r6,3707
	ctx.r6.s64 = 3707;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,139(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 139);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r5,3615
	ctx.r5.s64 = 3615;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,143(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 143);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r4,3624
	ctx.r4.s64 = 3624;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,147(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 147);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// li r10,3698
	ctx.r10.s64 = 3698;
	// stwu r29,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r29.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,151(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 151);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// li r9,3618
	ctx.r9.s64 = 3618;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r29,r11,0,0,23
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,155(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 155);
	// or r11,r11,r29
	r11.u64 = r11.u64 | r29.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,159(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 159);
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// li r8,3633
	ctx.r8.s64 = 3633;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r7,3645
	ctx.r7.s64 = 3645;
	// li r29,2590
	r29.s64 = 2590;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,163(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 163);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r6,3540
	ctx.r6.s64 = 3540;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,167(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 167);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r5,3844
	ctx.r5.s64 = 3844;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,171(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 171);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r9,3627
	ctx.r9.s64 = 3627;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,175(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 175);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// rlwinm r11,r11,0,0,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// lbz r10,179(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 179);
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// li r10,3636
	ctx.r10.s64 = 3636;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// rlwinm r4,r11,0,0,23
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r9,3642
	ctx.r9.s64 = 3642;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,183(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 183);
	// or r11,r11,r4
	r11.u64 = r11.u64 | ctx.r4.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r4,r11,0,0,23
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,187(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 187);
	// or r11,r11,r4
	r11.u64 = r11.u64 | ctx.r4.u64;
	// li r4,2587
	ctx.r4.s64 = 2587;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r8,r11,0,0,23
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,191(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 191);
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r9,3846
	ctx.r9.s64 = 3846;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,195(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 195);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r7,3847
	ctx.r7.s64 = 3847;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,199(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 199);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r6,2133
	ctx.r6.s64 = 2133;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,203(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 203);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// li r5,2584
	ctx.r5.s64 = 2584;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,207(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 207);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// li r10,3845
	ctx.r10.s64 = 3845;
	// rlwinm r8,r11,0,0,23
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// li r10,2069
	ctx.r10.s64 = 2069;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,211(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 211);
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// rlwinm r8,r11,0,0,23
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,215(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 215);
	// or r11,r11,r8
	r11.u64 = r11.u64 | ctx.r8.u64;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,219(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 219);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,223(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 223);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,227(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 227);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,231(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 231);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,0,23
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,235(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 235);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// li r10,72
	ctx.r10.s64 = 72;
	// stwu r29,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r29.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lbz r11,239(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 239);
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,56(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// stw r3,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r3.u32);
	// ble cr6,0x8219ef8c
	if (!cr6.getGT()) goto loc_8219EF8C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_8219EF8C:
	// li r11,501
	r11.s64 = 501;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,8697
	ctx.r9.s64 = 8697;
	// li r8,23
	ctx.r8.s64 = 23;
	// li r7,501
	ctx.r7.s64 = 501;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// li r11,1
	r11.s64 = 1;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r3.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_8219EFC8"))) PPC_WEAK_FUNC(sub_8219EFC8);
PPC_FUNC_IMPL(__imp__sub_8219EFC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	// lwz r3,12(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x8219efd8
	if (!cr0.getEQ()) goto loc_8219EFD8;
	// blr 
	return;
loc_8219EFD8:
	// b 0x8219d810
	sub_8219D810(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8219EFE0"))) PPC_WEAK_FUNC(sub_8219EFE0);
PPC_FUNC_IMPL(__imp__sub_8219EFE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed11c
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// lwz r3,12(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x8219f008
	if (cr0.getEQ()) goto loc_8219F008;
	// bl 0x8219d298
	sub_8219D298(ctx, base);
loc_8219F008:
	// clrlwi. r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x8219f040
	if (cr0.getEQ()) goto loc_8219F040;
	// lwz r3,48(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r11,56(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x8219f028
	if (!cr6.getGT()) goto loc_8219F028;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_8219F028:
	// lis r11,-16384
	r11.s64 = -1073741824;
	// li r10,0
	ctx.r10.s64 = 0;
	// ori r11,r11,9728
	r11.u64 = r11.u64 | 9728;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r3.u32);
loc_8219F040:
	// li r4,360
	ctx.r4.s64 = 360;
	// lwz r31,4(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// lis r11,-16383
	r11.s64 = -1073676288;
	// addi r10,r31,4
	ctx.r10.s64 = r31.s64 + 4;
	// ori r11,r11,15872
	r11.u64 = r11.u64 | 15872;
	// rlwinm r6,r10,12,20,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// clrlwi r7,r10,3
	ctx.r7.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r6,512
	ctx.r10.s64 = ctx.r6.s64 + 512;
	// li r4,487
	ctx.r4.s64 = 487;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// mr r27,r11
	r27.u64 = r11.u64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// rlwinm r9,r31,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 12) & 0xFFF;
	// ori r6,r10,2
	ctx.r6.u64 = ctx.r10.u64 | 2;
	// addi r9,r9,512
	ctx.r9.s64 = ctx.r9.s64 + 512;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// li r28,488
	r28.s64 = 488;
	// rlwinm r9,r9,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r31,3
	ctx.r8.u64 = r31.u32 & 0x1FFFFFFF;
	// mr r26,r11
	r26.u64 = r11.u64;
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// addi r9,r31,12
	ctx.r9.s64 = r31.s64 + 12;
	// ori r25,r8,2
	r25.u64 = ctx.r8.u64 | 2;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// clrlwi r7,r9,3
	ctx.r7.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// li r24,919
	r24.s64 = 919;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r8,r31,8
	ctx.r8.s64 = r31.s64 + 8;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// ori r7,r9,2
	ctx.r7.u64 = ctx.r9.u64 | 2;
	// mr r23,r11
	r23.u64 = r11.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r9,r8,3
	ctx.r9.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r22,920
	r22.s64 = 920;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,16
	ctx.r9.s64 = r31.s64 + 16;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// li r4,921
	ctx.r4.s64 = 921;
	// mr r28,r11
	r28.u64 = r11.u64;
	// li r27,922
	r27.s64 = 922;
	// li r25,3532
	r25.s64 = 3532;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// mr r24,r11
	r24.u64 = r11.u64;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// addi r7,r31,24
	ctx.r7.s64 = r31.s64 + 24;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3533
	r23.s64 = 3533;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// mr r22,r11
	r22.u64 = r11.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,20
	ctx.r10.s64 = r31.s64 + 20;
	// rlwinm r6,r10,12,20,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r5,r10,3
	ctx.r5.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r6,512
	ctx.r10.s64 = ctx.r6.s64 + 512;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,28
	ctx.r8.s64 = r31.s64 + 28;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r5,r6,2
	ctx.r5.u64 = ctx.r6.u64 | 2;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// addi r7,r31,40
	ctx.r7.s64 = r31.s64 + 40;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// addi r9,r31,32
	ctx.r9.s64 = r31.s64 + 32;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// li r27,3535
	r27.s64 = 3535;
	// mr r21,r11
	r21.u64 = r11.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// addi r6,r31,52
	ctx.r6.s64 = r31.s64 + 52;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3536
	r25.s64 = 3536;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// li r8,3534
	ctx.r8.s64 = 3534;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3537
	r23.s64 = 3537;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,36
	ctx.r10.s64 = r31.s64 + 36;
	// rlwinm r4,r10,12,20,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r5,r10,3
	ctx.r5.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r4,512
	ctx.r10.s64 = ctx.r4.s64 + 512;
	// rlwinm r4,r9,12,20,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r4,r4,512
	ctx.r4.s64 = ctx.r4.s64 + 512;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r5,r4,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x1000;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// ori r4,r10,2
	ctx.r4.u64 = ctx.r10.u64 | 2;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r8,r31,44
	ctx.r8.s64 = r31.s64 + 44;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// ori r5,r9,2
	ctx.r5.u64 = ctx.r9.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// li r22,3538
	r22.s64 = 3538;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// clrlwi r9,r6,3
	ctx.r9.u64 = ctx.r6.u32 & 0x1FFFFFFF;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// li r28,3539
	r28.s64 = 3539;
	// mr r27,r11
	r27.u64 = r11.u64;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r6,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r31,48
	ctx.r10.s64 = r31.s64 + 48;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// addi r9,r31,60
	ctx.r9.s64 = r31.s64 + 60;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r6,r10,12,20,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// clrlwi r5,r10,3
	ctx.r5.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r6,512
	ctx.r10.s64 = ctx.r6.s64 + 512;
	// li r26,3148
	r26.s64 = 3148;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r5,r6,2
	ctx.r5.u64 = ctx.r6.u64 | 2;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r8,r31,56
	ctx.r8.s64 = r31.s64 + 56;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// mr r25,r11
	r25.u64 = r11.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r24,3149
	r24.s64 = 3149;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r7,r31,68
	ctx.r7.s64 = r31.s64 + 68;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// mr r23,r11
	r23.u64 = r11.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// li r22,3150
	r22.s64 = 3150;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// li r28,3151
	r28.s64 = 3151;
	// ori r9,r10,2
	ctx.r9.u64 = ctx.r10.u64 | 2;
	// addi r10,r31,64
	ctx.r10.s64 = r31.s64 + 64;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// addi r7,r31,84
	ctx.r7.s64 = r31.s64 + 84;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// li r26,3152
	r26.s64 = 3152;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r24,3153
	r24.s64 = 3153;
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// ori r6,r6,2
	ctx.r6.u64 = ctx.r6.u64 | 2;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,72
	ctx.r8.s64 = r31.s64 + 72;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// li r22,3154
	r22.s64 = 3154;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// addi r9,r31,76
	ctx.r9.s64 = r31.s64 + 76;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// ori r5,r9,2
	ctx.r5.u64 = ctx.r9.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// addi r9,r31,92
	ctx.r9.s64 = r31.s64 + 92;
	// li r4,3155
	ctx.r4.s64 = 3155;
	// mr r28,r11
	r28.u64 = r11.u64;
	// li r27,3658
	r27.s64 = 3658;
	// mr r26,r11
	r26.u64 = r11.u64;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// li r25,3657
	r25.s64 = 3657;
	// addi r7,r31,100
	ctx.r7.s64 = r31.s64 + 100;
	// mr r24,r11
	r24.u64 = r11.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3661
	r23.s64 = 3661;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// li r22,3660
	r22.s64 = 3660;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,80
	ctx.r10.s64 = r31.s64 + 80;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r8,r31,88
	ctx.r8.s64 = r31.s64 + 88;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// addi r7,r31,116
	ctx.r7.s64 = r31.s64 + 116;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,108
	ctx.r9.s64 = r31.s64 + 108;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// li r27,3664
	r27.s64 = 3664;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// addi r6,r31,112
	ctx.r6.s64 = r31.s64 + 112;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3663
	r25.s64 = 3663;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,104
	ctx.r8.s64 = r31.s64 + 104;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3667
	r23.s64 = 3667;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,96
	ctx.r10.s64 = r31.s64 + 96;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r4,r10,3
	ctx.r4.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// rlwinm r5,r10,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r4,r5,2
	ctx.r4.u64 = ctx.r5.u64 | 2;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// rlwinm r5,r10,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// li r22,3666
	r22.s64 = 3666;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r5,r9,2
	ctx.r5.u64 = ctx.r9.u64 | 2;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// ori r4,r9,2
	ctx.r4.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r7,3
	ctx.r8.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// rlwinm r10,r6,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r28,3212
	r28.s64 = 3212;
	// mr r27,r11
	r27.u64 = r11.u64;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// li r26,3213
	r26.s64 = 3213;
	// addi r7,r31,128
	ctx.r7.s64 = r31.s64 + 128;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// mr r25,r11
	r25.u64 = r11.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// li r24,3214
	r24.s64 = 3214;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// mr r23,r11
	r23.u64 = r11.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r6,3
	ctx.r9.u64 = ctx.r6.u32 & 0x1FFFFFFF;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,120
	ctx.r9.s64 = r31.s64 + 120;
	// ori r8,r10,2
	ctx.r8.u64 = ctx.r10.u64 | 2;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,124
	ctx.r10.s64 = r31.s64 + 124;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r8,r31,132
	ctx.r8.s64 = r31.s64 + 132;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// li r22,3215
	r22.s64 = 3215;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// li r28,3216
	r28.s64 = 3216;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// addi r9,r31,136
	ctx.r9.s64 = r31.s64 + 136;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// li r26,3217
	r26.s64 = 3217;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,148
	ctx.r8.s64 = r31.s64 + 148;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,140
	ctx.r10.s64 = r31.s64 + 140;
	// rlwinm r6,r10,12,20,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// clrlwi r5,r10,3
	ctx.r5.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r6,r6,512
	ctx.r6.s64 = ctx.r6.s64 + 512;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r6,r6,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x1000;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// ori r5,r6,2
	ctx.r5.u64 = ctx.r6.u64 | 2;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// li r24,3218
	r24.s64 = 3218;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// addi r7,r31,144
	ctx.r7.s64 = r31.s64 + 144;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// li r22,3219
	r22.s64 = 3219;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r4,3228
	ctx.r4.s64 = 3228;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,152
	ctx.r9.s64 = r31.s64 + 152;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// mr r28,r11
	r28.u64 = r11.u64;
	// addi r7,r31,160
	ctx.r7.s64 = r31.s64 + 160;
	// li r27,3229
	r27.s64 = 3229;
	// mr r26,r11
	r26.u64 = r11.u64;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3230
	r25.s64 = 3230;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// mr r24,r11
	r24.u64 = r11.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,164
	ctx.r8.s64 = r31.s64 + 164;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3231
	r23.s64 = 3231;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// li r22,3232
	r22.s64 = 3232;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,156
	ctx.r10.s64 = r31.s64 + 156;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// addi r9,r31,168
	ctx.r9.s64 = r31.s64 + 168;
	// li r27,3233
	r27.s64 = 3233;
	// addi r7,r31,176
	ctx.r7.s64 = r31.s64 + 176;
	// addi r6,r31,188
	ctx.r6.s64 = r31.s64 + 188;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3234
	r25.s64 = 3234;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,180
	ctx.r8.s64 = r31.s64 + 180;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3235
	r23.s64 = 3235;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,172
	ctx.r10.s64 = r31.s64 + 172;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r4,r10,3
	ctx.r4.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// rlwinm r5,r10,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// ori r4,r5,2
	ctx.r4.u64 = ctx.r5.u64 | 2;
	// rlwinm r5,r10,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r5,r9,2
	ctx.r5.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r4,r9,2
	ctx.r4.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r7,3
	ctx.r8.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r6,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// li r22,4102
	r22.s64 = 4102;
	// li r28,4101
	r28.s64 = 4101;
	// mr r27,r11
	r27.u64 = r11.u64;
	// addi r7,r31,204
	ctx.r7.s64 = r31.s64 + 204;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// li r26,4105
	r26.s64 = 4105;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// mr r25,r11
	r25.u64 = r11.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// li r24,4104
	r24.s64 = 4104;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// mr r23,r11
	r23.u64 = r11.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r6,3
	ctx.r9.u64 = ctx.r6.u32 & 0x1FFFFFFF;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,196
	ctx.r9.s64 = r31.s64 + 196;
	// ori r8,r10,2
	ctx.r8.u64 = ctx.r10.u64 | 2;
	// addi r10,r31,184
	ctx.r10.s64 = r31.s64 + 184;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// li r22,3591
	r22.s64 = 3591;
	// addi r8,r31,192
	ctx.r8.s64 = r31.s64 + 192;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// li r28,3590
	r28.s64 = 3590;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// addi r9,r31,212
	ctx.r9.s64 = r31.s64 + 212;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// addi r7,r31,220
	ctx.r7.s64 = r31.s64 + 220;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// li r26,3594
	r26.s64 = 3594;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// li r24,3593
	r24.s64 = 3593;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,208
	ctx.r8.s64 = r31.s64 + 208;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// li r22,3670
	r22.s64 = 3670;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,200
	ctx.r10.s64 = r31.s64 + 200;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// li r4,3669
	ctx.r4.s64 = 3669;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// ori r5,r6,2
	ctx.r5.u64 = ctx.r6.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r6,r9,3
	ctx.r6.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// mr r28,r11
	r28.u64 = r11.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,228
	ctx.r9.s64 = r31.s64 + 228;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// li r27,3673
	r27.s64 = 3673;
	// mr r26,r11
	r26.u64 = r11.u64;
	// addi r7,r31,236
	ctx.r7.s64 = r31.s64 + 236;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3672
	r25.s64 = 3672;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,224
	ctx.r8.s64 = r31.s64 + 224;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,216
	ctx.r10.s64 = r31.s64 + 216;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// mr r24,r11
	r24.u64 = r11.u64;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// li r23,3676
	r23.s64 = 3676;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,244
	ctx.r9.s64 = r31.s64 + 244;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// li r22,3675
	r22.s64 = 3675;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// li r27,3679
	r27.s64 = 3679;
	// addi r7,r31,252
	ctx.r7.s64 = r31.s64 + 252;
	// addi r6,r31,248
	ctx.r6.s64 = r31.s64 + 248;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3678
	r25.s64 = 3678;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,240
	ctx.r8.s64 = r31.s64 + 240;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3682
	r23.s64 = 3682;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,232
	ctx.r10.s64 = r31.s64 + 232;
	// rlwinm r4,r10,12,20,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r5,r10,3
	ctx.r5.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r4,512
	ctx.r10.s64 = ctx.r4.s64 + 512;
	// rlwinm r4,r9,12,20,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r4,r4,512
	ctx.r4.s64 = ctx.r4.s64 + 512;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// rlwinm r5,r4,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x1000;
	// ori r4,r10,2
	ctx.r4.u64 = ctx.r10.u64 | 2;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// ori r5,r9,2
	ctx.r5.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r4,r9,2
	ctx.r4.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r7,3
	ctx.r8.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// rlwinm r10,r6,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// li r22,3681
	r22.s64 = 3681;
	// ori r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r28,3685
	r28.s64 = 3685;
	// mr r27,r11
	r27.u64 = r11.u64;
	// li r26,3684
	r26.s64 = 3684;
	// mr r25,r11
	r25.u64 = r11.u64;
	// addi r7,r31,264
	ctx.r7.s64 = r31.s64 + 264;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// li r24,3688
	r24.s64 = 3688;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// mr r23,r11
	r23.u64 = r11.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r6,3
	ctx.r9.u64 = ctx.r6.u32 & 0x1FFFFFFF;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,256
	ctx.r9.s64 = r31.s64 + 256;
	// ori r8,r10,2
	ctx.r8.u64 = ctx.r10.u64 | 2;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,260
	ctx.r10.s64 = r31.s64 + 260;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r8,r31,268
	ctx.r8.s64 = r31.s64 + 268;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// li r22,3687
	r22.s64 = 3687;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,272
	ctx.r9.s64 = r31.s64 + 272;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// li r4,3691
	ctx.r4.s64 = 3691;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// mr r28,r11
	r28.u64 = r11.u64;
	// li r27,3690
	r27.s64 = 3690;
	// addi r7,r31,280
	ctx.r7.s64 = r31.s64 + 280;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// mr r26,r11
	r26.u64 = r11.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3694
	r25.s64 = 3694;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// mr r24,r11
	r24.u64 = r11.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3693
	r23.s64 = 3693;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// mr r22,r11
	r22.u64 = r11.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,276
	ctx.r10.s64 = r31.s64 + 276;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r8,r31,284
	ctx.r8.s64 = r31.s64 + 284;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// addi r7,r31,296
	ctx.r7.s64 = r31.s64 + 296;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,288
	ctx.r9.s64 = r31.s64 + 288;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// li r27,3696
	r27.s64 = 3696;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// addi r6,r31,308
	ctx.r6.s64 = r31.s64 + 308;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3700
	r25.s64 = 3700;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// li r8,3697
	ctx.r8.s64 = 3697;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3699
	r23.s64 = 3699;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,292
	ctx.r10.s64 = r31.s64 + 292;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r4,r10,3
	ctx.r4.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// li r22,3703
	r22.s64 = 3703;
	// rlwinm r5,r10,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// ori r4,r5,2
	ctx.r4.u64 = ctx.r5.u64 | 2;
	// rlwinm r5,r10,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r8,r31,300
	ctx.r8.s64 = r31.s64 + 300;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// ori r5,r9,2
	ctx.r5.u64 = ctx.r9.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// clrlwi r9,r6,3
	ctx.r9.u64 = ctx.r6.u32 & 0x1FFFFFFF;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r6,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// ori r8,r10,2
	ctx.r8.u64 = ctx.r10.u64 | 2;
	// addi r10,r31,304
	ctx.r10.s64 = r31.s64 + 304;
	// addi r9,r31,316
	ctx.r9.s64 = r31.s64 + 316;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r28,3702
	r28.s64 = 3702;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// mr r27,r11
	r27.u64 = r11.u64;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// li r26,3706
	r26.s64 = 3706;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r8,r31,312
	ctx.r8.s64 = r31.s64 + 312;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// mr r25,r11
	r25.u64 = r11.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r24,3705
	r24.s64 = 3705;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r7,r31,324
	ctx.r7.s64 = r31.s64 + 324;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// mr r22,r11
	r22.u64 = r11.u64;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r23,3709
	r23.s64 = 3709;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// addi r7,r31,340
	ctx.r7.s64 = r31.s64 + 340;
	// ori r9,r10,2
	ctx.r9.u64 = ctx.r10.u64 | 2;
	// addi r10,r31,320
	ctx.r10.s64 = r31.s64 + 320;
	// li r28,3708
	r28.s64 = 3708;
	// rlwinm r6,r10,12,20,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// clrlwi r5,r10,3
	ctx.r5.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r6,r6,512
	ctx.r6.s64 = ctx.r6.s64 + 512;
	// li r26,3617
	r26.s64 = 3617;
	// rlwinm r6,r6,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x1000;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// li r24,3616
	r24.s64 = 3616;
	// ori r5,r6,2
	ctx.r5.u64 = ctx.r6.u64 | 2;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,328
	ctx.r8.s64 = r31.s64 + 328;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// li r22,3620
	r22.s64 = 3620;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// mr r23,r11
	r23.u64 = r11.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// addi r9,r31,332
	ctx.r9.s64 = r31.s64 + 332;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// li r4,3619
	ctx.r4.s64 = 3619;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// addi r9,r31,348
	ctx.r9.s64 = r31.s64 + 348;
	// mr r28,r11
	r28.u64 = r11.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// addi r7,r31,356
	ctx.r7.s64 = r31.s64 + 356;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// li r27,3626
	r27.s64 = 3626;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// mr r26,r11
	r26.u64 = r11.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3625
	r25.s64 = 3625;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// mr r24,r11
	r24.u64 = r11.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3629
	r23.s64 = 3629;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// mr r22,r11
	r22.u64 = r11.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,336
	ctx.r10.s64 = r31.s64 + 336;
	// rlwinm r6,r10,12,20,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r5,r10,3
	ctx.r5.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r6,512
	ctx.r10.s64 = ctx.r6.s64 + 512;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,344
	ctx.r8.s64 = r31.s64 + 344;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// ori r5,r6,2
	ctx.r5.u64 = ctx.r6.u64 | 2;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// addi r7,r31,372
	ctx.r7.s64 = r31.s64 + 372;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,364
	ctx.r9.s64 = r31.s64 + 364;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// li r27,3635
	r27.s64 = 3635;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// addi r6,r31,368
	ctx.r6.s64 = r31.s64 + 368;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3634
	r25.s64 = 3634;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// li r8,3628
	ctx.r8.s64 = 3628;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3638
	r23.s64 = 3638;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,352
	ctx.r10.s64 = r31.s64 + 352;
	// rlwinm r4,r10,12,20,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r5,r10,3
	ctx.r5.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r4,512
	ctx.r10.s64 = ctx.r4.s64 + 512;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// rlwinm r4,r9,12,20,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r22,3637
	r22.s64 = 3637;
	// addi r4,r4,512
	ctx.r4.s64 = ctx.r4.s64 + 512;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,360
	ctx.r8.s64 = r31.s64 + 360;
	// rlwinm r5,r4,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x1000;
	// ori r4,r10,2
	ctx.r4.u64 = ctx.r10.u64 | 2;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// ori r5,r9,2
	ctx.r5.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// clrlwi r9,r6,3
	ctx.r9.u64 = ctx.r6.u32 & 0x1FFFFFFF;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// li r28,3644
	r28.s64 = 3644;
	// mr r27,r11
	r27.u64 = r11.u64;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// addi r7,r31,384
	ctx.r7.s64 = r31.s64 + 384;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// li r26,3643
	r26.s64 = 3643;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// mr r25,r11
	r25.u64 = r11.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// li r24,3647
	r24.s64 = 3647;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// mr r23,r11
	r23.u64 = r11.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r6,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r31,380
	ctx.r10.s64 = r31.s64 + 380;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r6,r10,12,20,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// clrlwi r5,r10,3
	ctx.r5.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r6,512
	ctx.r10.s64 = ctx.r6.s64 + 512;
	// addi r9,r31,376
	ctx.r9.s64 = r31.s64 + 376;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r5,r6,2
	ctx.r5.u64 = ctx.r6.u64 | 2;
	// rlwinm r6,r10,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,388
	ctx.r8.s64 = r31.s64 + 388;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// li r22,3646
	r22.s64 = 3646;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// ori r9,r10,2
	ctx.r9.u64 = ctx.r10.u64 | 2;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,396
	ctx.r10.s64 = r31.s64 + 396;
	// li r28,3544
	r28.s64 = 3544;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// addi r9,r31,392
	ctx.r9.s64 = r31.s64 + 392;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r26,3545
	r26.s64 = 3545;
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// ori r6,r6,2
	ctx.r6.u64 = ctx.r6.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,404
	ctx.r8.s64 = r31.s64 + 404;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// ori r5,r9,2
	ctx.r5.u64 = ctx.r9.u64 | 2;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r24,3848
	r24.s64 = 3848;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r7,r31,400
	ctx.r7.s64 = r31.s64 + 400;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// li r22,3849
	r22.s64 = 3849;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r4,3850
	ctx.r4.s64 = 3850;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,408
	ctx.r9.s64 = r31.s64 + 408;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// addi r7,r31,416
	ctx.r7.s64 = r31.s64 + 416;
	// mr r28,r11
	r28.u64 = r11.u64;
	// li r27,3851
	r27.s64 = 3851;
	// mr r26,r11
	r26.u64 = r11.u64;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,3852
	r25.s64 = 3852;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// mr r24,r11
	r24.u64 = r11.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,3853
	r23.s64 = 3853;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// li r22,3854
	r22.s64 = 3854;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,412
	ctx.r10.s64 = r31.s64 + 412;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r8,r31,420
	ctx.r8.s64 = r31.s64 + 420;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// addi r7,r31,432
	ctx.r7.s64 = r31.s64 + 432;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// addi r9,r31,424
	ctx.r9.s64 = r31.s64 + 424;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// li r27,3855
	r27.s64 = 3855;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// addi r6,r31,444
	ctx.r6.s64 = r31.s64 + 444;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,2071
	r25.s64 = 2071;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,436
	ctx.r8.s64 = r31.s64 + 436;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,2070
	r23.s64 = 2070;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,428
	ctx.r10.s64 = r31.s64 + 428;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r4,r10,3
	ctx.r4.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// rlwinm r5,r10,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// rlwinm r10,r9,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// ori r4,r5,2
	ctx.r4.u64 = ctx.r5.u64 | 2;
	// rlwinm r5,r10,0,19,19
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r5,r9,2
	ctx.r5.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r4,r9,2
	ctx.r4.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r7,3
	ctx.r8.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r6,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 2;
	// li r22,2135
	r22.s64 = 2135;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// li r28,2134
	r28.s64 = 2134;
	// mr r27,r11
	r27.u64 = r11.u64;
	// addi r7,r31,460
	ctx.r7.s64 = r31.s64 + 460;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// li r26,2586
	r26.s64 = 2586;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// mr r25,r11
	r25.u64 = r11.u64;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// li r24,2585
	r24.s64 = 2585;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// mr r23,r11
	r23.u64 = r11.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// clrlwi r9,r6,3
	ctx.r9.u64 = ctx.r6.u32 & 0x1FFFFFFF;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r9,r31,452
	ctx.r9.s64 = r31.s64 + 452;
	// ori r8,r10,2
	ctx.r8.u64 = ctx.r10.u64 | 2;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,440
	ctx.r10.s64 = r31.s64 + 440;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// li r22,2589
	r22.s64 = 2589;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,448
	ctx.r8.s64 = r31.s64 + 448;
	// rlwinm r5,r10,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r5,512
	ctx.r10.s64 = ctx.r5.s64 + 512;
	// rlwinm r5,r9,12,20,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// addi r5,r5,512
	ctx.r5.s64 = ctx.r5.s64 + 512;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// rlwinm r6,r5,0,19,19
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r10,r8,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// ori r6,r9,2
	ctx.r6.u64 = ctx.r9.u64 | 2;
	// rlwinm r9,r10,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r8,r8,3
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r10,r7,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// ori r8,r9,2
	ctx.r8.u64 = ctx.r9.u64 | 2;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r7,3
	ctx.r9.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mr r28,r11
	r28.u64 = r11.u64;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// mr r26,r11
	r26.u64 = r11.u64;
	// li r4,2588
	ctx.r4.s64 = 2588;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// addi r9,r31,468
	ctx.r9.s64 = r31.s64 + 468;
	// li r27,2592
	r27.s64 = 2592;
	// addi r7,r31,476
	ctx.r7.s64 = r31.s64 + 476;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// li r25,2591
	r25.s64 = 2591;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// mr r24,r11
	r24.u64 = r11.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// addi r8,r31,464
	ctx.r8.s64 = r31.s64 + 464;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// li r23,74
	r23.s64 = 74;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// mr r22,r11
	r22.u64 = r11.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// addi r10,r31,456
	ctx.r10.s64 = r31.s64 + 456;
	// rlwinm r11,r10,12,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r6,r10,3
	ctx.r6.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// rlwinm r11,r9,12,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// ori r6,r10,2
	ctx.r6.u64 = ctx.r10.u64 | 2;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// rlwinm r10,r11,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r9,3
	ctx.r9.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// rlwinm r11,r8,12,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// ori r5,r10,2
	ctx.r5.u64 = ctx.r10.u64 | 2;
	// rlwinm r10,r11,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// clrlwi r9,r8,3
	ctx.r9.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// rlwinm r11,r7,12,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFF;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// ori r9,r10,2
	ctx.r9.u64 = ctx.r10.u64 | 2;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// clrlwi r10,r7,3
	ctx.r10.u64 = ctx.r7.u32 & 0x1FFFFFFF;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// li r8,73
	ctx.r8.s64 = 73;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// addi r11,r31,472
	r11.s64 = r31.s64 + 472;
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// stwu r22,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r22.u32);
	ctx.r3.u32 = ea;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,10908(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 10908);
	// stw r3,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r3.u32);
	// stw r11,12(r29)
	PPC_STORE_U32(r29.u32 + 12, r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x823ed16c
	return;
}

__attribute__((alias("__imp__sub_821A0618"))) PPC_WEAK_FUNC(sub_821A0618);
PPC_FUNC_IMPL(__imp__sub_821A0618) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addic. r3,r11,-1
	xer.ca = r11.u32 > 0;
	ctx.r3.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// bne 0x821a0668
	if (!cr0.getEQ()) goto loc_821A0668;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a064c
	if (cr0.getEQ()) goto loc_821A064C;
	// bl 0x8219d298
	sub_8219D298(ctx, base);
loc_821A064C:
	// lis r4,-20096
	ctx.r4.s64 = -1317011456;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_821A0668:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A0680"))) PPC_WEAK_FUNC(sub_821A0680);
PPC_FUNC_IMPL(__imp__sub_821A0680) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a06a8
	if (cr0.getEQ()) goto loc_821A06A8;
	// bl 0x8219d298
	sub_8219D298(ctx, base);
loc_821A06A8:
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r5,480
	ctx.r5.s64 = 480;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r31,480
	ctx.r4.s64 = r31.s64 + 480;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x821a06e0
	if (cr6.getEQ()) goto loc_821A06E0;
	// lis r11,10922
	r11.s64 = 715784192;
	// ori r11,r11,43690
	r11.u64 = r11.u64 | 43690;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
loc_821A06E0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821A06F0"))) PPC_WEAK_FUNC(sub_821A06F0);
PPC_FUNC_IMPL(__imp__sub_821A06F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed134
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
loc_821A0710:
	// lwz r11,52(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 52);
	// twllei r30,0
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// divw. r11,r10,r30
	r11.s32 = ctx.r10.s32 / r30.s32;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r10,r30,r10
	ctx.r10.u64 = r30.u64 & ~ctx.r10.u64;
	// twlgei r10,-1
	// ble 0x821a0780
	if (!cr0.getGT()) goto loc_821A0780;
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// blt cr6,0x821a074c
	if (cr6.getLT()) goto loc_821A074C;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_821A074C:
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// subf r31,r11,r31
	r31.s64 = r31.s64 - r11.s64;
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// or r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 | r28.u64;
	// stwu r10,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r4.u32 = ea;
	// beq cr6,0x821a077c
	if (cr6.getEQ()) goto loc_821A077C;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821A076C:
	// lwzu r9,4(r27)
	ea = 4 + r27.u32;
	ctx.r9.u64 = PPC_LOAD_U32(ea);
	r27.u32 = ea;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stwu r9,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r4.u32 = ea;
	// bne 0x821a076c
	if (!cr0.getEQ()) goto loc_821A076C;
loc_821A077C:
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
loc_821A0780:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821a079c
	if (cr6.getEQ()) goto loc_821A079C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r4,48(r29)
	PPC_STORE_U32(r29.u32 + 48, ctx.r4.u32);
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// lwz r4,48(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 48);
	// b 0x821a0710
	goto loc_821A0710;
loc_821A079C:
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed184
	return;
}

__attribute__((alias("__imp__sub_821A07A8"))) PPC_WEAK_FUNC(sub_821A07A8);
PPC_FUNC_IMPL(__imp__sub_821A07A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r11,12684(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12684);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a07e4
	if (cr0.getEQ()) goto loc_821A07E4;
	// lwz r10,64(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,48(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm r8,r11,5,31,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
loc_821A07E4:
	// lwz r9,11844(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 11844);
	// lwz r10,10548(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10548);
	// rlwinm r9,r9,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// rlwinm r7,r10,31,1,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r6,10560(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 10560);
	// rlwinm r11,r11,27,5,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// rlwimi r11,r6,0,0,30
	r11.u64 = (__builtin_rotateleft32(ctx.r6.u32, 0) & 0xFFFFFFFE) | (r11.u64 & 0xFFFFFFFF00000001);
	// stw r11,10560(r31)
	PPC_STORE_U32(r31.u32 + 10560, r11.u32);
	// rlwinm. r5,r9,0,30,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne 0x821a081c
	if (!cr0.getEQ()) goto loc_821A081C;
	// clrlwi r11,r9,31
	r11.u64 = ctx.r9.u32 & 0x1;
	// b 0x821a08c0
	goto loc_821A08C0;
loc_821A081C:
	// rlwinm. r6,r11,0,27,27
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// rlwinm r9,r10,28,29,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x7;
	// bne 0x821a0838
	if (!cr0.getEQ()) goto loc_821A0838;
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// beq cr6,0x821a0854
	if (cr6.getEQ()) goto loc_821A0854;
	// cmplwi cr6,r9,3
	cr6.compare<uint32_t>(ctx.r9.u32, 3, xer);
	// b 0x821a0844
	goto loc_821A0844;
loc_821A0838:
	// cmplwi cr6,r9,4
	cr6.compare<uint32_t>(ctx.r9.u32, 4, xer);
	// beq cr6,0x821a0854
	if (cr6.getEQ()) goto loc_821A0854;
	// cmplwi cr6,r9,6
	cr6.compare<uint32_t>(ctx.r9.u32, 6, xer);
loc_821A0844:
	// beq cr6,0x821a0854
	if (cr6.getEQ()) goto loc_821A0854;
	// cmplwi cr6,r9,2
	cr6.compare<uint32_t>(ctx.r9.u32, 2, xer);
	// li r9,0
	ctx.r9.s64 = 0;
	// bne cr6,0x821a0858
	if (!cr6.getEQ()) goto loc_821A0858;
loc_821A0854:
	// li r9,1
	ctx.r9.s64 = 1;
loc_821A0858:
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// clrlwi. r6,r10,31
	ctx.r6.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// and r11,r8,r11
	r11.u64 = ctx.r8.u64 & r11.u64;
	// and r11,r11,r7
	r11.u64 = r11.u64 & ctx.r7.u64;
	// and r11,r11,r9
	r11.u64 = r11.u64 & ctx.r9.u64;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// beq 0x821a08c0
	if (cr0.getEQ()) goto loc_821A08C0;
	// rlwinm r9,r10,0,18,20
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3800;
	// rlwinm r8,r10,0,12,14
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE0000;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// rlwinm. r6,r10,0,24,24
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// and r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ctx.r8.u64;
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
	// beq 0x821a08c0
	if (cr0.getEQ()) goto loc_821A08C0;
	// rlwinm r9,r10,0,0,2
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xE0000000;
	// rlwinm r8,r10,0,6,8
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3800000;
	// cntlzw r9,r9
	ctx.r9.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// and r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ctx.r8.u64;
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
loc_821A08C0:
	// lwz r9,10560(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 10560);
	// rlwimi r9,r11,1,30,30
	ctx.r9.u64 = (__builtin_rotateleft32(r11.u32, 1) & 0x2) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFFFD);
	// stw r9,10560(r31)
	PPC_STORE_U32(r31.u32 + 10560, ctx.r9.u32);
	// rlwinm. r11,r9,0,30,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a0934
	if (cr0.getEQ()) goto loc_821A0934;
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// rlwinm. r10,r11,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a098c
	if (cr0.getEQ()) goto loc_821A098C;
	// lwz r10,13488(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 13488);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x821a08f8
	if (cr6.getEQ()) goto loc_821A08F8;
	// lwz r10,13496(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 13496);
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bne cr6,0x821a098c
	if (!cr6.getEQ()) goto loc_821A098C;
loc_821A08F8:
	// andi. r11,r11,251
	r11.u64 = r11.u64 & 251;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r3,r10
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r10.u32, xer);
	// stb r11,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, r11.u8);
	// ble cr6,0x821a0918
	if (!cr6.getGT()) goto loc_821A0918;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821A0918:
	// lis r11,-16384
	r11.s64 = -1073741824;
	// li r10,15
	ctx.r10.s64 = 15;
	// ori r11,r11,17920
	r11.u64 = r11.u64 | 17920;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// b 0x821a098c
	goto loc_821A098C;
loc_821A0934:
	// clrlwi. r11,r7,31
	r11.u64 = ctx.r7.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a098c
	if (cr0.getEQ()) goto loc_821A098C;
	// rlwinm. r11,r10,0,29,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a098c
	if (cr0.getEQ()) goto loc_821A098C;
	// rlwinm r11,r10,28,29,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x7;
	// cmplwi cr6,r11,7
	cr6.compare<uint32_t>(r11.u32, 7, xer);
	// beq cr6,0x821a0980
	if (cr6.getEQ()) goto loc_821A0980;
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// beq cr6,0x821a0980
	if (cr6.getEQ()) goto loc_821A0980;
	// rlwinm. r10,r9,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a0970
	if (!cr0.getEQ()) goto loc_821A0970;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// beq cr6,0x821a0980
	if (cr6.getEQ()) goto loc_821A0980;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// b 0x821a097c
	goto loc_821A097C;
loc_821A0970:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x821a0980
	if (cr6.getEQ()) goto loc_821A0980;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
loc_821A097C:
	// bne cr6,0x821a098c
	if (!cr6.getEQ()) goto loc_821A098C;
loc_821A0980:
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// ori r11,r11,4
	r11.u64 = r11.u64 | 4;
	// stb r11,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, r11.u8);
loc_821A098C:
	// ori r3,r30,256
	ctx.r3.u64 = r30.u64 | 256;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A09A8"))) PPC_WEAK_FUNC(sub_821A09A8);
PPC_FUNC_IMPL(__imp__sub_821A09A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed118
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r21,r4
	r21.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a09d8
	if (!cr6.getGT()) goto loc_821A09D8;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A09D8:
	// li r10,-1
	ctx.r10.s64 = -1;
	// rlwinm r9,r21,0,23,23
	ctx.r9.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 0) & 0x100;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// mr r22,r10
	r22.u64 = ctx.r10.u64;
	// mr r20,r10
	r20.u64 = ctx.r10.u64;
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// ori r25,r10,24832
	r25.u64 = ctx.r10.u64 | 24832;
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// ori r28,r10,24576
	r28.u64 = ctx.r10.u64 | 24576;
	// lis r10,-16383
	ctx.r10.s64 = -1073676288;
	// ori r26,r10,11521
	r26.u64 = ctx.r10.u64 | 11521;
	// lis r10,4
	ctx.r10.s64 = 262144;
	// ori r24,r10,515
	r24.u64 = ctx.r10.u64 | 515;
	// beq cr6,0x821a0c1c
	if (cr6.getEQ()) goto loc_821A0C1C;
	// lbz r10,10943(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// rlwinm. r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a0bdc
	if (cr0.getEQ()) goto loc_821A0BDC;
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r9,r10,0,27,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x821a0a34
	if (cr0.getEQ()) goto loc_821A0A34;
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x821a0ac4
	goto loc_821A0AC4;
loc_821A0A34:
	// rlwinm. r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a0abc
	if (cr0.getEQ()) goto loc_821A0ABC;
	// lwz r10,12432(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12432);
	// lwz r9,12720(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12720);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x821a0a54
	if (cr6.getEQ()) goto loc_821A0A54;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x821a0abc
	if (!cr6.getEQ()) goto loc_821A0ABC;
loc_821A0A54:
	// lwz r10,12436(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12436);
	// lwz r9,12724(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12724);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x821a0a6c
	if (cr6.getEQ()) goto loc_821A0A6C;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x821a0abc
	if (!cr6.getEQ()) goto loc_821A0ABC;
loc_821A0A6C:
	// lwz r10,12440(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12440);
	// lwz r9,12728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12728);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x821a0a84
	if (cr6.getEQ()) goto loc_821A0A84;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x821a0abc
	if (!cr6.getEQ()) goto loc_821A0ABC;
loc_821A0A84:
	// lwz r10,12444(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12444);
	// lwz r9,12732(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12732);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x821a0a9c
	if (cr6.getEQ()) goto loc_821A0A9C;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x821a0abc
	if (!cr6.getEQ()) goto loc_821A0ABC;
loc_821A0A9C:
	// lwz r10,12448(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12448);
	// lwz r9,12736(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12736);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x821a0ab4
	if (cr6.getEQ()) goto loc_821A0AB4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x821a0abc
	if (!cr6.getEQ()) goto loc_821A0ABC;
loc_821A0AB4:
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x821a0ac0
	goto loc_821A0AC0;
loc_821A0ABC:
	// li r10,0
	ctx.r10.s64 = 0;
loc_821A0AC0:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
loc_821A0AC4:
	// clrlwi. r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a0bdc
	if (cr0.getEQ()) goto loc_821A0BDC;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// lwz r10,12740(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// addic. r30,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r30.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821a0b7c
	if (cr0.getLT()) goto loc_821A0B7C;
loc_821A0AE8:
	// lwz r10,11844(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 11844);
	// addi r9,r30,3276
	ctx.r9.s64 = r30.s64 + 3276;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// rlwimi r27,r9,17,0,14
	r27.u64 = (__builtin_rotateleft32(ctx.r9.u32, 17) & 0xFFFE0000) | (r27.u64 & 0xFFFFFFFF0001FFFF);
	// mr r29,r27
	r29.u64 = r27.u64;
	// rlwinm. r10,r10,15,29,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0x7;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a0b24
	if (cr0.getEQ()) goto loc_821A0B24;
	// cmplwi cr6,r10,2
	cr6.compare<uint32_t>(ctx.r10.u32, 2, xer);
	// bne cr6,0x821a0b28
	if (!cr6.getEQ()) goto loc_821A0B28;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x821a0b24
	if (!cr6.getEQ()) goto loc_821A0B24;
	// lbz r10,10943(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// rlwinm. r10,r10,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a0b28
	if (!cr0.getEQ()) goto loc_821A0B28;
loc_821A0B24:
	// rlwinm r29,r27,0,0,30
	r29.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
loc_821A0B28:
	// rlwinm r9,r30,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// li r8,3
	ctx.r8.s64 = 3;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// slw r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r9.u8 & 0x3F));
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r29,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r29.u32);
	r11.u32 = ea;
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// ble cr6,0x821a0b70
	if (!cr6.getGT()) goto loc_821A0B70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A0B70:
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bge 0x821a0ae8
	if (!cr0.getLT()) goto loc_821A0AE8;
	// b 0x821a0b80
	goto loc_821A0B80;
loc_821A0B7C:
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_821A0B80:
	// cmplw cr6,r29,r27
	cr6.compare<uint32_t>(r29.u32, r27.u32, xer);
	// beq cr6,0x821a0bb8
	if (cr6.getEQ()) goto loc_821A0BB8;
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r10,r10,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a0bb8
	if (cr0.getEQ()) goto loc_821A0BB8;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// li r9,1
	ctx.r9.s64 = 1;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r27,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r27.u32);
	r11.u32 = ea;
loc_821A0BB8:
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,12700(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12700);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// lwz r10,12704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12704);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// b 0x821a0c14
	goto loc_821A0C14;
loc_821A0BDC:
	// lwz r9,11844(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 11844);
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// rlwinm. r9,r9,0,12,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xE0000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x821a0bf0
	if (!cr0.getEQ()) goto loc_821A0BF0;
	// rlwinm r10,r27,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFFFE;
loc_821A0BF0:
	// li r9,8707
	ctx.r9.s64 = 8707;
	// cmplw cr6,r10,r27
	cr6.compare<uint32_t>(ctx.r10.u32, r27.u32, xer);
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// beq cr6,0x821a0c14
	if (cr6.getEQ()) goto loc_821A0C14;
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r10,r10,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a0c14
	if (cr0.getEQ()) goto loc_821A0C14;
	// mr r23,r27
	r23.u64 = r27.u64;
loc_821A0C14:
	// li r12,-257
	r12.s64 = -257;
	// and r21,r21,r12
	r21.u64 = r21.u64 & r12.u64;
loc_821A0C1C:
	// ld r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 40);
	// li r12,1
	r12.s64 = 1;
	// and r9,r10,r21
	ctx.r9.u64 = ctx.r10.u64 & r21.u64;
	// rldicr r12,r12,57,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 57) & 0xFFFFFFFFFFFFFFFF;
	// and r10,r9,r12
	ctx.r10.u64 = ctx.r9.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821a0c5c
	if (cr6.getEQ()) goto loc_821A0C5C;
	// li r8,8192
	ctx.r8.s64 = 8192;
	// lwz r10,10368(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10368);
	// li r12,-2
	r12.s64 = -2;
	// rldicr r12,r12,57,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 57) & 0xFFFFFFFFFFFFFFFF;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// and r21,r21,r12
	r21.u64 = r21.u64 & r12.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r22,13168(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 13168);
	// rlwimi r22,r10,0,0,17
	r22.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFC000) | (r22.u64 & 0xFFFFFFFF00003FFF);
loc_821A0C5C:
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,37,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 37) & 0xFFFFFFFFFFFFFFFF;
	// and r10,r9,r12
	ctx.r10.u64 = ctx.r9.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821a0c90
	if (cr6.getEQ()) goto loc_821A0C90;
	// li r10,8452
	ctx.r10.s64 = 8452;
	// li r12,-2
	r12.s64 = -2;
	// li r20,0
	r20.s64 = 0;
	// rldicr r12,r12,37,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 37) & 0xFFFFFFFFFFFFFFFF;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// and r21,r21,r12
	r21.u64 = r21.u64 & r12.u64;
	// lwz r10,10460(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10460);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
loc_821A0C90:
	// and r10,r20,r22
	ctx.r10.u64 = r20.u64 & r22.u64;
	// and r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 & r23.u64;
	// cmpwi cr6,r10,-1
	cr6.compare<int32_t>(ctx.r10.s32, -1, xer);
	// beq cr6,0x821a0d2c
	if (cr6.getEQ()) goto loc_821A0D2C;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lis r8,5461
	ctx.r8.s64 = 357892096;
	// cmpwi cr6,r23,-1
	cr6.compare<int32_t>(r23.s32, -1, xer);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// ori r8,r8,21845
	ctx.r8.u64 = ctx.r8.u64 | 21845;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// beq cr6,0x821a0cdc
	if (cr6.getEQ()) goto loc_821A0CDC;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r24,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r24.u32);
	r11.u32 = ea;
	// stwu r23,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r23.u32);
	r11.u32 = ea;
loc_821A0CDC:
	// cmpwi cr6,r22,-1
	cr6.compare<int32_t>(r22.s32, -1, xer);
	// beq cr6,0x821a0cf8
	if (cr6.getEQ()) goto loc_821A0CF8;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// lis r9,4
	ctx.r9.s64 = 262144;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r22,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r22.u32);
	r11.u32 = ea;
loc_821A0CF8:
	// cmpwi cr6,r20,-1
	cr6.compare<int32_t>(r20.s32, -1, xer);
	// beq cr6,0x821a0d14
	if (cr6.getEQ()) goto loc_821A0D14;
	// lis r10,4
	ctx.r10.s64 = 262144;
	// stwu r26,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r26.u32);
	r11.u32 = ea;
	// ori r10,r10,260
	ctx.r10.u64 = ctx.r10.u64 | 260;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r20,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r20.u32);
	r11.u32 = ea;
loc_821A0D14:
	// stwu r28,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r28.u32);
	r11.u32 = ea;
	// lwz r10,12700(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12700);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r25,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r25.u32);
	r11.u32 = ea;
	// lwz r10,12704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12704);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
loc_821A0D2C:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x823ed168
	return;
}

__attribute__((alias("__imp__sub_821A0D40"))) PPC_WEAK_FUNC(sub_821A0D40);
PPC_FUNC_IMPL(__imp__sub_821A0D40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// addi r29,r6,-4
	r29.s64 = ctx.r6.s64 + -4;
	// lwz r4,48(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 48);
loc_821A0D5C:
	// cntlzd r11,r31
	r11.u64 = r31.u64 == 0 ? 64 : __builtin_clzll(r31.u64);
	// lwz r9,52(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 52);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r11,r5
	r28.u64 = r11.u64 + ctx.r5.u64;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// sld r31,r31,r8
	r31.u64 = ctx.r8.u8 & 0x40 ? 0 : (r31.u64 << (ctx.r8.u8 & 0x7F));
	// not r11,r31
	r11.u64 = ~r31.u64;
	// cntlzd r30,r11
	r30.u64 = r11.u64 == 0 ? 64 : __builtin_clzll(r11.u64);
	// rlwinm r27,r30,2,0,29
	r27.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r30
	r11.u64 = r30.u64;
	// add r10,r27,r4
	ctx.r10.u64 = r27.u64 + ctx.r4.u64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x821a0dc4
	if (cr6.getLT()) goto loc_821A0DC4;
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x821a06f0
	sub_821A06F0(ctx, base);
	// clrldi r11,r30,32
	r11.u64 = r30.u64 & 0xFFFFFFFF;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// add r29,r27,r29
	r29.u64 = r27.u64 + r29.u64;
	// add r5,r30,r28
	ctx.r5.u64 = r30.u64 + r28.u64;
	// sld r31,r31,r11
	r31.u64 = r11.u8 & 0x40 ? 0 : (r31.u64 << (r11.u8 & 0x7F));
	// b 0x821a0dec
	goto loc_821A0DEC;
loc_821A0DC4:
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// add r5,r30,r28
	ctx.r5.u64 = r30.u64 + r28.u64;
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// or r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 | r28.u64;
	// stwu r10,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r4.u32 = ea;
loc_821A0DD8:
	// lwzu r10,4(r29)
	ea = 4 + r29.u32;
	ctx.r10.u64 = PPC_LOAD_U32(ea);
	r29.u32 = ea;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rldicr r31,r31,1,62
	r31.u64 = __builtin_rotateleft64(r31.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// stwu r10,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r4.u32 = ea;
	// bne 0x821a0dd8
	if (!cr0.getEQ()) goto loc_821A0DD8;
loc_821A0DEC:
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// bne cr6,0x821a0d5c
	if (!cr6.getEQ()) goto loc_821A0D5C;
	// stw r4,48(r26)
	PPC_STORE_U32(r26.u32 + 48, ctx.r4.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821A0E00"))) PPC_WEAK_FUNC(sub_821A0E00);
PPC_FUNC_IMPL(__imp__sub_821A0E00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed124
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r29,9096
	r29.s64 = 9096;
	// addi r31,r30,10272
	r31.s64 = r30.s64 + 10272;
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r10,56(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a0e34
	if (!cr6.getGT()) goto loc_821A0E34;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A0E34:
	// li r10,8199
	ctx.r10.s64 = 8199;
	// li r9,2609
	ctx.r9.s64 = 2609;
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lis r7,1
	ctx.r7.s64 = 65536;
	// li r6,0
	ctx.r6.s64 = 0;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// ori r7,r7,2607
	ctx.r7.u64 = ctx.r7.u64 | 2607;
	// lwz r10,10396(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 10396);
	// li r5,4096
	ctx.r5.s64 = 4096;
	// lis r4,-16380
	ctx.r4.s64 = -1073479680;
	// li r3,3
	ctx.r3.s64 = 3;
	// ori r4,r4,15360
	ctx.r4.u64 = ctx.r4.u64 | 15360;
	// li r27,2609
	r27.s64 = 2609;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// li r26,0
	r26.s64 = 0;
	// lis r24,-32768
	r24.s64 = -2147483648;
	// li r25,8
	r25.s64 = 8;
	// mr r23,r24
	r23.u64 = r24.u64;
	// addi r31,r31,-4
	r31.s64 = r31.s64 + -4;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r6,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	r11.u32 = ea;
	// stwu r5,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	r11.u32 = ea;
	// stwu r4,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	r11.u32 = ea;
	// stwu r3,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	r11.u32 = ea;
	// stwu r27,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r27.u32);
	r11.u32 = ea;
	// stwu r26,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r26.u32);
	r11.u32 = ea;
	// stwu r23,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r23.u32);
	r11.u32 = ea;
	// stwu r25,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r25.u32);
	r11.u32 = ea;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stw r4,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r4.u32);
loc_821A0EB4:
	// cntlzd r11,r28
	r11.u64 = r28.u64 == 0 ? 64 : __builtin_clzll(r28.u64);
	// lwz r9,52(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 52);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// rlwinm r10,r11,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r10,r31
	r31.u64 = ctx.r10.u64 + r31.u64;
	// add r27,r11,r29
	r27.u64 = r11.u64 + r29.u64;
	// sld r28,r28,r8
	r28.u64 = ctx.r8.u8 & 0x40 ? 0 : (r28.u64 << (ctx.r8.u8 & 0x7F));
	// not r11,r28
	r11.u64 = ~r28.u64;
	// cntlzd r26,r11
	r26.u64 = r11.u64 == 0 ? 64 : __builtin_clzll(r11.u64);
	// rlwinm r29,r26,2,0,29
	r29.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r29,2,0,29
	r25.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r25,r4
	r11.u64 = r25.u64 + ctx.r4.u64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x821a0f24
	if (cr6.getLT()) goto loc_821A0F24;
	// li r8,4
	ctx.r8.s64 = 4;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a06f0
	sub_821A06F0(ctx, base);
	// clrldi r11,r26,32
	r11.u64 = r26.u64 & 0xFFFFFFFF;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// add r31,r25,r31
	r31.u64 = r25.u64 + r31.u64;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// sld r28,r28,r11
	r28.u64 = r11.u8 & 0x40 ? 0 : (r28.u64 << (r11.u8 & 0x7F));
	// b 0x821a0f68
	goto loc_821A0F68;
loc_821A0F24:
	// addi r10,r29,-1
	ctx.r10.s64 = r29.s64 + -1;
	// stw r24,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r24.u32);
	// clrlwi r11,r4,29
	r11.u64 = ctx.r4.u32 & 0x7;
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// or r11,r10,r27
	r11.u64 = ctx.r10.u64 | r27.u64;
	// add r29,r29,r27
	r29.u64 = r29.u64 + r27.u64;
	// stwu r11,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r4.u32 = ea;
loc_821A0F44:
	// ld r11,4(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 4);
	// addic. r26,r26,-1
	xer.ca = r26.u32 > 0;
	r26.s64 = r26.s64 + -1;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// ld r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 12);
	// rldicr r28,r28,1,62
	r28.u64 = __builtin_rotateleft64(r28.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// std r11,4(r4)
	PPC_STORE_U64(ctx.r4.u32 + 4, r11.u64);
	// std r10,12(r4)
	PPC_STORE_U64(ctx.r4.u32 + 12, ctx.r10.u64);
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// bne 0x821a0f44
	if (!cr0.getEQ()) goto loc_821A0F44;
loc_821A0F68:
	// cmpldi cr6,r28,0
	cr6.compare<uint64_t>(r28.u64, 0, xer);
	// bne cr6,0x821a0eb4
	if (!cr6.getEQ()) goto loc_821A0EB4;
	// stw r4,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r4.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed174
	return;
}

__attribute__((alias("__imp__sub_821A0F80"))) PPC_WEAK_FUNC(sub_821A0F80);
PPC_FUNC_IMPL(__imp__sub_821A0F80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// addi r11,r28,1152
	r11.s64 = r28.s64 + 1152;
	// li r10,18432
	ctx.r10.s64 = 18432;
	// addi r30,r11,-4
	r30.s64 = r11.s64 + -4;
	// lwz r4,48(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 48);
loc_821A0FA4:
	// cntlzd r11,r31
	r11.u64 = r31.u64 == 0 ? 64 : __builtin_clzll(r31.u64);
	// lwz r8,52(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 52);
	// clrldi r7,r11,32
	ctx.r7.u64 = r11.u64 & 0xFFFFFFFF;
	// mulli r9,r11,24
	ctx.r9.s64 = r11.s64 * 24;
	// mulli r11,r11,6
	r11.s64 = r11.s64 * 6;
	// sld r31,r31,r7
	r31.u64 = ctx.r7.u8 & 0x40 ? 0 : (r31.u64 << (ctx.r7.u8 & 0x7F));
	// add r27,r11,r10
	r27.u64 = r11.u64 + ctx.r10.u64;
	// not r11,r31
	r11.u64 = ~r31.u64;
	// add r30,r9,r30
	r30.u64 = ctx.r9.u64 + r30.u64;
	// cntlzd r26,r11
	r26.u64 = r11.u64 == 0 ? 64 : __builtin_clzll(r11.u64);
	// mulli r29,r26,6
	r29.s64 = r26.s64 * 6;
	// addi r11,r29,5
	r11.s64 = r29.s64 + 5;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x821a1068
	if (cr6.getLT()) goto loc_821A1068;
	// li r8,6
	ctx.r8.s64 = 6;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x821a06f0
	sub_821A06F0(ctx, base);
	// clrldi r9,r26,32
	ctx.r9.u64 = r26.u64 & 0xFFFFFFFF;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// add r30,r11,r30
	r30.u64 = r11.u64 + r30.u64;
	// add r10,r29,r27
	ctx.r10.u64 = r29.u64 + r27.u64;
	// sld r31,r31,r9
	r31.u64 = ctx.r9.u8 & 0x40 ? 0 : (r31.u64 << (ctx.r9.u8 & 0x7F));
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// bne cr6,0x821a0fa4
	if (!cr6.getEQ()) goto loc_821A0FA4;
	// lwz r10,56(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 56);
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// stw r4,48(r28)
	PPC_STORE_U32(r28.u32 + 48, ctx.r4.u32);
	// cmplw cr6,r4,r10
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a103c
	if (!cr6.getGT()) goto loc_821A103C;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A103C:
	// lis r10,2
	ctx.r10.s64 = 131072;
	// li r9,0
	ctx.r9.s64 = 0;
	// ori r10,r10,20480
	ctx.r10.u64 = ctx.r10.u64 | 20480;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stw r11,48(r28)
	PPC_STORE_U32(r28.u32 + 48, r11.u32);
	// b 0x821a10d8
	goto loc_821A10D8;
loc_821A1068:
	// lis r10,-32768
	ctx.r10.s64 = -2147483648;
	// addi r9,r29,-1
	ctx.r9.s64 = r29.s64 + -1;
	// clrlwi r11,r4,29
	r11.u64 = ctx.r4.u32 & 0x7;
	// rlwinm r9,r9,16,0,15
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 16) & 0xFFFF0000;
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// add r4,r11,r4
	ctx.r4.u64 = r11.u64 + ctx.r4.u64;
	// or r11,r9,r27
	r11.u64 = ctx.r9.u64 | r27.u64;
	// add r10,r29,r27
	ctx.r10.u64 = r29.u64 + r27.u64;
	// stwu r11,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r4.u32 = ea;
loc_821A108C:
	// ld r11,4(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 4);
	// addic. r26,r26,-1
	xer.ca = r26.u32 > 0;
	r26.s64 = r26.s64 + -1;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// ld r9,12(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 12);
	// rldicr r31,r31,1,62
	r31.u64 = __builtin_rotateleft64(r31.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// ld r8,20(r30)
	ctx.r8.u64 = PPC_LOAD_U64(r30.u32 + 20);
	// addi r30,r30,24
	r30.s64 = r30.s64 + 24;
	// std r11,4(r4)
	PPC_STORE_U64(ctx.r4.u32 + 4, r11.u64);
	// std r9,12(r4)
	PPC_STORE_U64(ctx.r4.u32 + 12, ctx.r9.u64);
	// std r8,20(r4)
	PPC_STORE_U64(ctx.r4.u32 + 20, ctx.r8.u64);
	// addi r4,r4,24
	ctx.r4.s64 = ctx.r4.s64 + 24;
	// bne 0x821a108c
	if (!cr0.getEQ()) goto loc_821A108C;
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// bne cr6,0x821a0fa4
	if (!cr6.getEQ()) goto loc_821A0FA4;
	// li r11,37
	r11.s64 = 37;
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// rldicr r11,r11,44,19
	r11.u64 = __builtin_rotateleft64(r11.u64, 44) & 0xFFFFF00000000000;
	// stw r10,48(r28)
	PPC_STORE_U32(r28.u32 + 48, ctx.r10.u32);
	// std r11,4(r4)
	PPC_STORE_U64(ctx.r4.u32 + 4, r11.u64);
	// std r11,12(r4)
	PPC_STORE_U64(ctx.r4.u32 + 12, r11.u64);
loc_821A10D8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821A10E0"))) PPC_WEAK_FUNC(sub_821A10E0);
PPC_FUNC_IMPL(__imp__sub_821A10E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed11c
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r31,r6,-4
	r31.s64 = ctx.r6.s64 + -4;
	// lis r25,-32768
	r25.s64 = -2147483648;
	// li r21,4
	r21.s64 = 4;
	// lwz r4,48(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 48);
	// li r22,20
	r22.s64 = 20;
	// li r23,36
	r23.s64 = 36;
	// li r24,52
	r24.s64 = 52;
loc_821A1110:
	// cntlzd r11,r30
	r11.u64 = r30.u64 == 0 ? 64 : __builtin_clzll(r30.u64);
	// lwz r9,52(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 52);
	// clrldi r8,r11,32
	ctx.r8.u64 = r11.u64 & 0xFFFFFFFF;
	// rlwinm r10,r11,6,0,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r31,r10,r31
	r31.u64 = ctx.r10.u64 + r31.u64;
	// add r27,r11,r5
	r27.u64 = r11.u64 + ctx.r5.u64;
	// sld r30,r30,r8
	r30.u64 = ctx.r8.u8 & 0x40 ? 0 : (r30.u64 << (ctx.r8.u8 & 0x7F));
	// not r11,r30
	r11.u64 = ~r30.u64;
	// cntlzd r28,r11
	r28.u64 = r11.u64 == 0 ? 64 : __builtin_clzll(r11.u64);
	// rlwinm r29,r28,4,0,27
	r29.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r11,r29,3
	r11.s64 = r29.s64 + 3;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x821a1184
	if (cr6.getLT()) goto loc_821A1184;
	// li r8,16
	ctx.r8.s64 = 16;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x821a06f0
	sub_821A06F0(ctx, base);
	// clrldi r10,r28,32
	ctx.r10.u64 = r28.u64 & 0xFFFFFFFF;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
	// add r5,r29,r27
	ctx.r5.u64 = r29.u64 + r27.u64;
	// sld r30,r30,r10
	r30.u64 = ctx.r10.u8 & 0x40 ? 0 : (r30.u64 << (ctx.r10.u8 & 0x7F));
	// b 0x821a1228
	goto loc_821A1228;
loc_821A1184:
	// clrlwi r11,r4,28
	r11.u64 = ctx.r4.u32 & 0xF;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bne cr6,0x821a119c
	if (!cr6.getEQ()) goto loc_821A119C;
	// mr r11,r25
	r11.u64 = r25.u64;
	// stwu r11,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r4.u32 = ea;
	// b 0x821a11e0
	goto loc_821A11E0;
loc_821A119C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a11b8
	if (!cr6.getEQ()) goto loc_821A11B8;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// stwu r11,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r4.u32 = ea;
	// stwu r10,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r4.u32 = ea;
	// b 0x821a11e0
	goto loc_821A11E0;
loc_821A11B8:
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bne cr6,0x821a11e0
	if (!cr6.getEQ()) goto loc_821A11E0;
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// stwu r11,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r4.u32 = ea;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
loc_821A11E0:
	// addi r11,r29,-1
	r11.s64 = r29.s64 + -1;
	// add r5,r29,r27
	ctx.r5.u64 = r29.u64 + r27.u64;
	// rlwinm r11,r11,16,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// or r11,r11,r27
	r11.u64 = r11.u64 | r27.u64;
	// stwu r11,4(r4)
	ea = 4 + ctx.r4.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r4.u32 = ea;
loc_821A11F4:
	// lvx128 v0,r31,r21
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r21.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// lvx128 v13,r31,r22
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rldicr r30,r30,1,62
	r30.u64 = __builtin_rotateleft64(r30.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// lvx128 v12,r31,r23
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r31,r24
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r31,64
	r31.s64 = r31.s64 + 64;
	// stvx128 v0,r4,r21
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r21.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r4,r22
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r22.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v12,r4,r23
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v11,r4,r24
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32 + r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r4,64
	ctx.r4.s64 = ctx.r4.s64 + 64;
	// bne 0x821a11f4
	if (!cr0.getEQ()) goto loc_821A11F4;
loc_821A1228:
	// cmpldi cr6,r30,0
	cr6.compare<uint64_t>(r30.u64, 0, xer);
	// bne cr6,0x821a1110
	if (!cr6.getEQ()) goto loc_821A1110;
	// stw r4,48(r26)
	PPC_STORE_U32(r26.u32 + 48, ctx.r4.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x823ed16c
	return;
}

__attribute__((alias("__imp__sub_821A1240"))) PPC_WEAK_FUNC(sub_821A1240);
PPC_FUNC_IMPL(__imp__sub_821A1240) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed118
	// stwu r1,-592(r1)
	ea = -592 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r7,112
	r11.s64 = ctx.r7.s64 + 112;
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r11,r11,r3
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r3.u32);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// addi r11,r11,872
	r11.s64 = r11.s64 + 872;
	// lwz r26,28(r11)
	r26.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// cmplwi r26,0
	cr0.compare<uint32_t>(r26.u32, 0, xer);
	// beq 0x821a1848
	if (cr0.getEQ()) goto loc_821A1848;
	// lwz r9,24(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// mulli r10,r7,416
	ctx.r10.s64 = ctx.r7.s64 * 416;
	// addi r9,r9,9
	ctx.r9.s64 = ctx.r9.s64 + 9;
	// lwz r25,24(r5)
	r25.u64 = PPC_LOAD_U32(ctx.r5.u32 + 24);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// add r30,r9,r11
	r30.u64 = ctx.r9.u64 + r11.u64;
	// addi r10,r10,68
	ctx.r10.s64 = ctx.r10.s64 + 68;
	// mr r27,r30
	r27.u64 = r30.u64;
	// beq cr6,0x821a157c
	if (cr6.getEQ()) goto loc_821A157C;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// lhz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// addi r24,r5,52
	r24.s64 = ctx.r5.s64 + 52;
	// subf r23,r11,r10
	r23.s64 = ctx.r10.s64 - r11.s64;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r4,r11,-17312
	ctx.r4.s64 = r11.s64 + -17312;
loc_821A12BC:
	// add r9,r23,r7
	ctx.r9.u64 = r23.u64 + ctx.r7.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r29,4(r9)
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r28,8(r9)
	r28.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// beq cr6,0x821a1318
	if (cr6.getEQ()) goto loc_821A1318;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm r31,r9,20,28,31
	r31.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 20) & 0xF;
loc_821A12E4:
	// lbz r21,9(r5)
	r21.u64 = PPC_LOAD_U8(ctx.r5.u32 + 9);
	// cmplw cr6,r21,r31
	cr6.compare<uint32_t>(r21.u32, r31.u32, xer);
	// bne cr6,0x821a1300
	if (!cr6.getEQ()) goto loc_821A1300;
	// lbz r21,10(r5)
	r21.u64 = PPC_LOAD_U8(ctx.r5.u32 + 10);
	// rlwinm r20,r9,16,28,31
	r20.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 16) & 0xF;
	// cmplw cr6,r21,r20
	cr6.compare<uint32_t>(r21.u32, r20.u32, xer);
	// beq cr6,0x821a1310
	if (cr6.getEQ()) goto loc_821A1310;
loc_821A1300:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r5,r5,12
	ctx.r5.s64 = ctx.r5.s64 + 12;
	// cmplw cr6,r10,r25
	cr6.compare<uint32_t>(ctx.r10.u32, r25.u32, xer);
	// blt cr6,0x821a12e4
	if (cr6.getLT()) goto loc_821A12E4;
loc_821A1310:
	// cmplw cr6,r10,r25
	cr6.compare<uint32_t>(ctx.r10.u32, r25.u32, xer);
	// blt cr6,0x821a1348
	if (cr6.getLT()) goto loc_821A1348;
loc_821A1318:
	// lis r12,-16442
	r12.s64 = -1077542912;
	// lbz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// li r10,95
	ctx.r10.s64 = 95;
	// ori r12,r12,53247
	r12.u64 = r12.u64 | 53247;
	// clrlwi r9,r8,28
	ctx.r9.u64 = ctx.r8.u32 & 0xF;
	// rlwimi r11,r10,20,2,11
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 20) & 0x3FF00000) | (r11.u64 & 0xFFFFFFFFC00FFFFF);
	// and r10,r29,r12
	ctx.r10.u64 = r29.u64 & r12.u64;
	// rlwinm r8,r28,0,0,0
	ctx.r8.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x80000000;
	// ori r9,r9,37456
	ctx.r9.u64 = ctx.r9.u64 | 37456;
	// oris r10,r10,6
	ctx.r10.u64 = ctx.r10.u64 | 393216;
	// or r5,r8,r5
	ctx.r5.u64 = ctx.r8.u64 | ctx.r5.u64;
	// b 0x821a1468
	goto loc_821A1468;
loc_821A1348:
	// lwz r10,4(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// clrlwi r9,r8,28
	ctx.r9.u64 = ctx.r8.u32 & 0xF;
	// rlwinm r31,r10,16,29,31
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0x7;
	// rlwinm r8,r10,22,26,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 22) & 0x38;
	// rlwinm r21,r10,13,29,31
	r21.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 13) & 0x7;
	// or r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 | r31.u64;
	// rlwinm r31,r10,0,16,21
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFC00;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r20,r10,0,24,25
	r20.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xC0;
	// or r8,r8,r21
	ctx.r8.u64 = ctx.r8.u64 | r21.u64;
	// cmplwi cr6,r20,64
	cr6.compare<uint32_t>(r20.u32, 64, xer);
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// or r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 | r31.u64;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// bne cr6,0x821a140c
	if (!cr6.getEQ()) goto loc_821A140C;
	// clrlwi r8,r9,16
	ctx.r8.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwinm r31,r8,0,0,18
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFE000;
	// cmplwi cr6,r31,24576
	cr6.compare<uint32_t>(r31.u32, 24576, xer);
	// bgt cr6,0x821a13ac
	if (cr6.getGT()) goto loc_821A13AC;
	// rlwinm r9,r8,0,16,18
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xE000;
	// clrlwi r8,r8,19
	ctx.r8.u64 = ctx.r8.u32 & 0x1FFF;
	// xori r9,r9,8192
	ctx.r9.u64 = ctx.r9.u64 ^ 8192;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
loc_821A13AC:
	// clrlwi r8,r9,16
	ctx.r8.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwinm r31,r8,0,19,21
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x1C00;
	// cmplwi cr6,r31,3072
	cr6.compare<uint32_t>(r31.u32, 3072, xer);
	// bgt cr6,0x821a13cc
	if (cr6.getGT()) goto loc_821A13CC;
	// rlwinm r9,r8,0,19,21
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x1C00;
	// andi. r8,r8,58367
	ctx.r8.u64 = ctx.r8.u64 & 58367;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// xori r9,r9,1024
	ctx.r9.u64 = ctx.r9.u64 ^ 1024;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
loc_821A13CC:
	// clrlwi r8,r9,16
	ctx.r8.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwinm r31,r8,0,22,24
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x380;
	// cmplwi cr6,r31,384
	cr6.compare<uint32_t>(r31.u32, 384, xer);
	// bgt cr6,0x821a13ec
	if (cr6.getGT()) goto loc_821A13EC;
	// rlwinm r9,r8,0,22,24
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x380;
	// andi. r8,r8,64639
	ctx.r8.u64 = ctx.r8.u64 & 64639;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// xori r9,r9,128
	ctx.r9.u64 = ctx.r9.u64 ^ 128;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
loc_821A13EC:
	// clrlwi r8,r9,16
	ctx.r8.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwinm r31,r8,0,25,27
	r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x70;
	// cmplwi cr6,r31,48
	cr6.compare<uint32_t>(r31.u32, 48, xer);
	// bgt cr6,0x821a140c
	if (cr6.getGT()) goto loc_821A140C;
	// rlwinm r9,r8,0,25,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x70;
	// andi. r8,r8,65423
	ctx.r8.u64 = ctx.r8.u64 & 65423;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// xori r9,r9,16
	ctx.r9.u64 = ctx.r9.u64 ^ 16;
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
loc_821A140C:
	// rlwinm r21,r10,12,14,19
	r21.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x3F000;
	// lhz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// rlwinm r10,r10,0,22,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x300;
	// lhz r5,2(r5)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r5.u32 + 2);
	// lis r12,-16448
	r12.s64 = -1077936128;
	// or r10,r21,r10
	ctx.r10.u64 = r21.u64 | ctx.r10.u64;
	// ori r12,r12,53247
	r12.u64 = r12.u64 | 53247;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// lbzx r21,r8,r6
	r21.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r6.u32);
	// and r29,r29,r12
	r29.u64 = r29.u64 & r12.u64;
	// lis r31,342
	r31.s64 = 22413312;
	// or r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 | r29.u64;
	// ori r31,r31,86
	r31.u64 = r31.u64 | 86;
	// subfic r29,r8,95
	xer.ca = ctx.r8.u32 <= 95;
	r29.s64 = 95 - ctx.r8.s64;
	// rlwinm r5,r5,6,1,23
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 6) & 0x7FFFFF00;
	// mullw r8,r29,r31
	ctx.r8.s64 = int64_t(r29.s32) * int64_t(r31.s32);
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
	// rlwinm r28,r28,0,0,0
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 0) & 0x80000000;
	// or r5,r5,r21
	ctx.r5.u64 = ctx.r5.u64 | r21.u64;
	// rlwimi r31,r8,11,8,12
	r31.u64 = (__builtin_rotateleft32(ctx.r8.u32, 11) & 0xF80000) | (r31.u64 & 0xFFFFFFFFFF07FFFF);
	// or r5,r5,r28
	ctx.r5.u64 = ctx.r5.u64 | r28.u64;
	// rlwimi r11,r31,1,5,11
	r11.u64 = (__builtin_rotateleft32(r31.u32, 1) & 0x7F00000) | (r11.u64 & 0xFFFFFFFFF80FFFFF);
	// rlwinm r11,r11,0,5,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFC7FFFFFF;
loc_821A1468:
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// stw r5,8(r7)
	PPC_STORE_U32(ctx.r7.u32 + 8, ctx.r5.u32);
	// addic. r3,r3,-1
	xer.ca = ctx.r3.u32 > 0;
	ctx.r3.s64 = ctx.r3.s64 + -1;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ori r8,r9,14
	ctx.r8.u64 = ctx.r9.u64 | 14;
	// rlwinm r9,r10,1,28,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xE;
	// clrlwi r11,r8,16
	r11.u64 = ctx.r8.u32 & 0xFFFF;
	// clrlwi r31,r8,16
	r31.u64 = ctx.r8.u32 & 0xFFFF;
	// clrlwi r29,r8,16
	r29.u64 = ctx.r8.u32 & 0xFFFF;
	// clrlwi r28,r8,16
	r28.u64 = ctx.r8.u32 & 0xFFFF;
	// lhzx r9,r9,r4
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + ctx.r4.u32);
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// and r11,r9,r11
	r11.u64 = ctx.r9.u64 & r11.u64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// rlwinm r9,r11,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// or r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 | r11.u64;
	// rlwinm r9,r9,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFF;
	// or r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 | r11.u64;
	// rlwinm r9,r9,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFF;
	// or r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 | r11.u64;
	// rlwinm r9,r9,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFF;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// rlwimi r10,r11,31,29,31
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 31) & 0x7) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFF8);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// rlwinm r11,r9,30,28,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0xE;
	// lhzx r11,r11,r4
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r4.u32);
	// and r11,r11,r31
	r11.u64 = r11.u64 & r31.u64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// rlwinm r10,r11,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// rlwinm r31,r11,2,0,29
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// or r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r10,r10,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	// or r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r10,r10,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// or r10,r11,r31
	ctx.r10.u64 = r11.u64 | r31.u64;
	// rlwimi r10,r9,0,29,25
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0xFFFFFFFFFFFFFFC7) | (ctx.r10.u64 & 0x38);
	// rlwinm r11,r10,27,28,30
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0xE;
	// lhzx r11,r11,r4
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r4.u32);
	// and r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// rlwinm r9,r11,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// rlwinm r31,r11,3,0,28
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// or r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 | r11.u64;
	// or r31,r31,r11
	r31.u64 = r31.u64 | r11.u64;
	// rlwinm r9,r9,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFF;
	// rlwinm r31,r31,2,0,29
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// or r9,r11,r31
	ctx.r9.u64 = r11.u64 | r31.u64;
	// rlwimi r9,r10,0,26,22
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFFFFFFFE3F) | (ctx.r9.u64 & 0x1C0);
	// rlwinm r11,r9,24,28,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xE;
	// lhzx r11,r11,r4
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r4.u32);
	// and r11,r11,r28
	r11.u64 = r11.u64 & r28.u64;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r31,r11,29,3,31
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// or r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 | r11.u64;
	// or r31,r31,r11
	r31.u64 = r31.u64 | r11.u64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r31,r31,31,1,31
	r31.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 31) & 0x7FFFFFFF;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// or r11,r11,r31
	r11.u64 = r11.u64 | r31.u64;
	// rlwimi r11,r9,0,23,19
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0xFFFFFFFFFFFFF1FF) | (r11.u64 & 0xE00);
	// stw r11,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, r11.u32);
	// addi r7,r7,12
	ctx.r7.s64 = ctx.r7.s64 + 12;
	// bne 0x821a12bc
	if (!cr0.getEQ()) goto loc_821A12BC;
loc_821A157C:
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r27,r30
	r27.u64 = r30.u64;
	// add r26,r11,r30
	r26.u64 = r11.u64 + r30.u64;
	// cmplw cr6,r30,r26
	cr6.compare<uint32_t>(r30.u32, r26.u32, xer);
	// bge cr6,0x821a180c
	if (!cr6.getLT()) goto loc_821A180C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r25,r11,-17296
	r25.s64 = r11.s64 + -17296;
loc_821A1598:
	// cmplw cr6,r27,r26
	cr6.compare<uint32_t>(r27.u32, r26.u32, xer);
	// bge cr6,0x821a180c
	if (!cr6.getLT()) goto loc_821A180C;
loc_821A15A0:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm. r11,r11,0,10,11
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x300000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a15b8
	if (!cr0.getEQ()) goto loc_821A15B8;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmplw cr6,r27,r26
	cr6.compare<uint32_t>(r27.u32, r26.u32, xer);
	// blt cr6,0x821a15a0
	if (cr6.getLT()) goto loc_821A15A0;
loc_821A15B8:
	// cmplw cr6,r27,r26
	cr6.compare<uint32_t>(r27.u32, r26.u32, xer);
	// bge cr6,0x821a180c
	if (!cr6.getLT()) goto loc_821A180C;
	// addi r11,r27,4
	r11.s64 = r27.s64 + 4;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bge cr6,0x821a180c
	if (!cr6.getLT()) goto loc_821A180C;
loc_821A15D0:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r9,r9,0,10,11
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x300000;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x821a15e8
	if (!cr0.getEQ()) goto loc_821A15E8;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// blt cr6,0x821a15d0
	if (cr6.getLT()) goto loc_821A15D0;
loc_821A15E8:
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bge cr6,0x821a180c
	if (!cr6.getLT()) goto loc_821A180C;
	// subf r8,r30,r10
	ctx.r8.s64 = ctx.r10.s64 - r30.s64;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// srawi r8,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 2;
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// mulli r10,r8,12
	ctx.r10.s64 = ctx.r8.s64 * 12;
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r5,0,10,11
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x300000;
	// lis r7,48
	ctx.r7.s64 = 3145728;
	// addi r9,r6,-1
	ctx.r9.s64 = ctx.r6.s64 + -1;
	// addi r27,r11,4
	r27.s64 = r11.s64 + 4;
	// li r31,-1
	r31.s64 = -1;
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// bne cr6,0x821a1660
	if (!cr6.getEQ()) goto loc_821A1660;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// mulli r11,r11,12
	r11.s64 = r11.s64 * 12;
	// lwzx r11,r11,r8
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// rlwinm r9,r11,14,25,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 14) & 0x7C;
	// rlwinm r11,r11,7,30,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x3;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// or r31,r9,r11
	r31.u64 = ctx.r9.u64 | r11.u64;
loc_821A1660:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x821a1754
	if (cr6.getEQ()) goto loc_821A1754;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
loc_821A1674:
	// cmplw cr6,r4,r5
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r5.u32, xer);
	// bge cr6,0x821a1744
	if (!cr6.getLT()) goto loc_821A1744;
	// addi r10,r11,12
	ctx.r10.s64 = r11.s64 + 12;
	// subf r29,r4,r5
	r29.s64 = ctx.r5.s64 - ctx.r4.s64;
loc_821A1684:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r7,r9,14,25,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 14) & 0x7C;
	// rlwinm r9,r9,7,30,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0x3;
	// rlwinm r24,r8,14,25,29
	r24.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 14) & 0x7C;
	// rlwinm r23,r8,7,30,31
	r23.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0x3;
	// or r8,r7,r9
	ctx.r8.u64 = ctx.r7.u64 | ctx.r9.u64;
	// or r7,r24,r23
	ctx.r7.u64 = r24.u64 | r23.u64;
	// subf. r9,r8,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r8.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x821a16c4
	if (cr0.getEQ()) goto loc_821A16C4;
	// cmpw cr6,r31,r8
	cr6.compare<int32_t>(r31.s32, ctx.r8.s32, xer);
	// beq cr6,0x821a1708
	if (cr6.getEQ()) goto loc_821A1708;
	// cmpw cr6,r31,r7
	cr6.compare<int32_t>(r31.s32, ctx.r7.s32, xer);
	// bne cr6,0x821a1700
	if (!cr6.getEQ()) goto loc_821A1700;
	// li r9,-1
	ctx.r9.s64 = -1;
	// b 0x821a1700
	goto loc_821A1700;
loc_821A16C4:
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r8,8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r9,r9,9
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1FF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 9;
	// srawi r8,r8,9
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1FF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 9;
	// subf. r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x821a1700
	if (!cr0.getEQ()) goto loc_821A1700;
	// lhz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// lhz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 4);
	// clrlwi r9,r9,26
	ctx.r9.u64 = ctx.r9.u32 & 0x3F;
	// clrlwi r8,r8,26
	ctx.r8.u64 = ctx.r8.u32 & 0x3F;
	// lbzx r9,r9,r25
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + r25.u32);
	// lbzx r8,r8,r25
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + r25.u32);
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
loc_821A1700:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x821a1738
	if (!cr6.getGT()) goto loc_821A1738;
loc_821A1708:
	// lwz r24,0(r10)
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r24,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r24.u32);
	// lwz r24,4(r10)
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r24,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r24.u32);
	// lwz r24,8(r10)
	r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r24,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r24.u32);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
loc_821A1738:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// bne 0x821a1684
	if (!cr0.getEQ()) goto loc_821A1684;
loc_821A1744:
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// bne 0x821a1674
	if (!cr0.getEQ()) goto loc_821A1674;
loc_821A1754:
	// mulli r11,r6,12
	r11.s64 = ctx.r6.s64 * 12;
	// add r6,r11,r3
	ctx.r6.u64 = r11.u64 + ctx.r3.u64;
	// addi r11,r3,12
	r11.s64 = ctx.r3.s64 + 12;
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// b 0x821a1800
	goto loc_821A1800;
loc_821A1768:
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// rlwimi r9,r8,25,12,13
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r8.u32, 25) & 0xC0000) | (ctx.r9.u64 & 0xFFFFFFFFFFF3FFFF);
	// rlwimi r5,r10,25,12,13
	ctx.r5.u64 = (__builtin_rotateleft32(ctx.r10.u32, 25) & 0xC0000) | (ctx.r5.u64 & 0xFFFFFFFFFFF3FFFF);
	// rlwinm r10,r9,0,7,13
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1FC0000;
	// rlwinm r9,r5,0,7,13
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1FC0000;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x821a17f8
	if (!cr6.getEQ()) goto loc_821A17F8;
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r10,r10,9
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1FF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 9;
	// srawi r9,r9,9
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1FF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 9;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bgt cr6,0x821a17f8
	if (cr6.getGT()) goto loc_821A17F8;
	// lhz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// clrlwi r5,r5,26
	ctx.r5.u64 = ctx.r5.u32 & 0x3F;
	// lbzx r5,r5,r25
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r5.u32 + r25.u32);
	// subf r10,r10,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r10.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bgt cr6,0x821a17f8
	if (cr6.getGT()) goto loc_821A17F8;
	// rlwinm r9,r8,5,29,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0x7;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a17e0
	if (!cr6.getGT()) goto loc_821A17E0;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_821A17E0:
	// rlwimi r8,r10,27,2,4
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r10.u32, 27) & 0x38000000) | (ctx.r8.u64 & 0xFFFFFFFFC7FFFFFF);
	// stw r8,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r8.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// oris r10,r10,16384
	ctx.r10.u64 = ctx.r10.u64 | 1073741824;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// b 0x821a17fc
	goto loc_821A17FC;
loc_821A17F8:
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
loc_821A17FC:
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
loc_821A1800:
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// blt cr6,0x821a1768
	if (cr6.getLT()) goto loc_821A1768;
	// b 0x821a1598
	goto loc_821A1598;
loc_821A180C:
	// addi r31,r1,96
	r31.s64 = ctx.r1.s64 + 96;
	// b 0x821a1840
	goto loc_821A1840;
loc_821A1814:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// clrlwi r11,r11,20
	r11.u64 = r11.u32 & 0xFFF;
	// mulli r11,r11,12
	r11.s64 = r11.s64 * 12;
	// add r3,r11,r22
	ctx.r3.u64 = r11.u64 + r22.u64;
	// cmplw cr6,r3,r31
	cr6.compare<uint32_t>(ctx.r3.u32, r31.u32, xer);
	// beq cr6,0x821a1838
	if (cr6.getEQ()) goto loc_821A1838;
	// li r5,12
	ctx.r5.s64 = 12;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
loc_821A1838:
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r31,r31,12
	r31.s64 = r31.s64 + 12;
loc_821A1840:
	// cmplw cr6,r30,r26
	cr6.compare<uint32_t>(r30.u32, r26.u32, xer);
	// blt cr6,0x821a1814
	if (cr6.getLT()) goto loc_821A1814;
loc_821A1848:
	// addi r1,r1,592
	ctx.r1.s64 = ctx.r1.s64 + 592;
	// b 0x823ed168
	return;
}

__attribute__((alias("__imp__sub_821A1850"))) PPC_WEAK_FUNC(sub_821A1850);
PPC_FUNC_IMPL(__imp__sub_821A1850) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// lwz r8,28(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// li r6,0
	ctx.r6.s64 = 0;
	// rlwinm r7,r10,27,27,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1F;
	// lhz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// clrlwi r10,r10,20
	ctx.r10.u64 = ctx.r10.u32 & 0xFFF;
	// li r4,1
	ctx.r4.s64 = 1;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// std r6,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r6.u64);
	// stw r6,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r6.u32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwimi r11,r4,25,3,7
	r11.u64 = (__builtin_rotateleft32(ctx.r4.u32, 25) & 0x1F000000) | (r11.u64 & 0xFFFFFFFFE0FFFFFF);
	// addi r10,r10,9
	ctx.r10.s64 = ctx.r10.s64 + 9;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r10,r3
	r31.u64 = ctx.r10.u64 + ctx.r3.u64;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// clrlwi r10,r10,6
	ctx.r10.u64 = ctx.r10.u32 & 0x3FFFFFF;
	// oris r10,r10,51200
	ctx.r10.u64 = ctx.r10.u64 | 3355443200;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
loc_821A18C8:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// clrlwi r11,r11,20
	r11.u64 = r11.u32 & 0xFFF;
	// mulli r11,r11,12
	r11.s64 = r11.s64 * 12;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// cmplw cr6,r3,r10
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r10.u32, xer);
	// beq cr6,0x821a18f0
	if (cr6.getEQ()) goto loc_821A18F0;
	// li r5,12
	ctx.r5.s64 = 12;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
loc_821A18F0:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm. r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a1904
	if (!cr0.getEQ()) goto loc_821A1904;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// b 0x821a18c8
	goto loc_821A18C8;
loc_821A1904:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A1920"))) PPC_WEAK_FUNC(sub_821A1920);
PPC_FUNC_IMPL(__imp__sub_821A1920) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// rlwinm r8,r11,27,27,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1F;
	// lhz r11,0(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// clrlwi r11,r11,20
	r11.u64 = r11.u32 & 0xFFF;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,9
	r11.s64 = r11.s64 + 9;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
loc_821A1964:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// clrlwi r11,r11,20
	r11.u64 = r11.u32 & 0xFFF;
	// mulli r11,r11,12
	r11.s64 = r11.s64 * 12;
	// add r31,r11,r29
	r31.u64 = r11.u64 + r29.u64;
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// beq cr6,0x821a1990
	if (cr6.getEQ()) goto loc_821A1990;
	// li r5,12
	ctx.r5.s64 = 12;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
loc_821A1990:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// rlwimi r10,r11,24,28,31
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 24) & 0xF) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFF0);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// beq cr6,0x821a19bc
	if (cr6.getEQ()) goto loc_821A19BC;
	// li r5,12
	ctx.r5.s64 = 12;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
loc_821A19BC:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// rlwinm. r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a19d0
	if (!cr0.getEQ()) goto loc_821A19D0;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// b 0x821a1964
	goto loc_821A1964;
loc_821A19D0:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821A19D8"))) PPC_WEAK_FUNC(sub_821A19D8);
PPC_FUNC_IMPL(__imp__sub_821A19D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed120
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// lbz r11,8(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 8);
	// clrlwi r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	// cmplwi cr6,r11,7
	cr6.compare<uint32_t>(r11.u32, 7, xer);
	// beq cr6,0x821a1b20
	if (cr6.getEQ()) goto loc_821A1B20;
	// lwz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// rlwinm. r11,r10,0,13,13
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a1b20
	if (!cr0.getEQ()) goto loc_821A1B20;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x821a1a20
	if (cr6.getEQ()) goto loc_821A1A20;
	// lwz r11,20(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// rlwinm. r11,r11,0,14,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a1b20
	if (!cr0.getEQ()) goto loc_821A1B20;
loc_821A1A20:
	// rlwinm r26,r10,27,27,31
	r26.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1F;
	// lwz r11,28(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,9
	r11.s64 = r11.s64 + 9;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r30
	r31.u64 = r11.u64 + r30.u64;
	// beq cr6,0x821a1a64
	if (cr6.getEQ()) goto loc_821A1A64;
	// lwz r11,20(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// addi r28,r7,32
	r28.s64 = ctx.r7.s64 + 32;
	// clrlwi. r24,r11,27
	r24.u64 = r11.u32 & 0x1F;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// rlwinm r11,r11,27,27,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1F;
	// beq 0x821a1a70
	if (cr0.getEQ()) goto loc_821A1A70;
	// addi r24,r24,-1
	r24.s64 = r24.s64 + -1;
	// b 0x821a1a70
	goto loc_821A1A70;
loc_821A1A64:
	// li r28,0
	r28.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// li r24,0
	r24.s64 = 0;
loc_821A1A70:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a1aec
	if (cr6.getEQ()) goto loc_821A1AEC;
	// mr r25,r11
	r25.u64 = r11.u64;
loc_821A1A7C:
	// lbz r27,3(r28)
	r27.u64 = PPC_LOAD_U8(r28.u32 + 3);
	// b 0x821a1aa8
	goto loc_821A1AA8;
loc_821A1A84:
	// lbz r11,3(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 3);
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bge cr6,0x821a1ab0
	if (!cr6.getLT()) goto loc_821A1AB0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a1850
	sub_821A1850(ctx, base);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
loc_821A1AA8:
	// cmplw cr6,r29,r26
	cr6.compare<uint32_t>(r29.u32, r26.u32, xer);
	// blt cr6,0x821a1a84
	if (cr6.getLT()) goto loc_821A1A84;
loc_821A1AB0:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// rlwinm. r11,r11,0,20,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xF00;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a1ad8
	if (cr0.getEQ()) goto loc_821A1AD8;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a1920
	sub_821A1920(ctx, base);
loc_821A1AD8:
	// addic. r25,r25,-1
	xer.ca = r25.u32 > 0;
	r25.s64 = r25.s64 + -1;
	cr0.compare<int32_t>(r25.s32, 0, xer);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// bne 0x821a1a7c
	if (!cr0.getEQ()) goto loc_821A1A7C;
loc_821A1AEC:
	// cmplw cr6,r29,r26
	cr6.compare<uint32_t>(r29.u32, r26.u32, xer);
	// bge cr6,0x821a1b14
	if (!cr6.getLT()) goto loc_821A1B14;
	// subf r29,r29,r26
	r29.s64 = r26.s64 - r29.s64;
loc_821A1AF8:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a1850
	sub_821A1850(ctx, base);
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// bne 0x821a1af8
	if (!cr0.getEQ()) goto loc_821A1AF8;
loc_821A1B14:
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// rlwimi r11,r24,20,8,11
	r11.u64 = (__builtin_rotateleft32(r24.u32, 20) & 0xF00000) | (r11.u64 & 0xFFFFFFFFFF0FFFFF);
	// stw r11,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r11.u32);
loc_821A1B20:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x823ed170
	return;
}

__attribute__((alias("__imp__sub_821A1B28"))) PPC_WEAK_FUNC(sub_821A1B28);
PPC_FUNC_IMPL(__imp__sub_821A1B28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed114
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// addi r11,r23,112
	r11.s64 = r23.s64 + 112;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// rlwinm r29,r11,3,0,28
	r29.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// mr r20,r8
	r20.u64 = ctx.r8.u64;
	// lwzx r11,r29,r31
	r11.u64 = PPC_LOAD_U32(r29.u32 + r31.u32);
	// mr r19,r9
	r19.u64 = ctx.r9.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwz r27,876(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 876);
	// rlwinm r28,r27,30,2,31
	r28.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r4,r28,5
	ctx.r4.s64 = r28.s64 + 5;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a1c80
	if (cr0.getEQ()) goto loc_821A1C80;
	// lis r11,-16384
	r11.s64 = -1073741824;
	// li r10,256
	ctx.r10.s64 = 256;
	// ori r11,r11,15104
	r11.u64 = r11.u64 | 15104;
	// lis r8,-16384
	ctx.r8.s64 = -1073741824;
	// addi r9,r28,1
	ctx.r9.s64 = r28.s64 + 1;
	// ori r8,r8,11008
	ctx.r8.u64 = ctx.r8.u64 | 11008;
	// li r7,0
	ctx.r7.s64 = 0;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// rlwimi r8,r9,16,2,15
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0x3FFF0000) | (ctx.r8.u64 & 0xFFFFFFFFC000FFFF);
	// clrlwi r11,r28,18
	r11.u64 = r28.u32 & 0x3FFF;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwzx r11,r29,r31
	r11.u64 = PPC_LOAD_U32(r29.u32 + r31.u32);
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// addi r26,r25,4
	r26.s64 = r25.s64 + 4;
	// lwz r11,872(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 872);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addis r29,r11,-16384
	r29.s64 = r11.s64 + -1073741824;
	// sync 
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// add r4,r29,r27
	ctx.r4.u64 = r29.u64 + r27.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x821a1c2c
	if (cr6.getEQ()) goto loc_821A1C2C;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821a19d8
	sub_821A19D8(ctx, base);
loc_821A1C2C:
	// cntlzw r10,r24
	ctx.r10.u64 = r24.u32 == 0 ? 32 : __builtin_clz(r24.u32);
	// lbz r11,10942(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10942);
	// addi r6,r30,12520
	ctx.r6.s64 = r30.s64 + 12520;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// rlwinm r10,r10,7,24,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x80;
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// xori r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 ^ 128;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,10942(r30)
	PPC_STORE_U8(r30.u32 + 10942, r11.u8);
	// bl 0x821a1240
	sub_821A1240(ctx, base);
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// ld r10,12520(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 12520);
	// ld r9,12528(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 12528);
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// std r10,11824(r30)
	PPC_STORE_U64(r30.u32 + 11824, ctx.r10.u64);
	// std r9,11832(r30)
	PPC_STORE_U64(r30.u32 + 11832, ctx.r9.u64);
	// stw r11,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r11.u32);
loc_821A1C80:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x823ed164
	return;
}

__attribute__((alias("__imp__sub_821A1C88"))) PPC_WEAK_FUNC(sub_821A1C88);
PPC_FUNC_IMPL(__imp__sub_821A1C88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed12c
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,20(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a1d30
	if (cr0.getEQ()) goto loc_821A1D30;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// addi r31,r11,20
	r31.s64 = r11.s64 + 20;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// add r27,r11,r31
	r27.u64 = r11.u64 + r31.u64;
	// b 0x821a1d28
	goto loc_821A1D28;
loc_821A1CBC:
	// lhz r28,2(r31)
	r28.u64 = PPC_LOAD_U16(r31.u32 + 2);
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lhz r26,0(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// cmplwi r28,0
	cr0.compare<uint32_t>(r28.u32, 0, xer);
	// beq 0x821a1d30
	if (cr0.getEQ()) goto loc_821A1D30;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r31,r11,4
	r31.s64 = r11.s64 + 4;
	// lwz r3,48(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 48);
	// lwz r11,56(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 56);
	// add r30,r10,r25
	r30.u64 = ctx.r10.u64 + r25.u64;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x821a1cf4
	if (!cr6.getGT()) goto loc_821A1CF4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821A1CF4:
	// lis r11,-16382
	r11.s64 = -1073610752;
	// clrlwi r10,r30,3
	ctx.r10.u64 = r30.u32 & 0x1FFFFFFF;
	// ori r9,r11,12032
	ctx.r9.u64 = r11.u64 | 12032;
	// rlwinm r11,r30,12,20,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 12) & 0xFFF;
	// rlwinm r8,r26,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r29)
	PPC_STORE_U32(r29.u32 + 48, ctx.r3.u32);
loc_821A1D28:
	// cmplw cr6,r31,r27
	cr6.compare<uint32_t>(r31.u32, r27.u32, xer);
	// blt cr6,0x821a1cbc
	if (cr6.getLT()) goto loc_821A1CBC;
loc_821A1D30:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed17c
	return;
}

__attribute__((alias("__imp__sub_821A1D38"))) PPC_WEAK_FUNC(sub_821A1D38);
PPC_FUNC_IMPL(__imp__sub_821A1D38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mulli r11,r7,416
	r11.s64 = ctx.r7.s64 * 416;
	// lwz r10,48(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// add r28,r11,r4
	r28.u64 = r11.u64 + ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,40(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 40);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a1da0
	if (!cr6.getEQ()) goto loc_821A1DA0;
	// ld r10,12520(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 12520);
	// addi r29,r31,12520
	r29.s64 = r31.s64 + 12520;
	// ld r11,48(r28)
	r11.u64 = PPC_LOAD_U64(r28.u32 + 48);
	// ld r9,56(r28)
	ctx.r9.u64 = PPC_LOAD_U64(r28.u32 + 56);
	// ld r8,12528(r31)
	ctx.r8.u64 = PPC_LOAD_U64(r31.u32 + 12528);
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// ld r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 32);
	// ld r6,40(r30)
	ctx.r6.u64 = PPC_LOAD_U64(r30.u32 + 40);
	// xor r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r8.u64;
	// and r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	// and r10,r9,r6
	ctx.r10.u64 = ctx.r9.u64 & ctx.r6.u64;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x821a1e5c
	if (cr6.getEQ()) goto loc_821A1E5C;
loc_821A1DA0:
	// lwz r9,64(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 64);
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x821a1dd0
	if (cr0.getEQ()) goto loc_821A1DD0;
	// lwz r10,10896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x821a1dd0
	if (!cr6.getLT()) goto loc_821A1DD0;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x821a1e70
	goto loc_821A1E70;
loc_821A1DD0:
	// addi r11,r7,112
	r11.s64 = ctx.r7.s64 + 112;
	// lwz r10,32(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	// addi r29,r31,12520
	r29.s64 = r31.s64 + 12520;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// lwzx r11,r11,r4
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r4.u32);
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// lwz r11,872(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 872);
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x821a1240
	sub_821A1240(ctx, base);
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a1e44
	if (!cr6.getEQ()) goto loc_821A1E44;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r10,r11,-27288
	ctx.r10.s64 = r11.s64 + -27288;
loc_821A1E14:
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_821A1E18:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r11,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwcx. r11,0,r8
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r9,1
	// bne 0x821a1e18
	if (!cr0.getEQ()) goto loc_821A1E18;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,2
	cr6.compare<uint32_t>(ctx.r9.u32, 2, xer);
	// blt cr6,0x821a1e14
	if (cr6.getLT()) goto loc_821A1E14;
	// stw r11,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r11.u32);
loc_821A1E44:
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// stw r11,40(r28)
	PPC_STORE_U32(r28.u32 + 40, r11.u32);
	// ld r11,0(r29)
	r11.u64 = PPC_LOAD_U64(r29.u32 + 0);
	// std r11,48(r28)
	PPC_STORE_U64(r28.u32 + 48, r11.u64);
	// ld r11,12528(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 12528);
	// std r11,56(r28)
	PPC_STORE_U64(r28.u32 + 56, r11.u64);
loc_821A1E5C:
	// ld r11,0(r29)
	r11.u64 = PPC_LOAD_U64(r29.u32 + 0);
	// li r3,1
	ctx.r3.s64 = 1;
	// ld r10,12528(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 12528);
	// std r11,11824(r31)
	PPC_STORE_U64(r31.u32 + 11824, r11.u64);
	// std r10,11832(r31)
	PPC_STORE_U64(r31.u32 + 11832, ctx.r10.u64);
loc_821A1E70:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821A1E78"))) PPC_WEAK_FUNC(sub_821A1E78);
PPC_FUNC_IMPL(__imp__sub_821A1E78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed100
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,-1
	r11.s64 = -1;
	// mr r20,r4
	r20.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// rlwinm r10,r20,0,14,14
	ctx.r10.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x20000;
	// mr r14,r11
	r14.u64 = r11.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// mr r15,r11
	r15.u64 = r11.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// mr r19,r11
	r19.u64 = r11.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821a1ebc
	if (cr6.getEQ()) goto loc_821A1EBC;
	// bl 0x821a07a8
	sub_821A07A8(ctx, base);
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
loc_821A1EBC:
	// lwz r31,12688(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 12688);
	// lwz r21,11812(r30)
	r21.u64 = PPC_LOAD_U32(r30.u32 + 11812);
	// lwz r29,12684(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 12684);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x821a2680
	if (cr0.getEQ()) goto loc_821A2680;
	// lwz r11,896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 896);
	// li r24,0
	r24.s64 = 0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// li r23,5
	r23.s64 = 5;
	// addi r25,r11,872
	r25.s64 = r11.s64 + 872;
	// lis r11,-16383
	r11.s64 = -1073676288;
	// ori r17,r11,9984
	r17.u64 = r11.u64 | 9984;
	// bne cr6,0x821a1f94
	if (!cr6.getEQ()) goto loc_821A1F94;
	// lwz r11,872(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 872);
	// li r28,0
	r28.s64 = 0;
	// li r18,0
	r18.s64 = 0;
	// li r16,0
	r16.s64 = 0;
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a1f1c
	if (cr0.getEQ()) goto loc_821A1F1C;
	// lwz r11,904(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 904);
	// li r24,1
	r24.s64 = 1;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// addi r25,r11,872
	r25.s64 = r11.s64 + 872;
loc_821A1F1C:
	// lwz r11,10580(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 10580);
	// clrlwi r10,r11,29
	ctx.r10.u64 = r11.u32 & 0x7;
	// cmplwi cr6,r10,5
	cr6.compare<uint32_t>(ctx.r10.u32, 5, xer);
	// beq cr6,0x821a1f60
	if (cr6.getEQ()) goto loc_821A1F60;
	// lbz r10,10943(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 10943);
	// rlwimi r11,r23,0,29,31
	r11.u64 = (__builtin_rotateleft32(r23.u32, 0) & 0x7) | (r11.u64 & 0xFFFFFFFFFFFFFFF8);
	// oris r20,r20,8
	r20.u64 = r20.u64 | 524288;
	// ori r20,r20,8
	r20.u64 = r20.u64 | 8;
	// stw r11,10580(r30)
	PPC_STORE_U32(r30.u32 + 10580, r11.u32);
	// rlwinm. r10,r10,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a1f60
	if (cr0.getEQ()) goto loc_821A1F60;
	// lwz r11,10372(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 10372);
	// li r12,1
	r12.s64 = 1;
	// rlwinm r11,r11,0,16,11
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFF0FFFF;
	// rldicr r12,r12,56,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 56) & 0xFFFFFFFFFFFFFFFF;
	// or r20,r20,r12
	r20.u64 = r20.u64 | r12.u64;
	// stw r11,10372(r30)
	PPC_STORE_U32(r30.u32 + 10372, r11.u32);
loc_821A1F60:
	// lwz r11,20(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 20);
	// clrlwi r11,r11,27
	r11.u64 = r11.u32 & 0x1F;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r27,r11,1
	r27.u64 = r11.u64 ^ 1;
loc_821A1F74:
	// lwz r11,13488(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 13488);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a20e8
	if (!cr6.getEQ()) goto loc_821A20E8;
	// lwz r11,872(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 872);
	// rlwinm. r11,r11,0,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a20e8
	if (cr0.getEQ()) goto loc_821A20E8;
	// li r3,245
	ctx.r3.s64 = 245;
	// bl 0x8240f9ec
	__imp__KeBugCheck(ctx, base);
loc_821A1F94:
	// lwz r10,64(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 64);
	// rlwinm r8,r20,0,11,11
	ctx.r8.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x100000;
	// lis r9,4096
	ctx.r9.s64 = 268435456;
	// lbz r11,10942(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10942);
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// cmpldi cr6,r8,0
	cr6.compare<uint64_t>(ctx.r8.u64, 0, xer);
	// addi r28,r10,40
	r28.s64 = ctx.r10.s64 + 40;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r18,8(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r16,12(r28)
	r16.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// rlwinm r8,r18,0,1,3
	ctx.r8.u64 = __builtin_rotateleft64(r18.u32 | (r18.u64 << 32), 0) & 0x70000000;
	// subfc r9,r8,r9
	xer.ca = ctx.r9.u32 >= ctx.r8.u32;
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subfe r9,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + xer.ca < xer.ca);
	ctx.r9.u64 = ~ctx.r9.u64 + ctx.r9.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r9,r9,3,28,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0x8;
	// or r11,r9,r11
	r11.u64 = ctx.r9.u64 | r11.u64;
	// rlwimi r10,r11,0,28,28
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0x8) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFF7);
	// stb r10,10942(r30)
	PPC_STORE_U8(r30.u32 + 10942, ctx.r10.u8);
	// beq cr6,0x821a20ac
	if (cr6.getEQ()) goto loc_821A20AC;
	// addi r4,r29,40
	ctx.r4.s64 = r29.s64 + 40;
	// lwz r5,24(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a1c88
	sub_821A1C88(ctx, base);
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r10,56(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a2008
	if (!cr6.getGT()) goto loc_821A2008;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A2008:
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// ori r20,r20,16384
	r20.u64 = r20.u64 | 16384;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,64(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 64);
	// lwz r9,24(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// lwz r10,40(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r10,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// rlwinm r9,r10,0,3,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1FFFFFFE;
	// addi r10,r8,512
	ctx.r10.s64 = ctx.r8.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// ori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 | 1;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,64(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 64);
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + r29.u64;
	// lwz r10,44(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,10580(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 10580);
	// stw r11,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r11.u32);
	// lwz r11,24(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 24);
	// stw r11,10536(r30)
	PPC_STORE_U32(r30.u32 + 10536, r11.u32);
	// clrlwi r11,r10,29
	r11.u64 = ctx.r10.u32 & 0x7;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// beq cr6,0x821a20ac
	if (cr6.getEQ()) goto loc_821A20AC;
	// lbz r11,10943(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10943);
	// li r9,1
	ctx.r9.s64 = 1;
	// oris r20,r20,8
	r20.u64 = r20.u64 | 524288;
	// rlwimi r10,r9,2,29,31
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 2) & 0x7) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFF8);
	// ori r20,r20,8
	r20.u64 = r20.u64 | 8;
	// stw r10,10580(r30)
	PPC_STORE_U32(r30.u32 + 10580, ctx.r10.u32);
	// rlwinm. r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a20ac
	if (cr0.getEQ()) goto loc_821A20AC;
	// lwz r11,12432(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12432);
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,56,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 56) & 0xFFFFFFFFFFFFFFFF;
	// or r20,r20,r12
	r20.u64 = r20.u64 | r12.u64;
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,10372(r30)
	PPC_STORE_U32(r30.u32 + 10372, r11.u32);
loc_821A20AC:
	// lwz r11,20(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 20);
	// lwz r10,20(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 20);
	// lwz r9,13488(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 13488);
	// xor r11,r11,r10
	r11.u64 = r11.u64 ^ ctx.r10.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// clrlwi r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r27,r11,1
	r27.u64 = r11.u64 ^ 1;
	// bne cr6,0x821a1f74
	if (!cr6.getEQ()) goto loc_821A1F74;
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// rlwinm. r11,r11,0,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a1f74
	if (cr0.getEQ()) goto loc_821A1F74;
	// li r3,245
	ctx.r3.s64 = 245;
	// bl 0x8240f9ec
	__imp__KeBugCheck(ctx, base);
loc_821A20E8:
	// lbz r11,10940(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10940);
	// lwz r10,8(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 8);
	// lwz r26,10580(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 10580);
	// lwz r22,12(r25)
	r22.u64 = PPC_LOAD_U32(r25.u32 + 12);
	// mr r29,r26
	r29.u64 = r26.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// rlwinm. r11,r11,0,25,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a2180
	if (cr0.getEQ()) goto loc_821A2180;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x821a2130
	if (cr6.getEQ()) goto loc_821A2130;
	// lwz r11,10556(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 10556);
	// rlwinm. r11,r11,0,28,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a2134
	if (!cr0.getEQ()) goto loc_821A2134;
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// rlwinm. r10,r11,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a2134
	if (!cr0.getEQ()) goto loc_821A2134;
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a2134
	if (!cr0.getEQ()) goto loc_821A2134;
loc_821A2130:
	// rlwimi r29,r23,0,29,31
	r29.u64 = (__builtin_rotateleft32(r23.u32, 0) & 0x7) | (r29.u64 & 0xFFFFFFFFFFFFFFF8);
loc_821A2134:
	// lwz r11,12692(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12692);
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bne cr6,0x821a214c
	if (!cr6.getEQ()) goto loc_821A214C;
	// rlwinm r11,r20,0,28,28
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x8;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x821a2180
	if (cr6.getEQ()) goto loc_821A2180;
loc_821A214C:
	// lbz r11,10943(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10943);
	// ori r20,r20,8
	r20.u64 = r20.u64 | 8;
	// mr r14,r29
	r14.u64 = r29.u64;
	// stw r29,12692(r30)
	PPC_STORE_U32(r30.u32 + 12692, r29.u32);
	// rlwinm. r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a2180
	if (cr0.getEQ()) goto loc_821A2180;
	// clrlwi r10,r29,29
	ctx.r10.u64 = r29.u32 & 0x7;
	// lwz r11,10372(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 10372);
	// cmplwi cr6,r10,5
	cr6.compare<uint32_t>(ctx.r10.u32, 5, xer);
	// bne cr6,0x821a2178
	if (!cr6.getEQ()) goto loc_821A2178;
	// rlwinm r11,r11,0,16,11
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFF0FFFF;
loc_821A2178:
	// mr r23,r11
	r23.u64 = r11.u64;
	// b 0x821a2184
	goto loc_821A2184;
loc_821A2180:
	// lwz r23,84(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_821A2184:
	// lbz r11,10942(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10942);
	// rlwinm. r10,r11,0,0,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a229c
	if (!cr0.getEQ()) goto loc_821A229C;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bne cr6,0x821a229c
	if (!cr6.getEQ()) goto loc_821A229C;
	// rlwinm r11,r20,0,12,12
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x80000;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x821a22e4
	if (cr6.getEQ()) goto loc_821A22E4;
	// addi r27,r31,872
	r27.s64 = r31.s64 + 872;
	// lwz r5,32(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x821a1c88
	sub_821A1C88(ctx, base);
	// lbz r11,10942(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10942);
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// andi. r11,r11,191
	r11.u64 = r11.u64 & 191;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stb r11,10942(r30)
	PPC_STORE_U8(r30.u32 + 10942, r11.u8);
	// beq cr6,0x821a227c
	if (cr6.getEQ()) goto loc_821A227C;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm. r10,r10,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a227c
	if (!cr0.getEQ()) goto loc_821A227C;
	// lbz r11,10940(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10940);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rlwinm. r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a21f0
	if (cr0.getEQ()) goto loc_821A21F0;
	// li r4,0
	ctx.r4.s64 = 0;
	// b 0x821a22c8
	goto loc_821A22C8;
loc_821A21F0:
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x821a1d38
	sub_821A1D38(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a2210
	if (cr0.getEQ()) goto loc_821A2210;
	// mr r19,r24
	r19.u64 = r24.u64;
	// b 0x821a2234
	goto loc_821A2234;
loc_821A2210:
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a1b28
	sub_821A1B28(ctx, base);
loc_821A2234:
	// xor r11,r29,r26
	r11.u64 = r29.u64 ^ r26.u64;
	// clrlwi. r11,r11,29
	r11.u64 = r11.u32 & 0x7;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a22e4
	if (cr0.getEQ()) goto loc_821A22E4;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a22e4
	if (cr0.getEQ()) goto loc_821A22E4;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a1d38
	sub_821A1D38(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a22e4
	if (cr0.getEQ()) goto loc_821A22E4;
	// lbz r11,10942(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10942);
loc_821A226C:
	// ori r11,r11,64
	r11.u64 = r11.u64 | 64;
	// li r15,1
	r15.s64 = 1;
	// stb r11,10942(r30)
	PPC_STORE_U8(r30.u32 + 10942, r11.u8);
	// b 0x821a22e4
	goto loc_821A22E4;
loc_821A227C:
	// xor r10,r29,r26
	ctx.r10.u64 = r29.u64 ^ r26.u64;
	// mr r19,r24
	r19.u64 = r24.u64;
	// clrlwi. r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a22e4
	if (cr0.getEQ()) goto loc_821A22E4;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm. r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a22e4
	if (cr0.getEQ()) goto loc_821A22E4;
	// b 0x821a226c
	goto loc_821A226C;
loc_821A229C:
	// rlwinm r10,r20,0,11,12
	ctx.r10.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x180000;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821a22e4
	if (cr6.getEQ()) goto loc_821A22E4;
	// andi. r11,r11,191
	r11.u64 = r11.u64 & 191;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r4,r31,872
	ctx.r4.s64 = r31.s64 + 872;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stb r11,10942(r30)
	PPC_STORE_U8(r30.u32 + 10942, r11.u8);
	// lwz r5,32(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// bl 0x821a1c88
	sub_821A1C88(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_821A22C8:
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x821a1b28
	sub_821A1B28(ctx, base);
loc_821A22E4:
	// rlwinm r11,r20,0,11,12
	r11.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x180000;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x821a2334
	if (cr6.getEQ()) goto loc_821A2334;
	// lbz r11,10942(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10942);
	// or r9,r22,r16
	ctx.r9.u64 = r22.u64 | r16.u64;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// oris r20,r20,1
	r20.u64 = r20.u64 | 65536;
	// or r10,r10,r18
	ctx.r10.u64 = ctx.r10.u64 | r18.u64;
	// ori r20,r20,32768
	r20.u64 = r20.u64 | 32768;
	// stw r9,10532(r30)
	PPC_STORE_U32(r30.u32 + 10532, ctx.r9.u32);
	// stw r10,10528(r30)
	PPC_STORE_U32(r30.u32 + 10528, ctx.r10.u32);
	// rlwinm. r11,r11,0,25,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a2334
	if (cr0.getEQ()) goto loc_821A2334;
	// lwz r11,904(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 904);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwz r10,880(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 880);
	// lwz r11,884(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 884);
	// or r29,r10,r18
	r29.u64 = ctx.r10.u64 | r18.u64;
	// or r28,r11,r16
	r28.u64 = r11.u64 | r16.u64;
	// b 0x821a233c
	goto loc_821A233C;
loc_821A2334:
	// lwz r28,88(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r29,92(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_821A233C:
	// lbz r11,10940(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 10940);
	// rlwinm. r11,r11,0,25,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a25fc
	if (cr0.getEQ()) goto loc_821A25FC;
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r10,56(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a2364
	if (!cr6.getGT()) goto loc_821A2364;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A2364:
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// lis r9,5461
	ctx.r9.s64 = 357892096;
	// ori r5,r10,24576
	ctx.r5.u64 = ctx.r10.u64 | 24576;
	// lis r8,-16384
	ctx.r8.s64 = -1073741824;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// ori r9,r9,21845
	ctx.r9.u64 = ctx.r9.u64 | 21845;
	// ori r4,r8,24832
	ctx.r4.u64 = ctx.r8.u64 | 24832;
	// rlwinm r7,r20,0,28,28
	ctx.r7.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x8;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpldi cr6,r7,0
	cr6.compare<uint64_t>(ctx.r7.u64, 0, xer);
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// beq cr6,0x821a23bc
	if (cr6.getEQ()) goto loc_821A23BC;
	// li r10,8712
	ctx.r10.s64 = 8712;
	// li r12,-9
	r12.s64 = -9;
	// and r20,r20,r12
	r20.u64 = r20.u64 & r12.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,10580(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 10580);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
loc_821A23BC:
	// lis r10,-16383
	ctx.r10.s64 = -1073676288;
	// cmpwi cr6,r14,-1
	cr6.compare<int32_t>(r14.s32, -1, xer);
	// ori r10,r10,11521
	ctx.r10.u64 = ctx.r10.u64 | 11521;
	// beq cr6,0x821a23e4
	if (cr6.getEQ()) goto loc_821A23E4;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// lis r8,4
	ctx.r8.s64 = 262144;
	// ori r8,r8,520
	ctx.r8.u64 = ctx.r8.u64 | 520;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r14,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r14.u32);
	r11.u32 = ea;
loc_821A23E4:
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,56,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 56) & 0xFFFFFFFFFFFFFFFF;
	// and r9,r20,r12
	ctx.r9.u64 = r20.u64 & r12.u64;
	// cmpldi cr6,r9,0
	cr6.compare<uint64_t>(ctx.r9.u64, 0, xer);
	// beq cr6,0x821a2414
	if (cr6.getEQ()) goto loc_821A2414;
	// li r9,8193
	ctx.r9.s64 = 8193;
	// li r12,-2
	r12.s64 = -2;
	// rldicr r12,r12,56,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 56) & 0xFFFFFFFFFFFFFFFF;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// and r20,r20,r12
	r20.u64 = r20.u64 & r12.u64;
	// lwz r9,10372(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 10372);
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
loc_821A2414:
	// cmpwi cr6,r23,-1
	cr6.compare<int32_t>(r23.s32, -1, xer);
	// beq cr6,0x821a2430
	if (cr6.getEQ()) goto loc_821A2430;
	// lis r9,4
	ctx.r9.s64 = 262144;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// ori r9,r9,1
	ctx.r9.u64 = ctx.r9.u64 | 1;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r23,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r23.u32);
	r11.u32 = ea;
loc_821A2430:
	// rlwinm r10,r20,0,15,15
	ctx.r10.u64 = __builtin_rotateleft64(r20.u32 | (r20.u64 << 32), 0) & 0x10000;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821a2464
	if (cr6.getEQ()) goto loc_821A2464;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// lis r12,-2
	r12.s64 = -131072;
	// ori r10,r10,8576
	ctx.r10.u64 = ctx.r10.u64 | 8576;
	// ori r12,r12,32767
	r12.u64 = r12.u64 | 32767;
	// and r20,r20,r12
	r20.u64 = r20.u64 & r12.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,10528(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 10528);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,10532(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 10532);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
loc_821A2464:
	// cmpwi cr6,r29,-1
	cr6.compare<int32_t>(r29.s32, -1, xer);
	// beq cr6,0x821a248c
	if (cr6.getEQ()) goto loc_821A248C;
	// lis r10,-16382
	ctx.r10.s64 = -1073610752;
	// lis r9,4
	ctx.r9.s64 = 262144;
	// ori r10,r10,11521
	ctx.r10.u64 = ctx.r10.u64 | 11521;
	// ori r9,r9,384
	ctx.r9.u64 = ctx.r9.u64 | 384;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r29,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r29.u32);
	r11.u32 = ea;
	// stwu r28,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r28.u32);
	r11.u32 = ea;
loc_821A248C:
	// cmpwi cr6,r15,-1
	cr6.compare<int32_t>(r15.s32, -1, xer);
	// beq cr6,0x821a257c
	if (cr6.getEQ()) goto loc_821A257C;
	// lis r10,-16383
	ctx.r10.s64 = -1073676288;
	// addi r9,r15,112
	ctx.r9.s64 = r15.s64 + 112;
	// ori r10,r10,9985
	ctx.r10.u64 = ctx.r10.u64 | 9985;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// mulli r9,r15,416
	ctx.r9.s64 = r15.s64 * 416;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// add r3,r9,r31
	ctx.r3.u64 = ctx.r9.u64 + r31.u64;
	// lwzx r9,r8,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// cmpwi cr6,r19,-1
	cr6.compare<int32_t>(r19.s32, -1, xer);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// lwz r9,872(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 872);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// rlwinm r6,r9,12,20,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// clrlwi r7,r9,3
	ctx.r7.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r9,r6,512
	ctx.r9.s64 = ctx.r6.s64 + 512;
	// rlwinm r9,r9,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1000;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// lwzx r9,r8,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + r31.u64;
	// lwz r9,876(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 876);
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// lwz r9,10908(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 10908);
	// stw r9,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r9.u32);
	// beq cr6,0x821a25e0
	if (cr6.getEQ()) goto loc_821A25E0;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// lis r9,10922
	ctx.r9.s64 = 715784192;
	// addi r7,r19,112
	ctx.r7.s64 = r19.s64 + 112;
	// ori r6,r9,43690
	ctx.r6.u64 = ctx.r9.u64 | 43690;
	// rlwinm r9,r7,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// mulli r10,r19,416
	ctx.r10.s64 = r19.s64 * 416;
	// stwu r6,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	r11.u32 = ea;
	// add r3,r10,r31
	ctx.r3.u64 = ctx.r10.u64 + r31.u64;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// lwzx r10,r9,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,872(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 872);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r7,r10,12,20,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r8,r10,3
	ctx.r8.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r7,512
	ctx.r10.s64 = ctx.r7.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwzx r10,r9,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,876(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 876);
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,10908(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 10908);
	// stw r10,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r10.u32);
	// b 0x821a25e0
	goto loc_821A25E0;
loc_821A257C:
	// cmpwi cr6,r19,-1
	cr6.compare<int32_t>(r19.s32, -1, xer);
	// beq cr6,0x821a25e0
	if (cr6.getEQ()) goto loc_821A25E0;
	// addi r10,r19,112
	ctx.r10.s64 = r19.s64 + 112;
	// stwu r17,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r17.u32);
	r11.u32 = ea;
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// mulli r10,r19,416
	ctx.r10.s64 = r19.s64 * 416;
	// add r6,r10,r31
	ctx.r6.u64 = ctx.r10.u64 + r31.u64;
	// lwzx r10,r9,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,872(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 872);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r7,r10,12,20,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r8,r10,3
	ctx.r8.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r7,512
	ctx.r10.s64 = ctx.r7.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwzx r10,r9,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,876(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 876);
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,10908(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 10908);
	// stw r10,64(r6)
	PPC_STORE_U32(ctx.r6.u32 + 64, ctx.r10.u32);
loc_821A25E0:
	// stwu r5,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	r11.u32 = ea;
	// lwz r10,12700(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 12700);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r4,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	r11.u32 = ea;
	// lwz r10,12704(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 12704);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// b 0x821a267c
	goto loc_821A267C;
loc_821A25FC:
	// cmpwi cr6,r19,-1
	cr6.compare<int32_t>(r19.s32, -1, xer);
	// beq cr6,0x821a2680
	if (cr6.getEQ()) goto loc_821A2680;
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 48);
	// lwz r10,56(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a2620
	if (!cr6.getGT()) goto loc_821A2620;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A2620:
	// addi r10,r19,112
	ctx.r10.s64 = r19.s64 + 112;
	// stwu r17,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r17.u32);
	r11.u32 = ea;
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// mulli r10,r19,416
	ctx.r10.s64 = r19.s64 * 416;
	// add r6,r10,r31
	ctx.r6.u64 = ctx.r10.u64 + r31.u64;
	// lwzx r10,r9,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,872(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 872);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r7,r10,12,20,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r8,r10,3
	ctx.r8.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r7,512
	ctx.r10.s64 = ctx.r7.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwzx r10,r9,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// lwz r10,876(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 876);
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,10908(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 10908);
	// stw r10,64(r6)
	PPC_STORE_U32(ctx.r6.u32 + 64, ctx.r10.u32);
loc_821A267C:
	// stw r11,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r11.u32);
loc_821A2680:
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x823ed150
	return;
}

__attribute__((alias("__imp__sub_821A2690"))) PPC_WEAK_FUNC(sub_821A2690);
PPC_FUNC_IMPL(__imp__sub_821A2690) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r4,24(r1)
	PPC_STORE_U64(ctx.r1.u32 + 24, ctx.r4.u64);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,392
	ctx.r10.s64 = ctx.r1.s64 + 392;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// li r4,260
	ctx.r4.s64 = 260;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8240fa0c
	__imp___vsnprintf(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8240f9fc
	__imp__DbgPrint(ctx, base);
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A26F0"))) PPC_WEAK_FUNC(sub_821A26F0);
PPC_FUNC_IMPL(__imp__sub_821A26F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// addis r11,r3,8178
	r11.s64 = ctx.r3.s64 + 535953408;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A2700"))) PPC_WEAK_FUNC(sub_821A2700);
PPC_FUNC_IMPL(__imp__sub_821A2700) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// addis r11,r3,8178
	r11.s64 = ctx.r3.s64 + 535953408;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r4,0(r11)
	PPC_MM_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// eieio 
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A2718"))) PPC_WEAK_FUNC(sub_821A2718);
PPC_FUNC_IMPL(__imp__sub_821A2718) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// addi r10,r1,432
	ctx.r10.s64 = ctx.r1.s64 + 432;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// li r4,260
	ctx.r4.s64 = 260;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// bl 0x8240fa0c
	__imp___vsnprintf(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a2780
	if (cr0.getEQ()) goto loc_821A2780;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x821a2784
	goto loc_821A2784;
loc_821A2780:
	// bl 0x821a2690
	sub_821A2690(ctx, base);
loc_821A2784:
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A2798"))) PPC_WEAK_FUNC(sub_821A2798);
PPC_FUNC_IMPL(__imp__sub_821A2798) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed124
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r3,1488
	ctx.r3.s64 = 1488;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r5,r11,-16536
	ctx.r5.s64 = r11.s64 + -16536;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,-16556
	ctx.r4.s64 = r11.s64 + -16556;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r28,0
	r28.s64 = 0;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// lis r11,-32019
	r11.s64 = -2098397184;
	// li r26,11
	r26.s64 = 11;
	// addi r27,r11,28928
	r27.s64 = r11.s64 + 28928;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r30,r27,4
	r30.s64 = r27.s64 + 4;
	// addi r25,r11,-16560
	r25.s64 = r11.s64 + -16560;
loc_821A27F4:
	// lwz r11,-4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + -4);
	// and. r11,r11,r29
	r11.u64 = r11.u64 & r29.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a2824
	if (cr0.getEQ()) goto loc_821A2824;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x821a2814
	if (cr6.getEQ()) goto loc_821A2814;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
loc_821A2814:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// li r28,1
	r28.s64 = 1;
loc_821A2824:
	// addic. r26,r26,-1
	xer.ca = r26.u32 > 0;
	r26.s64 = r26.s64 + -1;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// bne 0x821a27f4
	if (!cr0.getEQ()) goto loc_821A27F4;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x821a2848
	if (!cr6.getEQ()) goto loc_821A2848;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,-16580
	ctx.r4.s64 = r11.s64 + -16580;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
loc_821A2848:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,-16584
	ctx.r4.s64 = r11.s64 + -16584;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,3857
	ctx.r3.s64 = 3857;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x8235eb58
	sub_8235EB58(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r3,3858
	ctx.r3.s64 = 3858;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// addi r5,r11,-16604
	ctx.r5.s64 = r11.s64 + -16604;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r29,r11,-16624
	r29.s64 = r11.s64 + -16624;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// addi r30,r27,88
	r30.s64 = r27.s64 + 88;
	// li r28,30
	r28.s64 = 30;
loc_821A28AC:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// bne 0x821a28ac
	if (!cr0.getEQ()) goto loc_821A28AC;
	// addi r11,r27,328
	r11.s64 = r27.s64 + 328;
	// li r23,5
	r23.s64 = 5;
	// addi r29,r11,16
	r29.s64 = r11.s64 + 16;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r26,r11,-16628
	r26.s64 = r11.s64 + -16628;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r27,r11,-16636
	r27.s64 = r11.s64 + -16636;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r25,r11,-16648
	r25.s64 = r11.s64 + -16648;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r24,r11,23060
	r24.s64 = r11.s64 + 23060;
loc_821A2908:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x821a29ec
	if (!cr6.getGT()) goto loc_821A29EC;
loc_821A2924:
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r5,4(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8240f93c
	__imp__sprintf(ctx, base);
	// mr r28,r30
	r28.u64 = r30.u64;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
loc_821A2940:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bge cr6,0x821a2950
	if (!cr6.getLT()) goto loc_821A2950;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_821A2950:
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bge cr6,0x821a29d0
	if (!cr6.getLT()) goto loc_821A29D0;
	// lwz r11,-4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + -4);
	// lwz r10,-8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + -8);
	// lwz r3,-16(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + -16);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// slw r11,r28,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r28.u32 << (r11.u8 & 0x3F));
	// or r4,r11,r10
	ctx.r4.u64 = r11.u64 | ctx.r10.u64;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x8235eb58
	sub_8235EB58(ctx, base);
	// lwz r3,-12(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + -12);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821A299C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821a299c
	if (!cr6.getEQ()) goto loc_821A299C;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x8240f93c
	__imp__sprintf(ctx, base);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// b 0x821a2940
	goto loc_821A2940;
loc_821A29D0:
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x821a2924
	if (cr6.getLT()) goto loc_821A2924;
loc_821A29EC:
	// addic. r23,r23,-1
	xer.ca = r23.u32 > 0;
	r23.s64 = r23.s64 + -1;
	cr0.compare<int32_t>(r23.s32, 0, xer);
	// addi r29,r29,24
	r29.s64 = r29.s64 + 24;
	// bne 0x821a2908
	if (!cr0.getEQ()) goto loc_821A2908;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x823ed174
	return;
}

__attribute__((alias("__imp__sub_821A2A00"))) PPC_WEAK_FUNC(sub_821A2A00);
PPC_FUNC_IMPL(__imp__sub_821A2A00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed134
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// addi r4,r11,-16224
	ctx.r4.s64 = r11.s64 + -16224;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x821a2ac4
	if (!cr6.getEQ()) goto loc_821A2AC4;
	// lwz r11,152(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 152);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a2a54
	if (!cr6.getEQ()) goto loc_821A2A54;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lwz r6,164(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,172(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// addi r4,r11,-16352
	ctx.r4.s64 = r11.s64 + -16352;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
loc_821A2A54:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r4,r11,-16488
	ctx.r4.s64 = r11.s64 + -16488;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// lwz r30,116(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x821a2ac4
	if (cr0.getEQ()) goto loc_821A2AC4;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r27,r11,-16508
	r27.s64 = r11.s64 + -16508;
loc_821A2A78:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x821a2ab8
	if (!cr6.getGT()) goto loc_821A2AB8;
	// addi r31,r30,12
	r31.s64 = r30.s64 + 12;
loc_821A2A8C:
	// lwz r11,-4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + -4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// clrlwi r6,r11,8
	ctx.r6.u64 = r11.u32 & 0xFFFFFF;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// blt cr6,0x821a2a8c
	if (cr6.getLT()) goto loc_821A2A8C;
loc_821A2AB8:
	// lwz r30,0(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// bne 0x821a2a78
	if (!cr0.getEQ()) goto loc_821A2A78;
loc_821A2AC4:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed184
	return;
}

__attribute__((alias("__imp__sub_821A2AD0"))) PPC_WEAK_FUNC(sub_821A2AD0);
PPC_FUNC_IMPL(__imp__sub_821A2AD0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed118
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r21,r6
	r21.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// li r20,1
	r20.s64 = 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821a2b04
	if (cr6.getEQ()) goto loc_821A2B04;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x821a2b18
	if (!cr6.getEQ()) goto loc_821A2B18;
loc_821A2B04:
	// lis r10,-32230
	ctx.r10.s64 = -2112225280;
	// lis r11,-32230
	r11.s64 = -2112225280;
	// addi r31,r10,9968
	r31.s64 = ctx.r10.s64 + 9968;
	// addi r30,r11,9984
	r30.s64 = r11.s64 + 9984;
	// li r20,0
	r20.s64 = 0;
loc_821A2B18:
	// li r3,1488
	ctx.r3.s64 = 1488;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r3,3878
	ctx.r3.s64 = 3878;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,448
	ctx.r3.s64 = 448;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// li r3,1403
	ctx.r3.s64 = 1403;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// li r3,1404
	ctx.r3.s64 = 1404;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r3,1405
	ctx.r3.s64 = 1405;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// li r4,12
	ctx.r4.s64 = 12;
	// li r3,3200
	ctx.r3.s64 = 3200;
	// mtctr r30
	ctr.u64 = r30.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,3201
	ctx.r3.s64 = 3201;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// li r3,3201
	ctx.r3.s64 = 3201;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r3.u32);
	// li r3,3201
	ctx.r3.s64 = 3201;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r3.u32);
	// li r3,3201
	ctx.r3.s64 = 3201;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r3.u32);
	// li r3,3201
	ctx.r3.s64 = 3201;
	// mtctr r31
	ctr.u64 = r31.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r3,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r3.u32);
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// addi r7,r11,-14928
	ctx.r7.s64 = r11.s64 + -14928;
	// stw r25,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r25.u32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r31,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r31.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r30,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r30.u32);
	// addi r4,r11,-15000
	ctx.r4.s64 = r11.s64 + -15000;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r5,6274
	ctx.r5.s64 = 6274;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// li r27,0
	r27.s64 = 0;
	// lwz r11,1668(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1668);
	// lhz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// lis r11,-32763
	r11.s64 = -2147155968;
	// ori r11,r11,272
	r11.u64 = r11.u64 | 272;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bne cr6,0x821a2c74
	if (!cr6.getEQ()) goto loc_821A2C74;
	// lis r11,2989
	r11.s64 = 195887104;
	// ori r11,r11,61453
	r11.u64 = r11.u64 | 61453;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x821a2c74
	if (cr6.getEQ()) goto loc_821A2C74;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x821a2c74
	if (cr6.getEQ()) goto loc_821A2C74;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// addi r4,r11,-15160
	ctx.r4.s64 = r11.s64 + -15160;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x821a2d44
	if (cr6.getEQ()) goto loc_821A2D44;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lwz r5,0(r21)
	ctx.r5.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// addi r4,r11,-15204
	ctx.r4.s64 = r11.s64 + -15204;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// b 0x821a2d44
	goto loc_821A2D44;
loc_821A2C74:
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x821a2798
	sub_821A2798(ctx, base);
	// lis r12,32763
	r12.s64 = 2147155968;
	// lis r10,1025
	ctx.r10.s64 = 67174400;
	// ori r12,r12,61664
	r12.u64 = r12.u64 | 61664;
	// and r11,r29,r12
	r11.u64 = r29.u64 & r12.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a2cb8
	if (!cr6.getEQ()) goto loc_821A2CB8;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// addi r4,r11,-15336
	ctx.r4.s64 = r11.s64 + -15336;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// addi r4,r11,-15488
	ctx.r4.s64 = r11.s64 + -15488;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// li r27,1
	r27.s64 = 1;
loc_821A2CB8:
	// rlwinm. r11,r23,0,26,26
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a2d08
	if (cr0.getEQ()) goto loc_821A2D08;
	// rlwinm. r11,r23,0,21,25
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x7C0;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a2d08
	if (!cr0.getEQ()) goto loc_821A2D08;
	// rlwinm r11,r23,21,28,29
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 21) & 0xC;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// lis r9,32
	ctx.r9.s64 = 2097152;
	// lwzx r11,r11,r10
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// rlwinm r11,r11,0,5,11
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7F00000;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x821a2d08
	if (!cr6.getEQ()) goto loc_821A2D08;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// addi r4,r11,-15624
	ctx.r4.s64 = r11.s64 + -15624;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// addi r4,r11,-15800
	ctx.r4.s64 = r11.s64 + -15800;
	// bl 0x821a2718
	sub_821A2718(ctx, base);
	// li r27,1
	r27.s64 = 1;
loc_821A2D08:
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x821a2d20
	if (cr6.getEQ()) goto loc_821A2D20;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x821a2a00
	sub_821A2A00(ctx, base);
loc_821A2D20:
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// beq cr6,0x821a2d38
	if (cr6.getEQ()) goto loc_821A2D38;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r4,r11,-15936
	ctx.r4.s64 = r11.s64 + -15936;
	// b 0x821a2d40
	goto loc_821A2D40;
loc_821A2D38:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r4,r11,-16080
	ctx.r4.s64 = r11.s64 + -16080;
loc_821A2D40:
	// bl 0x821a2718
	sub_821A2718(ctx, base);
loc_821A2D44:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x823ed168
	return;
}

__attribute__((alias("__imp__sub_821A2D50"))) PPC_WEAK_FUNC(sub_821A2D50);
PPC_FUNC_IMPL(__imp__sub_821A2D50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r11,-14600
	ctx.r3.s64 = r11.s64 + -14600;
	// bl 0x821a2690
	sub_821A2690(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r7,16544(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 16544);
	// lwz r6,10900(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 10900);
	// lwz r3,13416(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 13416);
	// bl 0x821a2ad0
	sub_821A2AD0(ctx, base);
	// lwz r11,13416(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13416);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a2dc8
	if (cr6.getEQ()) goto loc_821A2DC8;
	// lbz r10,10941(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10908);
	// ori r10,r10,3
	ctx.r10.u64 = ctx.r10.u64 | 3;
	// lwz r8,10896(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// addi r7,r11,-2
	ctx.r7.s64 = r11.s64 + -2;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,-16524
	ctx.r3.s64 = r11.s64 + -16524;
	// stb r10,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, ctx.r10.u8);
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// stw r9,11000(r31)
	PPC_STORE_U32(r31.u32 + 11000, ctx.r9.u32);
	// bl 0x821a7258
	sub_821A7258(ctx, base);
	// b 0x821a2de4
	goto loc_821A2DE4;
loc_821A2DC8:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,-14824
	ctx.r3.s64 = r11.s64 + -14824;
	// bl 0x821a2690
	sub_821A2690(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,-14920
	ctx.r3.s64 = r11.s64 + -14920;
	// bl 0x821a2690
	sub_821A2690(ctx, base);
	// twi 31,r0,22
loc_821A2DE4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A2DF8"))) PPC_WEAK_FUNC(sub_821A2DF8);
PPC_FUNC_IMPL(__imp__sub_821A2DF8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,32712
	r11.s64 = 2143813632;
	// li r10,7
	ctx.r10.s64 = 7;
	// stw r10,12820(r11)
	PPC_MM_STORE_U32(r11.u32 + 12820, ctx.r10.u32);
	// eieio 
	// li r10,2048
	ctx.r10.s64 = 2048;
	// stw r10,13320(r11)
	PPC_MM_STORE_U32(r11.u32 + 13320, ctx.r10.u32);
	// eieio 
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A2E18"))) PPC_WEAK_FUNC(sub_821A2E18);
PPC_FUNC_IMPL(__imp__sub_821A2E18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8240fa2c
	__imp__ExRegisterTitleTerminateNotification(ctx, base);
	// bl 0x8240fa1c
	__imp__VdShutdownEngines(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A2E40"))) PPC_WEAK_FUNC(sub_821A2E40);
PPC_FUNC_IMPL(__imp__sub_821A2E40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed124
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32019
	r11.s64 = -2098397184;
	// li r23,0
	r23.s64 = 0;
	// addi r29,r11,27448
	r29.s64 = r11.s64 + 27448;
	// mr r30,r23
	r30.u64 = r23.u64;
	// lwz r25,13580(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 13580);
	// lwz r24,22280(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 22280);
loc_821A2E68:
	// rlwinm r11,r30,30,2,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// add r9,r30,r31
	ctx.r9.u64 = r30.u64 + r31.u64;
	// addi r8,r11,16
	ctx.r8.s64 = r11.s64 + 16;
	// addi r11,r11,137
	r11.s64 = r11.s64 + 137;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwx r10,r8,r31
	PPC_STORE_U32(ctx.r8.u32 + r31.u32, ctx.r10.u32);
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
	// lwz r4,8(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r11,64(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 64);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r29,r29,12
	r29.s64 = r29.s64 + 12;
	// cmplwi cr6,r30,404
	cr6.compare<uint32_t>(r30.u32, 404, xer);
	// blt cr6,0x821a2e68
	if (cr6.getLT()) goto loc_821A2E68;
	// lis r11,-32019
	r11.s64 = -2098397184;
	// mr r28,r23
	r28.u64 = r23.u64;
	// addi r27,r31,1152
	r27.s64 = r31.s64 + 1152;
	// addi r26,r11,28664
	r26.s64 = r11.s64 + 28664;
loc_821A2EC4:
	// mr r30,r23
	r30.u64 = r23.u64;
	// addi r29,r26,8
	r29.s64 = r26.s64 + 8;
loc_821A2ECC:
	// rlwinm r11,r30,30,2,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 30) & 0x3FFFFFFF;
	// lwz r10,-4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + -4);
	// add r9,r30,r31
	ctx.r9.u64 = r30.u64 + r31.u64;
	// addi r8,r11,117
	ctx.r8.s64 = r11.s64 + 117;
	// addi r11,r11,238
	r11.s64 = r11.s64 + 238;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwx r10,r8,r31
	PPC_STORE_U32(ctx.r8.u32 + r31.u32, ctx.r10.u32);
	// lwz r10,-8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + -8);
	// stwx r10,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r10.u32);
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r11,468(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 468);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r29,r29,12
	r29.s64 = r29.s64 + 12;
	// cmplwi cr6,r30,80
	cr6.compare<uint32_t>(r30.u32, 80, xer);
	// blt cr6,0x821a2ecc
	if (cr6.getLT()) goto loc_821A2ECC;
	// addi r11,r28,32
	r11.s64 = r28.s64 + 32;
	// li r10,1
	ctx.r10.s64 = 1;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// rldicr r10,r10,63,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 63) & 0xFFFFFFFFFFFFFFFF;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// srd r6,r10,r11
	ctx.r6.u64 = r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (r11.u8 & 0x7F));
	// bl 0x8218be00
	sub_8218BE00(ctx, base);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r27,r27,24
	r27.s64 = r27.s64 + 24;
	// cmplwi cr6,r28,26
	cr6.compare<uint32_t>(r28.u32, 26, xer);
	// blt cr6,0x821a2ec4
	if (cr6.getLT()) goto loc_821A2EC4;
	// li r11,5
	r11.s64 = 5;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,12184(r31)
	PPC_STORE_U32(r31.u32 + 12184, r11.u32);
	// stw r10,12188(r31)
	PPC_STORE_U32(r31.u32 + 12188, ctx.r10.u32);
	// bl 0x8219e7d0
	sub_8219E7D0(ctx, base);
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8218fa20
	sub_8218FA20(ctx, base);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82194868
	sub_82194868(ctx, base);
	// clrlwi r4,r24,31
	ctx.r4.u64 = r24.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82193670
	sub_82193670(ctx, base);
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// stw r23,12708(r31)
	PPC_STORE_U32(r31.u32 + 12708, r23.u32);
	// rlwinm. r11,r10,0,28,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a3060
	if (!cr0.getEQ()) goto loc_821A3060;
	// rlwinm. r11,r10,0,29,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a3060
	if (!cr0.getEQ()) goto loc_821A3060;
	// lbz r11,12179(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 12179);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821a3060
	if (!cr0.getEQ()) goto loc_821A3060;
	// rlwinm. r11,r10,0,27,27
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a2fc4
	if (cr0.getEQ()) goto loc_821A2FC4;
	// li r11,1
	r11.s64 = 1;
	// b 0x821a3054
	goto loc_821A3054;
loc_821A2FC4:
	// rlwinm. r11,r10,0,26,26
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a304c
	if (cr0.getEQ()) goto loc_821A304C;
	// lwz r11,12432(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12432);
	// lwz r9,12720(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12720);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x821a2fe4
	if (cr6.getEQ()) goto loc_821A2FE4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a304c
	if (!cr6.getEQ()) goto loc_821A304C;
loc_821A2FE4:
	// lwz r11,12436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12436);
	// lwz r9,12724(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12724);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x821a2ffc
	if (cr6.getEQ()) goto loc_821A2FFC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a304c
	if (!cr6.getEQ()) goto loc_821A304C;
loc_821A2FFC:
	// lwz r11,12440(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12440);
	// lwz r9,12728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12728);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x821a3014
	if (cr6.getEQ()) goto loc_821A3014;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a304c
	if (!cr6.getEQ()) goto loc_821A304C;
loc_821A3014:
	// lwz r11,12444(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12444);
	// lwz r9,12732(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12732);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x821a302c
	if (cr6.getEQ()) goto loc_821A302C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a304c
	if (!cr6.getEQ()) goto loc_821A304C;
loc_821A302C:
	// lwz r11,12448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12448);
	// lwz r9,12736(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12736);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x821a3044
	if (cr6.getEQ()) goto loc_821A3044;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a304c
	if (!cr6.getEQ()) goto loc_821A304C;
loc_821A3044:
	// li r11,1
	r11.s64 = 1;
	// b 0x821a3050
	goto loc_821A3050;
loc_821A304C:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_821A3050:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
loc_821A3054:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne 0x821a3064
	if (!cr0.getEQ()) goto loc_821A3064;
loc_821A3060:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_821A3064:
	// rlwimi r11,r10,0,24,30
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFE) | (r11.u64 & 0xFFFFFFFFFFFFFF01);
	// stw r23,12704(r31)
	PPC_STORE_U32(r31.u32 + 12704, r23.u32);
	// li r9,-1
	ctx.r9.s64 = -1;
	// stw r23,10932(r31)
	PPC_STORE_U32(r31.u32 + 10932, r23.u32);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stw r23,10936(r31)
	PPC_STORE_U32(r31.u32 + 10936, r23.u32);
	// stw r9,12700(r31)
	PPC_STORE_U32(r31.u32 + 12700, ctx.r9.u32);
	// stb r11,10940(r31)
	PPC_STORE_U8(r31.u32 + 10940, r11.u8);
	// rlwinm. r10,r11,0,24,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a30a0
	if (!cr0.getEQ()) goto loc_821A30A0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8218e450
	sub_8218E450(ctx, base);
loc_821A30A0:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed174
	return;
}

__attribute__((alias("__imp__sub_821A30A8"))) PPC_WEAK_FUNC(sub_821A30A8);
PPC_FUNC_IMPL(__imp__sub_821A30A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed120
	// stwu r1,-1760(r1)
	ea = -1760 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r30,64(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// lwz r28,0(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r27,4(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmpwi r30,0
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// lwz r24,16(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r22,40(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// lwz r25,8(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bne 0x821a30e4
	if (!cr0.getEQ()) goto loc_821A30E4;
	// clrlwi r11,r25,26
	r11.u64 = r25.u32 & 0x3F;
	// b 0x821a30f4
	goto loc_821A30F4;
loc_821A30E4:
	// lis r11,10280
	r11.s64 = 673710080;
	// ori r11,r11,390
	r11.u64 = r11.u64 | 390;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// bne cr6,0x821a30fc
	if (!cr6.getEQ()) goto loc_821A30FC;
loc_821A30F4:
	// lis r30,10280
	r30.s64 = 673710080;
	// b 0x821a3110
	goto loc_821A3110;
loc_821A30FC:
	// lis r11,6184
	r11.s64 = 405274624;
	// ori r11,r11,390
	r11.u64 = r11.u64 | 390;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// bne cr6,0x821a3118
	if (!cr6.getEQ()) goto loc_821A3118;
	// lis r30,6184
	r30.s64 = 405274624;
loc_821A3110:
	// ori r30,r30,262
	r30.u64 = r30.u64 | 262;
	// b 0x821a3148
	goto loc_821A3148;
loc_821A3118:
	// lis r11,10280
	r11.s64 = 673710080;
	// ori r11,r11,438
	r11.u64 = r11.u64 | 438;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// bne cr6,0x821a3130
	if (!cr6.getEQ()) goto loc_821A3130;
	// lis r30,10280
	r30.s64 = 673710080;
	// b 0x821a3144
	goto loc_821A3144;
loc_821A3130:
	// lis r11,6184
	r11.s64 = 405274624;
	// ori r11,r11,438
	r11.u64 = r11.u64 | 438;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// bne cr6,0x821a3148
	if (!cr6.getEQ()) goto loc_821A3148;
	// lis r30,6184
	r30.s64 = 405274624;
loc_821A3144:
	// ori r30,r30,310
	r30.u64 = r30.u64 | 310;
loc_821A3148:
	// clrlwi r23,r30,26
	r23.u64 = r30.u32 & 0x3F;
	// mr r26,r23
	r26.u64 = r23.u64;
	// cmplwi cr6,r26,50
	cr6.compare<uint32_t>(r26.u32, 50, xer);
	// bne cr6,0x821a3160
	if (!cr6.getEQ()) goto loc_821A3160;
	// li r26,6
	r26.s64 = 6;
	// b 0x821a316c
	goto loc_821A316C;
loc_821A3160:
	// cmplwi cr6,r26,7
	cr6.compare<uint32_t>(r26.u32, 7, xer);
	// bne cr6,0x821a316c
	if (!cr6.getEQ()) goto loc_821A316C;
	// li r26,54
	r26.s64 = 54;
loc_821A316C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8240fa4c
	__imp__VdQueryVideoMode(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,100(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f13.f64 = double(temp.f32);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r9,r29,21536
	ctx.r9.s64 = r29.s64 + 21536;
	// lfs f0,2692(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2692);
	f0.f64 = double(temp.f32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// fadds f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// stw r10,21528(r29)
	PPC_STORE_U32(r29.u32 + 21528, ctx.r10.u32);
	// stw r11,21524(r29)
	PPC_STORE_U32(r29.u32 + 21524, r11.u32);
	// stw r11,21532(r29)
	PPC_STORE_U32(r29.u32 + 21532, r11.u32);
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a31e8
	if (!cr6.getEQ()) goto loc_821A31E8;
	// li r10,3
	ctx.r10.s64 = 3;
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8218bad8
	sub_8218BAD8(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x821a31e4
	if (!cr0.getEQ()) goto loc_821A31E4;
loc_821A31DC:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x821a32dc
	goto loc_821A32DC;
loc_821A31E4:
	// stw r3,14812(r29)
	PPC_STORE_U32(r29.u32 + 14812, ctx.r3.u32);
loc_821A31E8:
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a3224
	if (!cr6.getEQ()) goto loc_821A3224;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8218bbf8
	sub_8218BBF8(ctx, base);
	// mr. r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq 0x821a31dc
	if (cr0.getEQ()) goto loc_821A31DC;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r5,14816(r29)
	PPC_STORE_U32(r29.u32 + 14816, ctx.r5.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82195c00
	sub_82195C00(ctx, base);
loc_821A3224:
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821a325c
	if (cr6.getEQ()) goto loc_821A325C;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8218bbf8
	sub_8218BBF8(ctx, base);
	// mr. r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq 0x821a31dc
	if (cr0.getEQ()) goto loc_821A31DC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r4,14808(r29)
	PPC_STORE_U32(r29.u32 + 14808, ctx.r4.u32);
	// bl 0x82195f68
	sub_82195F68(ctx, base);
loc_821A325C:
	// addi r3,r29,13528
	ctx.r3.s64 = r29.s64 + 13528;
	// li r5,124
	ctx.r5.s64 = 124;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// oris r11,r23,10280
	r11.u64 = r23.u64 | 673710080;
	// addi r4,r31,96
	ctx.r4.s64 = r31.s64 + 96;
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,13592(r29)
	PPC_STORE_U32(r29.u32 + 13592, r11.u32);
	// bl 0x821a9058
	sub_821A9058(ctx, base);
	// cmplwi cr6,r26,7
	cr6.compare<uint32_t>(r26.u32, 7, xer);
	// beq cr6,0x821a32b4
	if (cr6.getEQ()) goto loc_821A32B4;
	// cmplwi cr6,r26,54
	cr6.compare<uint32_t>(r26.u32, 54, xer);
	// beq cr6,0x821a32b4
	if (cr6.getEQ()) goto loc_821A32B4;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x8219e480
	sub_8219E480(ctx, base);
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x821955c8
	sub_821955C8(ctx, base);
	// b 0x821a32d0
	goto loc_821A32D0;
loc_821A32B4:
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x8219e4d8
	sub_8219E4D8(ctx, base);
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82195690
	sub_82195690(ctx, base);
loc_821A32D0:
	// li r11,-1
	r11.s64 = -1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,16692(r29)
	PPC_STORE_U32(r29.u32 + 16692, r11.u32);
loc_821A32DC:
	// addi r1,r1,1760
	ctx.r1.s64 = ctx.r1.s64 + 1760;
	// b 0x823ed170
	return;
}

__attribute__((alias("__imp__sub_821A32E8"))) PPC_WEAK_FUNC(sub_821A32E8);
PPC_FUNC_IMPL(__imp__sub_821A32E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x821aa518
	sub_821AA518(ctx, base);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a3340
	if (cr6.getEQ()) goto loc_821A3340;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195f68
	sub_82195F68(ctx, base);
	// mr r30,r29
	r30.u64 = r29.u64;
loc_821A331C:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195c00
	sub_82195C00(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmplwi cr6,r30,4
	cr6.compare<uint32_t>(r30.u32, 4, xer);
	// blt cr6,0x821a331c
	if (cr6.getLT()) goto loc_821A331C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d958
	sub_8219D958(ctx, base);
loc_821A3340:
	// lwz r3,14812(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14812);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a3354
	if (cr0.getEQ()) goto loc_821A3354;
	// bl 0x8219b708
	sub_8219B708(ctx, base);
	// stw r29,14812(r31)
	PPC_STORE_U32(r31.u32 + 14812, r29.u32);
loc_821A3354:
	// lwz r3,14816(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14816);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a3368
	if (cr0.getEQ()) goto loc_821A3368;
	// bl 0x8219b708
	sub_8219B708(ctx, base);
	// stw r29,14816(r31)
	PPC_STORE_U32(r31.u32 + 14816, r29.u32);
loc_821A3368:
	// lwz r3,14808(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 14808);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a337c
	if (cr0.getEQ()) goto loc_821A337C;
	// bl 0x8219b708
	sub_8219B708(ctx, base);
	// stw r29,14808(r31)
	PPC_STORE_U32(r31.u32 + 14808, r29.u32);
loc_821A337C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821A3388"))) PPC_WEAK_FUNC(sub_821A3388);
PPC_FUNC_IMPL(__imp__sub_821A3388) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821a33cc
	if (cr6.getEQ()) goto loc_821A33CC;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// bl 0x821a3388
	sub_821A3388(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lis r4,-20096
	ctx.r4.s64 = -1317011456;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_821A33CC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A33E0"))) PPC_WEAK_FUNC(sub_821A33E0);
PPC_FUNC_IMPL(__imp__sub_821A33E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x821a3454
	if (cr6.getEQ()) goto loc_821A3454;
	// lwz r11,64(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 64);
	// li r10,0
	ctx.r10.s64 = 0;
	// clrlwi r11,r11,26
	r11.u64 = r11.u32 & 0x3F;
	// cmplwi cr6,r11,7
	cr6.compare<uint32_t>(r11.u32, 7, xer);
	// beq cr6,0x821a3418
	if (cr6.getEQ()) goto loc_821A3418;
	// cmplwi cr6,r11,54
	cr6.compare<uint32_t>(r11.u32, 54, xer);
	// bne cr6,0x821a341c
	if (!cr6.getEQ()) goto loc_821A341C;
loc_821A3418:
	// lis r10,2048
	ctx.r10.s64 = 134217728;
loc_821A341C:
	// lwz r11,68(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 68);
	// lis r9,16384
	ctx.r9.s64 = 1073741824;
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// rlwimi r9,r11,28,2,3
	ctx.r9.u64 = (__builtin_rotateleft32(r11.u32, 28) & 0x30000000) | (ctx.r9.u64 & 0xFFFFFFFFCFFFFFFF);
	// cmplwi cr6,r8,720
	cr6.compare<uint32_t>(ctx.r8.u32, 720, xer);
	// or r3,r9,r10
	ctx.r3.u64 = ctx.r9.u64 | ctx.r10.u64;
	// bne cr6,0x821a3450
	if (!cr6.getEQ()) goto loc_821A3450;
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// cmplwi cr6,r11,480
	cr6.compare<uint32_t>(r11.u32, 480, xer);
	// beq cr6,0x821a344c
	if (cr6.getEQ()) goto loc_821A344C;
	// cmplwi cr6,r11,576
	cr6.compare<uint32_t>(r11.u32, 576, xer);
	// bne cr6,0x821a3450
	if (!cr6.getEQ()) goto loc_821A3450;
loc_821A344C:
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
loc_821A3450:
	// bl 0x8240fa6c
	__imp__VdSetDisplayMode(ctx, base);
loc_821A3454:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8240fa5c
	__imp__VdGetCurrentDisplayInformation(ctx, base);
	// lhz r11,152(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 152);
	// lhz r10,154(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 154);
	// lhz r9,166(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 166);
	// stw r11,21524(r31)
	PPC_STORE_U32(r31.u32 + 21524, r11.u32);
	// stw r10,21528(r31)
	PPC_STORE_U32(r31.u32 + 21528, ctx.r10.u32);
	// stw r9,21532(r31)
	PPC_STORE_U32(r31.u32 + 21532, ctx.r9.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A3488"))) PPC_WEAK_FUNC(sub_821A3488);
PPC_FUNC_IMPL(__imp__sub_821A3488) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r7,r11,-5040
	ctx.r7.s64 = r11.s64 + -5040;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r6,r11,-6192
	ctx.r6.s64 = r11.s64 + -6192;
	// lis r11,-32230
	r11.s64 = -2112225280;
	// lis r3,6274
	ctx.r3.s64 = 411172864;
	// addi r4,r11,11768
	ctx.r4.s64 = r11.s64 + 11768;
	// bl 0x8240fa8c
	__imp__VdInitializeEngines(ctx, base);
	// lis r11,-32019
	r11.s64 = -2098397184;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r11,29376
	ctx.r3.s64 = r11.s64 + 29376;
	// bl 0x8240fa2c
	__imp__ExRegisterTitleTerminateNotification(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// lis r11,-32230
	r11.s64 = -2112225280;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r11,-14792
	ctx.r3.s64 = r11.s64 + -14792;
	// stw r10,16696(r31)
	PPC_STORE_U32(r31.u32 + 16696, ctx.r10.u32);
	// bl 0x8240fa7c
	__imp__VdSetGraphicsInterruptCallback(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A3500"))) PPC_WEAK_FUNC(sub_821A3500);
PPC_FUNC_IMPL(__imp__sub_821A3500) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,1
	r11.s64 = 1;
	// stw r11,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r11.u32);
	// lis r11,-32019
	r11.s64 = -2098397184;
	// ld r11,28912(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 28912);
	// std r11,10880(r31)
	PPC_STORE_U64(r31.u32 + 10880, r11.u64);
	// bl 0x8235eaa8
	sub_8235EAA8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// lis r4,-19072
	ctx.r4.s64 = -1249902592;
	// li r3,4800
	ctx.r3.s64 = 4800;
	// stw r11,10888(r31)
	PPC_STORE_U32(r31.u32 + 10888, r11.u32);
	// stw r10,10892(r31)
	PPC_STORE_U32(r31.u32 + 10892, ctx.r10.u32);
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cntlzw r11,r3
	r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// stw r3,16712(r31)
	PPC_STORE_U32(r31.u32 + 16712, ctx.r3.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = r11.u64 ^ 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A3570"))) PPC_WEAK_FUNC(sub_821A3570);
PPC_FUNC_IMPL(__imp__sub_821A3570) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r3,r31,14928
	ctx.r3.s64 = r31.s64 + 14928;
	// bl 0x8240f94c
	__imp__RtlInitializeCriticalSection(ctx, base);
	// addi r3,r31,14956
	ctx.r3.s64 = r31.s64 + 14956;
	// bl 0x8240f94c
	__imp__RtlInitializeCriticalSection(ctx, base);
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// ori r11,r11,4
	r11.u64 = r11.u64 | 4;
	// stb r11,10942(r31)
	PPC_STORE_U8(r31.u32 + 10942, r11.u8);
	// bl 0x821a3488
	sub_821A3488(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821a35c0
	if (!cr0.getEQ()) goto loc_821A35C0;
loc_821A35B8:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x821a3764
	goto loc_821A3764;
loc_821A35C0:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r31,16700
	ctx.r5.s64 = r31.s64 + 16700;
	// li r4,10
	ctx.r4.s64 = 10;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// li r3,3
	ctx.r3.s64 = 3;
	// stw r31,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r31.u32);
	// li r11,0
	r11.s64 = 0;
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
	// bl 0x8240fa3c
	__imp__ExGetXConfigSetting(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821a35b8
	if (cr0.getLT()) goto loc_821A35B8;
	// addi r4,r30,72
	ctx.r4.s64 = r30.s64 + 72;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219da28
	sub_8219DA28(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821a35b8
	if (!cr0.getEQ()) goto loc_821A35B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a4ee8
	sub_821A4EE8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a35b8
	if (cr0.getEQ()) goto loc_821A35B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8218d928
	sub_8218D928(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a35b8
	if (cr0.getEQ()) goto loc_821A35B8;
	// li r11,-1
	r11.s64 = -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,21540(r31)
	PPC_STORE_U32(r31.u32 + 21540, r11.u32);
	// stw r11,21544(r31)
	PPC_STORE_U32(r31.u32 + 21544, r11.u32);
	// bl 0x821ab6f0
	sub_821AB6F0(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a30a8
	sub_821A30A8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a35b8
	if (cr0.getEQ()) goto loc_821A35B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a2e40
	sub_821A2E40(ctx, base);
	// lis r4,-20096
	ctx.r4.s64 = -1317011456;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1756(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1756);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a3680
	if (cr0.getEQ()) goto loc_821A3680;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// b 0x821a3684
	goto loc_821A3684;
loc_821A3680:
	// li r11,0
	r11.s64 = 0;
loc_821A3684:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a36b8
	if (cr6.getEQ()) goto loc_821A36B8;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// ble cr6,0x821a36a8
	if (!cr6.getGT()) goto loc_821A36A8;
	// addi r10,r31,16704
	ctx.r10.s64 = r31.s64 + 16704;
	// addi r9,r31,16708
	ctx.r9.s64 = r31.s64 + 16708;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
loc_821A36A8:
	// addi r10,r31,16544
	ctx.r10.s64 = r31.s64 + 16544;
	// addi r9,r31,21556
	ctx.r9.s64 = r31.s64 + 21556;
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// stw r9,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r9.u32);
loc_821A36B8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x8235eb60
	sub_8235EB60(ctx, base);
	// ld r11,88(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,2776(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	ctx.f13.f64 = double(temp.f32);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,21568(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 21568, temp.u32);
	// fdivs f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 / f0.f64));
	// stfs f0,21572(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 21572, temp.u32);
	// bl 0x821a6ce8
	sub_821A6CE8(ctx, base);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a9088
	sub_821A9088(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195758
	sub_82195758(ctx, base);
	// lis r30,-31991
	r30.s64 = -2096562176;
	// lwz r11,-31396(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + -31396);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a3738
	if (!cr6.getEQ()) goto loc_821A3738;
	// bl 0x8240fa9c
	__imp__VdIsHSIOTrainingSucceeded(ctx, base);
	// cntlzw r11,r3
	r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// rlwinm. r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,-31396(r30)
	PPC_STORE_U32(r30.u32 + -31396, r11.u32);
	// beq 0x821a3744
	if (cr0.getEQ()) goto loc_821A3744;
loc_821A3738:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,-14448
	ctx.r3.s64 = r11.s64 + -14448;
	// bl 0x821a2690
	sub_821A2690(ctx, base);
loc_821A3744:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a92a8
	sub_821A92A8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821a3760
	if (!cr0.getEQ()) goto loc_821A3760;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,-14520
	ctx.r3.s64 = r11.s64 + -14520;
	// bl 0x821a2690
	sub_821A2690(ctx, base);
loc_821A3760:
	// li r3,1
	ctx.r3.s64 = 1;
loc_821A3764:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A3780"))) PPC_WEAK_FUNC(sub_821A3780);
PPC_FUNC_IMPL(__imp__sub_821A3780) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x8219c7e0
	sub_8219C7E0(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,21540(r31)
	PPC_STORE_U32(r31.u32 + 21540, r11.u32);
	// stw r11,21544(r31)
	PPC_STORE_U32(r31.u32 + 21544, r11.u32);
	// stw r11,14920(r31)
	PPC_STORE_U32(r31.u32 + 14920, r11.u32);
	// bl 0x821ab6f0
	sub_821AB6F0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a2e40
	sub_821A2E40(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A37D0"))) PPC_WEAK_FUNC(sub_821A37D0);
PPC_FUNC_IMPL(__imp__sub_821A37D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a37f0
	if (cr6.getEQ()) goto loc_821A37F0;
	// bl 0x82198d58
	sub_82198D58(ctx, base);
loc_821A37F0:
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// lis r28,-20096
	r28.s64 = -1317011456;
	// rlwinm. r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a3970
	if (!cr0.getEQ()) goto loc_821A3970;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a3814
	if (cr6.getEQ()) goto loc_821A3814;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d958
	sub_8219D958(ctx, base);
loc_821A3814:
	// addi r30,r31,21612
	r30.s64 = r31.s64 + 21612;
	// li r29,4
	r29.s64 = 4;
loc_821A381C:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a382c
	if (cr0.getEQ()) goto loc_821A382C;
	// bl 0x821a0618
	sub_821A0618(ctx, base);
loc_821A382C:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// bne 0x821a381c
	if (!cr0.getEQ()) goto loc_821A381C;
	// lwz r3,21628(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21628);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a384c
	if (cr0.getEQ()) goto loc_821A384C;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_821A384C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821ab230
	sub_821AB230(ctx, base);
	// lis r30,-32256
	r30.s64 = -2113929216;
	// lwz r11,1756(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1756);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x821a3880
	if (cr0.getEQ()) goto loc_821A3880;
	// lwz r11,24(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,28
	ctx.r3.s64 = 28;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,1756(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1756);
loc_821A3880:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x821a38a4
	if (cr0.getEQ()) goto loc_821A38A4;
	// lwz r11,24(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// li r4,2
	ctx.r4.s64 = 2;
	// li r3,48
	ctx.r3.s64 = 48;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,1756(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1756);
loc_821A38A4:
	// lis r10,-32230
	ctx.r10.s64 = -2112225280;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// addi r10,r10,25352
	ctx.r10.s64 = ctx.r10.s64 + 25352;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a38d4
	if (cr0.getEQ()) goto loc_821A38D4;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,65
	ctx.r3.s64 = 65;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821A38D4:
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8240f9bc
	__imp__VdSetSystemCommandBufferGpuIdentifierAddress(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a32e8
	sub_821A32E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a5038
	sub_821A5038(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8218da60
	sub_8218DA60(ctx, base);
	// lwz r11,1756(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1756);
	// li r30,0
	r30.s64 = 0;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a3910
	if (cr0.getEQ()) goto loc_821A3910;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// b 0x821a3914
	goto loc_821A3914;
loc_821A3910:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_821A3914:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a3938
	if (cr6.getEQ()) goto loc_821A3938;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// ble cr6,0x821a3930
	if (!cr6.getGT()) goto loc_821A3930;
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
loc_821A3930:
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// stw r30,28(r11)
	PPC_STORE_U32(r11.u32 + 28, r30.u32);
loc_821A3938:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8240fa7c
	__imp__VdSetGraphicsInterruptCallback(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219da28
	sub_8219DA28(ctx, base);
	// lis r11,-32019
	r11.s64 = -2098397184;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r11,29376
	ctx.r3.s64 = r11.s64 + 29376;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// bl 0x8240fa2c
	__imp__ExRegisterTitleTerminateNotification(ctx, base);
	// bl 0x8240fa1c
	__imp__VdShutdownEngines(ctx, base);
loc_821A3970:
	// lwz r3,16712(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 16712);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a3984
	if (cr0.getEQ()) goto loc_821A3984;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_821A3984:
	// lwz r3,21552(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 21552);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a3994
	if (cr0.getEQ()) goto loc_821A3994;
	// bl 0x821a3388
	sub_821A3388(ctx, base);
loc_821A3994:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821A39A0"))) PPC_WEAK_FUNC(sub_821A39A0);
PPC_FUNC_IMPL(__imp__sub_821A39A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// lis r11,2031
	r11.s64 = 133103616;
	// addis r10,r3,-32528
	ctx.r10.s64 = ctx.r3.s64 + -2131755008;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blelr cr6
	if (!cr6.getGT()) return;
	// addi r10,r4,127
	ctx.r10.s64 = ctx.r4.s64 + 127;
	// rlwinm r11,r3,0,0,24
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFFFF80;
	// rlwinm r10,r10,0,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// srawi r10,r10,7
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm. r9,r10,29,3,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// beq 0x821a3a20
	if (cr0.getEQ()) goto loc_821A3A20;
loc_821A39D8:
	// dcbf r0,r11
	// li r8,128
	ctx.r8.s64 = 128;
	// dcbf r8,r11
	// li r8,256
	ctx.r8.s64 = 256;
	// dcbf r8,r11
	// li r8,384
	ctx.r8.s64 = 384;
	// dcbf r8,r11
	// li r8,512
	ctx.r8.s64 = 512;
	// dcbf r8,r11
	// li r8,640
	ctx.r8.s64 = 640;
	// dcbf r8,r11
	// li r8,768
	ctx.r8.s64 = 768;
	// dcbf r8,r11
	// li r8,896
	ctx.r8.s64 = 896;
	// dcbf r8,r11
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r11,r11,1024
	r11.s64 = r11.s64 + 1024;
	// bne 0x821a39d8
	if (!cr0.getEQ()) goto loc_821A39D8;
loc_821A3A20:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x821a3a38
	if (cr6.getEQ()) goto loc_821A3A38;
loc_821A3A28:
	// dcbf r0,r11
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,128
	r11.s64 = r11.s64 + 128;
	// bne 0x821a3a28
	if (!cr0.getEQ()) goto loc_821A3A28;
loc_821A3A38:
	// sync 
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A3A40"))) PPC_WEAK_FUNC(sub_821A3A40);
PPC_FUNC_IMPL(__imp__sub_821A3A40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r10,r11,-10896
	ctx.r10.s64 = r11.s64 + -10896;
loc_821A3A48:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// addi r6,r11,2
	ctx.r6.s64 = r11.s64 + 2;
	// clrlwi r6,r6,20
	ctx.r6.u64 = ctx.r6.u32 & 0xFFF;
loc_821A3A58:
	// mfmsr r8
	// mtmsrd r13,1
	// lwarx r9,0,r7
	reserved.u32 = *(uint32_t*)(base + ctx.r7.u32);
	ctx.r9.u64 = __builtin_bswap32(reserved.u32);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bne cr6,0x821a3a7c
	if (!cr6.getEQ()) goto loc_821A3A7C;
	// stwcx. r6,0,r7
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r7.u32), reserved.s32, __builtin_bswap32(ctx.r6.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r8,1
	// bne 0x821a3a58
	if (!cr0.getEQ()) goto loc_821A3A58;
	// b 0x821a3a84
	goto loc_821A3A84;
loc_821A3A7C:
	// stwcx. r9,0,r7
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r7.u32), reserved.s32, __builtin_bswap32(ctx.r9.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r8,1
loc_821A3A84:
	// mr r9,r9
	ctx.r9.u64 = ctx.r9.u64;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x821a3a48
	if (!cr6.getEQ()) goto loc_821A3A48;
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// lbz r8,268(r13)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r13.u32 + 268);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,-27280
	ctx.r10.s64 = ctx.r10.s64 + -27280;
	// addi r9,r10,4
	ctx.r9.s64 = ctx.r10.s64 + 4;
	// stwx r8,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r8.u32);
	// stwx r3,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, ctx.r3.u32);
	// lwsync 
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A3AB8"))) PPC_WEAK_FUNC(sub_821A3AB8);
PPC_FUNC_IMPL(__imp__sub_821A3AB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r10,1
	ctx.r10.s64 = 1;
	// clrlwi. r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// lis r31,4
	r31.s64 = 262144;
	// stw r10,96(r3)
	PPC_STORE_U32(ctx.r3.u32 + 96, ctx.r10.u32);
	// bne 0x821a3ae4
	if (!cr0.getEQ()) goto loc_821A3AE4;
	// lis r31,2
	r31.s64 = 131072;
loc_821A3AE4:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r30,32528
	r30.s64 = 2131755008;
	// ori r7,r11,1
	ctx.r7.u64 = r11.u64 | 1;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// lis r4,32528
	ctx.r4.s64 = 2131755008;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// bl 0x8240faac
	__imp__KeLockL2(ctx, base);
	// add r11,r31,r30
	r11.u64 = r31.u64 + r30.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_821A3B10:
	// dcbzl r0,r30
	memset(base + ((r30.u32) & ~127), 0, 128);
	// li r10,128
	ctx.r10.s64 = 128;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// dcbzl r10,r11
	memset(base + ((ctx.r10.u32 + r11.u32) & ~127), 0, 128);
	// li r10,256
	ctx.r10.s64 = 256;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// dcbzl r10,r11
	memset(base + ((ctx.r10.u32 + r11.u32) & ~127), 0, 128);
	// li r10,384
	ctx.r10.s64 = 384;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// dcbzl r10,r11
	memset(base + ((ctx.r10.u32 + r11.u32) & ~127), 0, 128);
	// li r10,512
	ctx.r10.s64 = 512;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// dcbzl r10,r11
	memset(base + ((ctx.r10.u32 + r11.u32) & ~127), 0, 128);
	// li r10,640
	ctx.r10.s64 = 640;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// dcbzl r10,r11
	memset(base + ((ctx.r10.u32 + r11.u32) & ~127), 0, 128);
	// li r10,768
	ctx.r10.s64 = 768;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// dcbzl r10,r11
	memset(base + ((ctx.r10.u32 + r11.u32) & ~127), 0, 128);
	// li r10,896
	ctx.r10.s64 = 896;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// dcbzl r10,r11
	memset(base + ((ctx.r10.u32 + r11.u32) & ~127), 0, 128);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r30,r11,1024
	r30.s64 = r11.s64 + 1024;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// blt cr6,0x821a3b10
	if (cr6.getLT()) goto loc_821A3B10;
	// sync 
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A3BA0"))) PPC_WEAK_FUNC(sub_821A3BA0);
PPC_FUNC_IMPL(__imp__sub_821A3BA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,40(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// srawi. r9,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x821a3bb8
	if (cr0.getEQ()) goto loc_821A3BB8;
	// lis r11,10922
	r11.s64 = 715784192;
	// ori r11,r11,43690
	r11.u64 = r11.u64 | 43690;
	// b 0x821a3bd0
	goto loc_821A3BD0;
loc_821A3BB8:
	// rlwinm. r11,r10,0,1,1
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a3bcc
	if (cr0.getEQ()) goto loc_821A3BCC;
	// lis r11,5461
	r11.s64 = 357892096;
	// ori r11,r11,21845
	r11.u64 = r11.u64 | 21845;
	// b 0x821a3bd0
	goto loc_821A3BD0;
loc_821A3BCC:
	// li r11,-1
	r11.s64 = -1;
loc_821A3BD0:
	// rlwinm. r10,r10,0,2,2
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a3c00
	if (cr0.getEQ()) goto loc_821A3C00;
	// lwz r10,44(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// li r8,3
	ctx.r8.s64 = 3;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// slw r10,r8,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r7.u8 & 0x3F));
	// and r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	// bne 0x821a3c00
	if (!cr0.getEQ()) goto loc_821A3C00;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x821a3c00
	if (!cr6.getEQ()) goto loc_821A3C00;
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
loc_821A3C00:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A3C08"))) PPC_WEAK_FUNC(sub_821A3C08);
PPC_FUNC_IMPL(__imp__sub_821A3C08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed128
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// addi r27,r11,8
	r27.s64 = r11.s64 + 8;
	// lwz r26,4(r11)
	r26.u64 = PPC_LOAD_U32(r11.u32 + 4);
loc_821A3C28:
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r10,r3,4
	ctx.r10.s64 = ctx.r3.s64 + 4;
	// cmplw cr6,r10,r4
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r4.u32, xer);
	// bge cr6,0x821a3ce0
	if (!cr6.getLT()) goto loc_821A3CE0;
	// addi r28,r26,-1
	r28.s64 = r26.s64 + -1;
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
loc_821A3C40:
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lhz r8,-2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lhz r31,-4(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// lhz r30,0(r11)
	r30.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// rlwinm r29,r9,3,0,28
	r29.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r9,r7,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// rotlwi r31,r31,3
	r31.u64 = __builtin_rotateleft32(r31.u32, 3);
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + r27.u64;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// rotlwi r30,r30,3
	r30.u64 = __builtin_rotateleft32(r30.u32, 3);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
loc_821A3C80:
	// lwz r24,-4(r9)
	r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplw cr6,r24,r8
	cr6.compare<uint32_t>(r24.u32, ctx.r8.u32, xer);
	// bge cr6,0x821a3cb8
	if (!cr6.getLT()) goto loc_821A3CB8;
	// lwz r24,4(r9)
	r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmplw cr6,r24,r31
	cr6.compare<uint32_t>(r24.u32, r31.u32, xer);
	// ble cr6,0x821a3cb8
	if (!cr6.getGT()) goto loc_821A3CB8;
	// lwz r24,0(r9)
	r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r24,r29
	cr6.compare<uint32_t>(r24.u32, r29.u32, xer);
	// bge cr6,0x821a3cb8
	if (!cr6.getLT()) goto loc_821A3CB8;
	// lwz r24,8(r9)
	r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplw cr6,r24,r30
	cr6.compare<uint32_t>(r24.u32, r30.u32, xer);
	// ble cr6,0x821a3cb8
	if (!cr6.getGT()) goto loc_821A3CB8;
	// ori r6,r6,3
	ctx.r6.u64 = ctx.r6.u64 | 3;
loc_821A3CB8:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r9,r9,-16
	ctx.r9.s64 = ctx.r9.s64 + -16;
	// bne cr6,0x821a3c80
	if (!cr6.getEQ()) goto loc_821A3C80;
	// oris r9,r6,32768
	ctx.r9.u64 = ctx.r6.u64 | 2147483648;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r10,r4
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r4.u32, xer);
	// stw r9,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, ctx.r9.u32);
	// blt cr6,0x821a3c40
	if (cr6.getLT()) goto loc_821A3C40;
loc_821A3CE0:
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// beq cr6,0x821a3cf4
	if (cr6.getEQ()) goto loc_821A3CF4;
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x821a39a0
	sub_821A39A0(ctx, base);
loc_821A3CF4:
	// lis r11,-16384
	r11.s64 = -1073741824;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x821a3c28
	if (!cr6.getEQ()) goto loc_821A3C28;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed178
	return;
}

__attribute__((alias("__imp__sub_821A3D10"))) PPC_WEAK_FUNC(sub_821A3D10);
PPC_FUNC_IMPL(__imp__sub_821A3D10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x821a3c08
	sub_821A3C08(ctx, base);
	// mr r11,r31
	r11.u64 = r31.u64;
loc_821A3D30:
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r31,r11,4
	r31.s64 = r11.s64 + 4;
	// b 0x821a3d50
	goto loc_821A3D50;
loc_821A3D3C:
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x821a3c08
	sub_821A3C08(ctx, base);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
loc_821A3D50:
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// blt cr6,0x821a3d3c
	if (cr6.getLT()) goto loc_821A3D3C;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a3d30
	if (!cr6.getEQ()) goto loc_821A3D30;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821A3D70"))) PPC_WEAK_FUNC(sub_821A3D70);
PPC_FUNC_IMPL(__imp__sub_821A3D70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a3ab8
	sub_821A3AB8(ctx, base);
	// li r29,0
	r29.s64 = 0;
	// lis r26,256
	r26.s64 = 16777216;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_821A3DA4:
	// slw r9,r26,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (r26.u32 << (r11.u8 & 0x3F));
	// and. r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 & r27.u64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x821a3db4
	if (cr0.getEQ()) goto loc_821A3DB4;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_821A3DB4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// blt cr6,0x821a3da4
	if (cr6.getLT()) goto loc_821A3DA4;
	// rlwinm r11,r27,0,2,7
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x3F000000;
	// clrlwi. r9,r27,31
	ctx.r9.u64 = r27.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stw r11,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r11.u32);
	// lis r11,4
	r11.s64 = 262144;
	// bne 0x821a3dd8
	if (!cr0.getEQ()) goto loc_821A3DD8;
	// lis r11,2
	r11.s64 = 131072;
loc_821A3DD8:
	// divwu r11,r11,r10
	r11.u32 = r11.u32 / ctx.r10.u32;
	// twllei r10,0
	// lwz r10,11796(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 11796);
	// rlwinm r6,r11,0,0,24
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lis r9,32528
	ctx.r9.s64 = 2131755008;
	// addi r11,r31,11332
	r11.s64 = r31.s64 + 11332;
loc_821A3DF4:
	// slw r8,r26,r7
	ctx.r8.u64 = ctx.r7.u8 & 0x20 ? 0 : (r26.u32 << (ctx.r7.u8 & 0x3F));
	// and. r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 & r27.u64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821a3e4c
	if (cr0.getEQ()) goto loc_821A3E4C;
	// lis r5,-16382
	ctx.r5.s64 = -1073610752;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// add r8,r9,r6
	ctx.r8.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stw r29,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, r29.u32);
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// ori r5,r5,22528
	ctx.r5.u64 = ctx.r5.u64 | 22528;
	// li r4,3
	ctx.r4.s64 = 3;
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// stw r9,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, ctx.r9.u32);
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// stw r5,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r5.u32);
	// stw r4,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r4.u32);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r8,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r8.u32);
	// lwz r8,-8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// lwz r5,-4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// rlwimi r8,r5,0,30,31
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r5.u32, 0) & 0x3) | (ctx.r8.u64 & 0xFFFFFFFFFFFFFFFC);
	// stw r8,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r8.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
loc_821A3E4C:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r11,r11,80
	r11.s64 = r11.s64 + 80;
	// cmplwi cr6,r7,6
	cr6.compare<uint32_t>(ctx.r7.u32, 6, xer);
	// blt cr6,0x821a3df4
	if (cr6.getLT()) goto loc_821A3DF4;
	// lwz r11,11796(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 11796);
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// li r5,1
	ctx.r5.s64 = 1;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// srawi r10,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// stw r29,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r29.u32);
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r10,8
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFFFF;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// oris r10,r10,33024
	ctx.r10.u64 = ctx.r10.u64 | 2164260864;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x8219caf8
	sub_8219CAF8(ctx, base);
	// addi r30,r31,11324
	r30.s64 = r31.s64 + 11324;
loc_821A3EA8:
	// slw r11,r26,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (r26.u32 << (r29.u8 & 0x3F));
	// and. r11,r11,r27
	r11.u64 = r11.u64 & r27.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a3ef8
	if (cr0.getEQ()) goto loc_821A3EF8;
	// lwz r28,4(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// rlwimi r28,r11,0,0,29
	r28.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0xFFFFFFFC) | (r28.u64 & 0xFFFFFFFF00000003);
	// bl 0x82198a90
	sub_82198A90(ctx, base);
loc_821A3ED0:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82198c28
	sub_82198C28(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a3ef0
	if (cr0.getEQ()) goto loc_821A3EF0;
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// bne cr6,0x821a3ed0
	if (!cr6.getEQ()) goto loc_821A3ED0;
loc_821A3EF0:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82198ac0
	sub_82198AC0(ctx, base);
loc_821A3EF8:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r30,r30,80
	r30.s64 = r30.s64 + 80;
	// cmplwi cr6,r29,6
	cr6.compare<uint32_t>(r29.u32, 6, xer);
	// blt cr6,0x821a3ea8
	if (cr6.getLT()) goto loc_821A3EA8;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821A3F10"))) PPC_WEAK_FUNC(sub_821A3F10);
PPC_FUNC_IMPL(__imp__sub_821A3F10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// bl 0x821a3ba0
	sub_821A3BA0(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r9,348(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 348);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// lwz r10,1768(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 1768);
	// lwz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// beq cr6,0x821a3f60
	if (cr6.getEQ()) goto loc_821A3F60;
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// stw r11,348(r6)
	PPC_STORE_U32(ctx.r6.u32 + 348, r11.u32);
	// li r5,2
	ctx.r5.s64 = 2;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ori r10,r10,25088
	ctx.r10.u64 = ctx.r10.u64 | 25088;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// bl 0x8219c480
	sub_8219C480(ctx, base);
loc_821A3F60:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A3F70"))) PPC_WEAK_FUNC(sub_821A3F70);
PPC_FUNC_IMPL(__imp__sub_821A3F70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r28,6
	r28.s64 = 6;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r31,r29,11328
	r31.s64 = r29.s64 + 11328;
loc_821A3F90:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a3fe0
	if (cr6.getEQ()) goto loc_821A3FE0;
	// lwz r30,-4(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + -4);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// rlwimi r30,r11,0,30,31
	r30.u64 = (__builtin_rotateleft32(r11.u32, 0) & 0x3) | (r30.u64 & 0xFFFFFFFFFFFFFFFC);
	// bl 0x82198a90
	sub_82198A90(ctx, base);
loc_821A3FB8:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198c28
	sub_82198C28(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a3fd8
	if (cr0.getEQ()) goto loc_821A3FD8;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bne cr6,0x821a3fb8
	if (!cr6.getEQ()) goto loc_821A3FB8;
loc_821A3FD8:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82198ac0
	sub_82198AC0(ctx, base);
loc_821A3FE0:
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r31,r31,80
	r31.s64 = r31.s64 + 80;
	// bne 0x821a3f90
	if (!cr0.getEQ()) goto loc_821A3F90;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821A3FF8"))) PPC_WEAK_FUNC(sub_821A3FF8);
PPC_FUNC_IMPL(__imp__sub_821A3FF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// lis r4,16384
	ctx.r4.s64 = 1073741824;
loc_821A3FFC:
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r10,r3,4
	ctx.r10.s64 = ctx.r3.s64 + 4;
	// b 0x821a40a4
	goto loc_821A40A4;
loc_821A4008:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// rlwinm r8,r11,29,3,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi. r7,r11,31
	ctx.r7.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// subf r11,r4,r8
	r11.s64 = ctx.r8.s64 - ctx.r4.s64;
	// beq 0x821a4040
	if (cr0.getEQ()) goto loc_821A4040;
	// lhz r8,2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// subfc r8,r7,r8
	xer.ca = ctx.r8.u32 >= ctx.r7.u32;
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// subfe r8,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + xer.ca < xer.ca);
	ctx.r8.u64 = ~ctx.r8.u64 + ctx.r8.u64 + xer.ca;
	xer.ca = temp.u8;
	// clrlwi r8,r8,31
	ctx.r8.u64 = ctx.r8.u32 & 0x1;
	// addi r6,r8,-1
	ctx.r6.s64 = ctx.r8.s64 + -1;
	// b 0x821a4074
	goto loc_821A4074;
loc_821A4040:
	// addi r8,r11,48
	ctx.r8.s64 = r11.s64 + 48;
	// addi r7,r11,16
	ctx.r7.s64 = r11.s64 + 16;
	// addi r6,r11,52
	ctx.r6.s64 = r11.s64 + 52;
	// addi r3,r11,20
	ctx.r3.s64 = r11.s64 + 20;
	// lwbrx r8,0,r8
	ctx.r8.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r8.u32));
	// lwbrx r7,0,r7
	ctx.r7.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r7.u32));
	// lwbrx r6,0,r6
	ctx.r6.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r6.u32));
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// lwbrx r7,0,r3
	ctx.r7.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r3.u32));
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subfic r8,r8,0
	xer.ca = ctx.r8.u32 <= 0;
	ctx.r8.s64 = 0 - ctx.r8.s64;
	// subfe r6,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + xer.ca < xer.ca);
	ctx.r6.u64 = ~ctx.r8.u64 + ctx.r8.u64 + xer.ca;
	xer.ca = temp.u8;
loc_821A4074:
	// rlwinm r8,r9,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// stw r6,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
	// clrlwi r7,r9,3
	ctx.r7.u64 = ctx.r9.u32 & 0x1FFFFFFF;
	// addi r8,r8,512
	ctx.r8.s64 = ctx.r8.s64 + 512;
	// rlwinm r8,r8,0,19,19
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x1000;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r9,r4,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r4.s64;
	// dcbf r0,r9
	// dcbf r0,r11
	// li r9,24
	ctx.r9.s64 = 24;
	// dcbf r9,r11
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
loc_821A40A4:
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// blt cr6,0x821a4008
	if (cr6.getLT()) goto loc_821A4008;
	// lwz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lis r11,-16384
	r11.s64 = -1073741824;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bne cr6,0x821a3ffc
	if (!cr6.getEQ()) goto loc_821A3FFC;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A40C0"))) PPC_WEAK_FUNC(sub_821A40C0);
PPC_FUNC_IMPL(__imp__sub_821A40C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x823ed134
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r30,r3,8
	r30.s64 = ctx.r3.s64 + 8;
	// lis r28,16384
	r28.s64 = 1073741824;
loc_821A40D4:
	// lwz r29,0(r4)
	r29.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r6,r4,4
	ctx.r6.s64 = ctx.r4.s64 + 4;
	// b 0x821a41dc
	goto loc_821A41DC;
loc_821A40E0:
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lwz r3,4(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// rlwinm r11,r10,0,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	// dcbf r0,r11
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a4184
	if (!cr0.getEQ()) goto loc_821A4184;
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addi r7,r31,-1
	ctx.r7.s64 = r31.s64 + -1;
	// lhz r10,6(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// li r8,0
	ctx.r8.s64 = 0;
	// lhz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r4,r9,1
	ctx.r4.s64 = ctx.r9.s64 + 1;
	// lhz r11,4(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// addi r27,r10,1
	r27.s64 = ctx.r10.s64 + 1;
	// rotlwi r9,r5,3
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r5.u32, 3);
	// rotlwi r5,r11,3
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 3);
	// rlwinm r11,r7,4,0,27
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// rlwinm r4,r27,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_821A4138:
	// lwz r27,-4(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplw cr6,r27,r10
	cr6.compare<uint32_t>(r27.u32, ctx.r10.u32, xer);
	// bge cr6,0x821a4170
	if (!cr6.getLT()) goto loc_821A4170;
	// lwz r27,4(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r27,r9
	cr6.compare<uint32_t>(r27.u32, ctx.r9.u32, xer);
	// ble cr6,0x821a4170
	if (!cr6.getGT()) goto loc_821A4170;
	// lwz r27,0(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r27,r4
	cr6.compare<uint32_t>(r27.u32, ctx.r4.u32, xer);
	// bge cr6,0x821a4170
	if (!cr6.getLT()) goto loc_821A4170;
	// lwz r27,8(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r27,r5
	cr6.compare<uint32_t>(r27.u32, ctx.r5.u32, xer);
	// ble cr6,0x821a4170
	if (!cr6.getGT()) goto loc_821A4170;
	// ori r8,r8,3
	ctx.r8.u64 = ctx.r8.u64 | 3;
loc_821A4170:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r11,r11,-16
	r11.s64 = r11.s64 + -16;
	// bne cr6,0x821a4138
	if (!cr6.getEQ()) goto loc_821A4138;
	// b 0x821a41b8
	goto loc_821A41B8;
loc_821A4184:
	// addi r10,r11,48
	ctx.r10.s64 = r11.s64 + 48;
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// addi r8,r11,52
	ctx.r8.s64 = r11.s64 + 52;
	// addi r11,r11,20
	r11.s64 = r11.s64 + 20;
	// lwbrx r10,0,r10
	ctx.r10.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r10.u32));
	// lwbrx r9,0,r9
	ctx.r9.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r9.u32));
	// lwbrx r8,0,r8
	ctx.r8.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r8.u32));
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwbrx r10,0,r11
	ctx.r10.u64 = __builtin_bswap32(PPC_LOAD_U32(r11.u32));
	// subf r11,r8,r9
	r11.s64 = ctx.r9.s64 - ctx.r8.s64;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subfic r11,r11,0
	xer.ca = r11.u32 <= 0;
	r11.s64 = 0 - r11.s64;
	// subfe r8,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	ctx.r8.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
loc_821A41B8:
	// rlwinm r11,r3,12,20,31
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 12) & 0xFFF;
	// stw r8,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r8.u32);
	// clrlwi r10,r3,3
	ctx.r10.u64 = ctx.r3.u32 & 0x1FFFFFFF;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	// dcbf r0,r11
	// addi r6,r6,8
	ctx.r6.s64 = ctx.r6.s64 + 8;
loc_821A41DC:
	// cmplw cr6,r6,r29
	cr6.compare<uint32_t>(ctx.r6.u32, r29.u32, xer);
	// blt cr6,0x821a40e0
	if (cr6.getLT()) goto loc_821A40E0;
	// lwz r4,0(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lis r11,-16384
	r11.s64 = -1073741824;
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// bne cr6,0x821a40d4
	if (!cr6.getEQ()) goto loc_821A40D4;
	// b 0x823ed184
	return;
}

__attribute__((alias("__imp__sub_821A41F8"))) PPC_WEAK_FUNC(sub_821A41F8);
PPC_FUNC_IMPL(__imp__sub_821A41F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x823ed130
	// lwz r27,4(r3)
	r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r29,r3,8
	r29.s64 = ctx.r3.s64 + 8;
loc_821A4208:
	// lwz r28,0(r4)
	r28.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r10,r4,4
	ctx.r10.s64 = ctx.r4.s64 + 4;
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// bge cr6,0x821a42d8
	if (!cr6.getLT()) goto loc_821A42D8;
	// addi r30,r27,-1
	r30.s64 = r27.s64 + -1;
	// addi r11,r10,8
	r11.s64 = ctx.r10.s64 + 8;
loc_821A4220:
	// lhz r8,-2(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + -2);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// lhz r5,-4(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + -4);
	// li r6,0
	ctx.r6.s64 = 0;
	// lhz r9,2(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addi r3,r8,1
	ctx.r3.s64 = ctx.r8.s64 + 1;
	// lhz r31,0(r11)
	r31.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rotlwi r4,r5,3
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r5.u32, 3);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r5,r3,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// rotlwi r3,r31,3
	ctx.r3.u64 = __builtin_rotateleft32(r31.u32, 3);
	// rlwinm r31,r9,3,0,28
	r31.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r7,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + r29.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
loc_821A4260:
	// lwz r26,-4(r9)
	r26.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplw cr6,r26,r5
	cr6.compare<uint32_t>(r26.u32, ctx.r5.u32, xer);
	// bge cr6,0x821a4298
	if (!cr6.getLT()) goto loc_821A4298;
	// lwz r26,4(r9)
	r26.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmplw cr6,r26,r4
	cr6.compare<uint32_t>(r26.u32, ctx.r4.u32, xer);
	// ble cr6,0x821a4298
	if (!cr6.getGT()) goto loc_821A4298;
	// lwz r26,0(r9)
	r26.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r26,r31
	cr6.compare<uint32_t>(r26.u32, r31.u32, xer);
	// bge cr6,0x821a4298
	if (!cr6.getLT()) goto loc_821A4298;
	// lwz r26,8(r9)
	r26.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplw cr6,r26,r3
	cr6.compare<uint32_t>(r26.u32, ctx.r3.u32, xer);
	// ble cr6,0x821a4298
	if (!cr6.getGT()) goto loc_821A4298;
	// ori r6,r6,3
	ctx.r6.u64 = ctx.r6.u64 | 3;
loc_821A4298:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// addi r9,r9,-16
	ctx.r9.s64 = ctx.r9.s64 + -16;
	// bne cr6,0x821a4260
	if (!cr6.getEQ()) goto loc_821A4260;
	// rlwinm r9,r8,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFF;
	// stw r6,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r6.u32);
	// clrlwi r7,r8,3
	ctx.r7.u64 = ctx.r8.u32 & 0x1FFFFFFF;
	// addi r9,r9,512
	ctx.r9.s64 = ctx.r9.s64 + 512;
	// rlwinm r9,r9,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1000;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// addis r9,r9,-16384
	ctx.r9.s64 = ctx.r9.s64 + -1073741824;
	// dcbf r0,r9
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// blt cr6,0x821a4220
	if (cr6.getLT()) goto loc_821A4220;
loc_821A42D8:
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lis r11,-16384
	r11.s64 = -1073741824;
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// bne cr6,0x821a4208
	if (!cr6.getEQ()) goto loc_821A4208;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821A42F0"))) PPC_WEAK_FUNC(sub_821A42F0);
PPC_FUNC_IMPL(__imp__sub_821A42F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed12c
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lwz r26,0(r11)
	r26.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r11,r10,0,0,0
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a4618
	if (cr0.getEQ()) goto loc_821A4618;
	// lis r25,-30976
	r25.s64 = -2030043136;
loc_821A4320:
	// rlwinm r11,r10,0,0,7
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFF000000;
	// cmplw cr6,r11,r25
	cr6.compare<uint32_t>(r11.u32, r25.u32, xer);
	// bgt cr6,0x821a44bc
	if (cr6.getGT()) goto loc_821A44BC;
	// beq cr6,0x821a448c
	if (cr6.getEQ()) goto loc_821A448C;
	// lis r9,-32768
	ctx.r9.s64 = -2147483648;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a4458
	if (cr6.getEQ()) goto loc_821A4458;
	// lis r9,-32512
	ctx.r9.s64 = -2130706432;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a4430
	if (cr6.getEQ()) goto loc_821A4430;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a460c
	if (cr6.getEQ()) goto loc_821A460C;
	// lis r9,-32000
	ctx.r9.s64 = -2097152000;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a43ec
	if (cr6.getEQ()) goto loc_821A43EC;
	// lis r9,-31744
	ctx.r9.s64 = -2080374784;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a43cc
	if (cr6.getEQ()) goto loc_821A43CC;
	// lis r9,-31488
	ctx.r9.s64 = -2063597568;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a439c
	if (cr6.getEQ()) goto loc_821A439C;
	// lis r9,-31232
	ctx.r9.s64 = -2046820352;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x821a4510
	if (!cr6.getEQ()) goto loc_821A4510;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// oris r11,r11,8192
	r11.u64 = r11.u64 | 536870912;
	// stw r10,364(r30)
	PPC_STORE_U32(r30.u32 + 364, ctx.r10.u32);
	// b 0x821a4420
	goto loc_821A4420;
loc_821A439C:
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// stw r10,360(r30)
	PPC_STORE_U32(r30.u32 + 360, ctx.r10.u32);
	// rlwinm r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwimi r11,r10,26,0,0
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 26) & 0x80000000) | (r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// rlwinm. r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a4424
	if (cr0.getEQ()) goto loc_821A4424;
	// lwz r31,356(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 356);
	// b 0x821a460c
	goto loc_821A460C;
loc_821A43CC:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// rlwinm r11,r11,0,2,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// rlwinm. r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a4424
	if (cr0.getEQ()) goto loc_821A4424;
	// lwz r31,360(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 360);
	// b 0x821a460c
	goto loc_821A460C;
loc_821A43EC:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// lwz r9,44(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 44);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// oris r11,r11,16384
	r11.u64 = r11.u64 | 1073741824;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r10,356(r30)
	PPC_STORE_U32(r30.u32 + 356, ctx.r10.u32);
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// ble cr6,0x821a4424
	if (!cr6.getGT()) goto loc_821A4424;
	// lwz r10,100(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 100);
	// rlwinm. r10,r10,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a4424
	if (cr0.getEQ()) goto loc_821A4424;
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
loc_821A4420:
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_821A4424:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821a3f10
	sub_821A3F10(ctx, base);
	// b 0x821a460c
	goto loc_821A460C;
loc_821A4430:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// clrlwi r11,r10,8
	r11.u64 = ctx.r10.u32 & 0xFFFFFF;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x8219caf8
	sub_8219CAF8(ctx, base);
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// b 0x821a460c
	goto loc_821A460C;
loc_821A4458:
	// addi r4,r31,4
	ctx.r4.s64 = r31.s64 + 4;
	// addi r3,r30,100
	ctx.r3.s64 = r30.s64 + 100;
	// li r5,248
	ctx.r5.s64 = 248;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// lis r10,32767
	ctx.r10.s64 = 2147418112;
	// li r11,0
	r11.s64 = 0;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// li r9,-1
	ctx.r9.s64 = -1;
	// addi r31,r31,252
	r31.s64 = r31.s64 + 252;
	// stw r11,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r11.u32);
	// stw r10,348(r30)
	PPC_STORE_U32(r30.u32 + 348, ctx.r10.u32);
	// stw r9,352(r30)
	PPC_STORE_U32(r30.u32 + 352, ctx.r9.u32);
	// b 0x821a460c
	goto loc_821A460C;
loc_821A448C:
	// lwz r11,44(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 44);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 40);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r9,104(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 104);
	// rlwinm r10,r10,0,3,1
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFDFFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// stw r11,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r11.u32);
	// stw r10,40(r30)
	PPC_STORE_U32(r30.u32 + 40, ctx.r10.u32);
	// bge cr6,0x821a4424
	if (!cr6.getLT()) goto loc_821A4424;
	// lwz r31,364(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 364);
	// b 0x821a460c
	goto loc_821A460C;
loc_821A44BC:
	// lis r9,-30720
	ctx.r9.s64 = -2013265920;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a45ec
	if (cr6.getEQ()) goto loc_821A45EC;
	// lis r9,-30464
	ctx.r9.s64 = -1996488704;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a45c8
	if (cr6.getEQ()) goto loc_821A45C8;
	// lis r9,-30208
	ctx.r9.s64 = -1979711488;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a4568
	if (cr6.getEQ()) goto loc_821A4568;
	// lis r9,-29952
	ctx.r9.s64 = -1962934272;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a4560
	if (cr6.getEQ()) goto loc_821A4560;
	// lis r9,-29696
	ctx.r9.s64 = -1946157056;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a4618
	if (cr6.getEQ()) goto loc_821A4618;
	// lis r9,-29440
	ctx.r9.s64 = -1929379840;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a4618
	if (cr6.getEQ()) goto loc_821A4618;
	// lis r9,-29184
	ctx.r9.s64 = -1912602624;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x821a4524
	if (cr6.getEQ()) goto loc_821A4524;
loc_821A4510:
	// lis r11,-16384
	r11.s64 = -1073741824;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x821a4618
	if (cr6.getEQ()) goto loc_821A4618;
	// addi r31,r10,4
	r31.s64 = ctx.r10.s64 + 4;
	// b 0x821a460c
	goto loc_821A460C;
loc_821A4524:
	// lwz r11,348(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 348);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a4550
	if (cr0.getEQ()) goto loc_821A4550;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x8219caf8
	sub_8219CAF8(ctx, base);
loc_821A4550:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// stw r11,352(r30)
	PPC_STORE_U32(r30.u32 + 352, r11.u32);
	// b 0x821a460c
	goto loc_821A460C;
loc_821A4560:
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// b 0x821a460c
	goto loc_821A460C;
loc_821A4568:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r29,4(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r28,8(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r27,12(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r5,16(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// bl 0x821a3d70
	sub_821A3D70(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x821a3ff8
	sub_821A3FF8(ctx, base);
	// lwz r11,348(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 348);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a45a8
	if (cr0.getEQ()) goto loc_821A45A8;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r30,100
	ctx.r3.s64 = r30.s64 + 100;
	// bl 0x821a40c0
	sub_821A40C0(ctx, base);
	// b 0x821a45c0
	goto loc_821A45C0;
loc_821A45A8:
	// lwz r11,44(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 44);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x821a45c0
	if (!cr6.getEQ()) goto loc_821A45C0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// addi r3,r30,100
	ctx.r3.s64 = r30.s64 + 100;
	// bl 0x821a41f8
	sub_821A41F8(ctx, base);
loc_821A45C0:
	// addi r31,r31,20
	r31.s64 = r31.s64 + 20;
	// b 0x821a460c
	goto loc_821A460C;
loc_821A45C8:
	// lwz r11,348(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 348);
	// rlwinm. r11,r11,0,0,0
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a45e4
	if (cr0.getEQ()) goto loc_821A45E4;
	// addi r3,r30,100
	ctx.r3.s64 = r30.s64 + 100;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x821a3d10
	sub_821A3D10(ctx, base);
loc_821A45E4:
	// addi r31,r31,12
	r31.s64 = r31.s64 + 12;
	// b 0x821a460c
	goto loc_821A460C;
loc_821A45EC:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// lwz r10,348(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 348);
	// and. r11,r10,r11
	r11.u64 = ctx.r10.u64 & r11.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a460c
	if (cr0.getEQ()) goto loc_821A460C;
	// mr r11,r31
	r11.u64 = r31.u64;
	// addi r31,r30,76
	r31.s64 = r30.s64 + 76;
	// stw r11,64(r30)
	PPC_STORE_U32(r30.u32 + 64, r11.u32);
loc_821A460C:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm. r11,r10,0,0,0
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a4320
	if (!cr0.getEQ()) goto loc_821A4320;
loc_821A4618:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed17c
	return;
}

__attribute__((alias("__imp__sub_821A4628"))) PPC_WEAK_FUNC(sub_821A4628);
PPC_FUNC_IMPL(__imp__sub_821A4628) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed134
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// addi r27,r31,60
	r27.s64 = r31.s64 + 60;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8240f8dc
	__imp__KfAcquireSpinLock(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,-2
	ctx.r3.s64 = -2;
	// lwz r29,0(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// lis r11,-16384
	r11.s64 = -1073741824;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bne cr6,0x821a4738
	if (!cr6.getEQ()) goto loc_821A4738;
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
	// lis r7,256
	ctx.r7.s64 = 16777216;
	// lwz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// slw r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r10.u8 & 0x3F));
	// andc r11,r11,r10
	r11.u64 = r11.u64 & ~ctx.r10.u64;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r11.u32);
	// bne 0x821a46f8
	if (!cr0.getEQ()) goto loc_821A46F8;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r11,11376
	r11.s64 = r11.s64 + 11376;
loc_821A46A8:
	// lwz r8,48(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// slw r9,r7,r10
	ctx.r9.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r10.u8 & 0x3F));
	// and. r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 & ctx.r9.u64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821a46d0
	if (cr0.getEQ()) goto loc_821A46D0;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// lwz r8,48(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// andc r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 & ~ctx.r9.u64;
	// stw r9,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r9.u32);
loc_821A46D0:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,80
	r11.s64 = r11.s64 + 80;
	// cmplwi cr6,r10,6
	cr6.compare<uint32_t>(ctx.r10.u32, 6, xer);
	// blt cr6,0x821a46a8
	if (cr6.getLT()) goto loc_821A46A8;
	// lwz r11,64(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a46f8
	if (!cr6.getEQ()) goto loc_821A46F8;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
loc_821A46F8:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8240f8cc
	__imp__KfReleaseSpinLock(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a3f70
	sub_821A3F70(ctx, base);
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821a471c
	if (cr6.getEQ()) goto loc_821A471C;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8240fabc
	__imp__KeUnlockL2(ctx, base);
loc_821A471C:
	// lwsync 
	// li r11,0
	r11.s64 = 0;
	// li r3,-1
	ctx.r3.s64 = -1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x821a4744
	goto loc_821A4744;
loc_821A4738:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8240f8cc
	__imp__KfReleaseSpinLock(ctx, base);
	// addi r3,r29,4
	ctx.r3.s64 = r29.s64 + 4;
loc_821A4744:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed184
	return;
}

__attribute__((alias("__imp__sub_821A4750"))) PPC_WEAK_FUNC(sub_821A4750);
PPC_FUNC_IMPL(__imp__sub_821A4750) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed134
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lis r3,-17477
	ctx.r3.s64 = -1145372672;
	// ori r3,r3,48059
	ctx.r3.u64 = ctx.r3.u64 | 48059;
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// lis r29,256
	r29.s64 = 16777216;
	// b 0x821a4790
	goto loc_821A4790;
loc_821A4778:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// slw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 << (r11.u8 & 0x3F));
	// and. r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a479c
	if (!cr0.getEQ()) goto loc_821A479C;
	// db16cyc 
loc_821A4790:
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a4778
	if (!cr6.getEQ()) goto loc_821A4778;
loc_821A479C:
	// li r28,0
	r28.s64 = 0;
	// li r27,1
	r27.s64 = 1;
loc_821A47A4:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r31
	reserved.u32 = *(uint32_t*)(base + r31.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// bne cr6,0x821a47c8
	if (!cr6.getEQ()) goto loc_821A47C8;
	// stwcx. r27,0,r31
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r31.u32), reserved.s32, __builtin_bswap32(r27.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r10,1
	// bne 0x821a47a4
	if (!cr0.getEQ()) goto loc_821A47A4;
	// b 0x821a47d0
	goto loc_821A47D0;
loc_821A47C8:
	// stwcx. r11,0,r31
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r31.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r10,1
loc_821A47D0:
	// mr r11,r11
	r11.u64 = r11.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a47a4
	if (!cr6.getEQ()) goto loc_821A47A4;
	// lwsync 
	// lwz r11,60(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 60);
	// lwz r10,56(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// lis r3,-17477
	ctx.r3.s64 = -1145372672;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a480c
	if (!cr6.getEQ()) goto loc_821A480C;
	// ori r3,r3,48060
	ctx.r3.u64 = ctx.r3.u64 | 48060;
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// lwsync 
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
loc_821A4804:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed184
	return;
loc_821A480C:
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// ori r3,r3,48061
	ctx.r3.u64 = ctx.r3.u64 | 48061;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r11.u32);
	// lwz r11,60(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 60);
	// clrlwi r10,r11,30
	ctx.r10.u64 = r11.u32 & 0x3;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r4,r10,r30
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// stw r11,60(r30)
	PPC_STORE_U32(r30.u32 + 60, r11.u32);
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// lwz r3,52(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// lwz r11,56(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 56);
	// lwz r10,60(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 60);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// clrlwi r3,r11,30
	ctx.r3.u64 = r11.u32 & 0x3;
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// ble cr6,0x821a4874
	if (!cr6.getGT()) goto loc_821A4874;
	// lwz r5,36(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// b 0x821a48a0
	goto loc_821A48A0;
loc_821A4874:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r5,64(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// stw r28,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r28.u32);
	// cmplwi r5,0
	cr0.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// slw r11,r29,r11
	r11.u64 = r11.u8 & 0x20 ? 0 : (r29.u32 << (r11.u8 & 0x3F));
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// beq 0x821a4898
	if (cr0.getEQ()) goto loc_821A4898;
	// stw r28,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r28.u32);
	// b 0x821a48a0
	goto loc_821A48A0;
loc_821A4898:
	// addi r5,r4,4
	ctx.r5.s64 = ctx.r4.s64 + 4;
	// stw r4,68(r31)
	PPC_STORE_U32(r31.u32 + 68, ctx.r4.u32);
loc_821A48A0:
	// lis r29,-16384
	r29.s64 = -1073741824;
loc_821A48A4:
	// lwz r6,24(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplw cr6,r6,r11
	cr6.compare<uint32_t>(ctx.r6.u32, r11.u32, xer);
	// bge cr6,0x821a48cc
	if (!cr6.getLT()) goto loc_821A48CC;
	// addi r11,r6,1
	r11.s64 = ctx.r6.s64 + 1;
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r5,32(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// lwsync 
	// b 0x821a491c
	goto loc_821A491C;
loc_821A48CC:
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwinm. r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a4938
	if (!cr0.getEQ()) goto loc_821A4938;
	// lwz r9,352(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// lwz r8,348(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 348);
	// rlwinm r10,r11,16,17,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0x7FFF;
	// and. r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 & ctx.r8.u64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x821a48f8
	if (!cr0.getEQ()) goto loc_821A48F8;
	// add r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 + ctx.r5.u64;
	// b 0x821a48a4
	goto loc_821A48A4;
loc_821A48F8:
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// stw r27,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r27.u32);
	// stw r5,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r5.u32);
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// stw r10,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r10.u32);
	// lwsync 
	// li r6,0
	ctx.r6.s64 = 0;
loc_821A491C:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r5,36(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// b 0x821a48a4
	goto loc_821A48A4;
loc_821A4938:
	// lis r10,-29440
	ctx.r10.s64 = -1929379840;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a4954
	if (!cr6.getEQ()) goto loc_821A4954;
	// lwz r11,4(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// stw r11,352(r31)
	PPC_STORE_U32(r31.u32 + 352, r11.u32);
	// b 0x821a48a4
	goto loc_821A48A4;
loc_821A4954:
	// lis r10,-29696
	ctx.r10.s64 = -1946157056;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a4978
	if (!cr6.getEQ()) goto loc_821A4978;
	// lwz r11,4(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// lwz r11,8(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// addi r5,r5,12
	ctx.r5.s64 = ctx.r5.s64 + 12;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// b 0x821a48a4
	goto loc_821A48A4;
loc_821A4978:
	// rlwinm r10,r11,0,0,1
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xC0000000;
	// cmplw cr6,r10,r29
	cr6.compare<uint32_t>(ctx.r10.u32, r29.u32, xer);
	// bne cr6,0x821a49ac
	if (!cr6.getEQ()) goto loc_821A49AC;
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// bne cr6,0x821a49a4
	if (!cr6.getEQ()) goto loc_821A49A4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a4628
	sub_821A4628(ctx, base);
	// mr. r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne 0x821a48a4
	if (!cr0.getEQ()) goto loc_821A48A4;
	// b 0x821a4804
	goto loc_821A4804;
loc_821A49A4:
	// addi r5,r11,4
	ctx.r5.s64 = r11.s64 + 4;
	// b 0x821a48a4
	goto loc_821A48A4;
loc_821A49AC:
	// lis r3,-12288
	ctx.r3.s64 = -805306368;
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// lwz r11,92(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// stw r27,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r27.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lwz r10,52(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,92(r31)
	PPC_STORE_U32(r31.u32 + 92, r11.u32);
	// beq cr6,0x821a4a34
	if (cr6.getEQ()) goto loc_821A4A34;
	// stw r5,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r5.u32);
	// lwsync 
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// b 0x821a49e4
	goto loc_821A49E4;
loc_821A49E0:
	// db16cyc 
loc_821A49E4:
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a49e0
	if (!cr6.getEQ()) goto loc_821A49E0;
loc_821A49F0:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r31
	reserved.u32 = *(uint32_t*)(base + r31.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// cmpw cr6,r11,r28
	cr6.compare<int32_t>(r11.s32, r28.s32, xer);
	// bne cr6,0x821a4a14
	if (!cr6.getEQ()) goto loc_821A4A14;
	// stwcx. r27,0,r31
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r31.u32), reserved.s32, __builtin_bswap32(r27.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r10,1
	// bne 0x821a49f0
	if (!cr0.getEQ()) goto loc_821A49F0;
	// b 0x821a4a1c
	goto loc_821A4A1C;
loc_821A4A14:
	// stwcx. r11,0,r31
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r31.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r10,1
loc_821A4A1C:
	// mr r11,r11
	r11.u64 = r11.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a49f0
	if (!cr6.getEQ()) goto loc_821A49F0;
	// lwsync 
	// lwz r5,36(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// b 0x821a4a48
	goto loc_821A4A48;
loc_821A4A34:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// bl 0x821a42f0
	sub_821A42F0(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// stw r28,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r28.u32);
loc_821A4A48:
	// lwz r11,92(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// lis r3,-12288
	ctx.r3.s64 = -805306368;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// ori r3,r3,1
	ctx.r3.u64 = ctx.r3.u64 | 1;
	// stw r11,92(r31)
	PPC_STORE_U32(r31.u32 + 92, r11.u32);
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// b 0x821a48a4
	goto loc_821A48A4;
}

__attribute__((alias("__imp__sub_821A4A68"))) PPC_WEAK_FUNC(sub_821A4A68);
PPC_FUNC_IMPL(__imp__sub_821A4A68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed12c
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r3,-2
	ctx.r3.s64 = -2;
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// bl 0x8235dd40
	sub_8235DD40(ctx, base);
	// lis r25,-32256
	r25.s64 = -2113929216;
loc_821A4A88:
	// lis r11,-5
	r11.s64 = -327680;
	// lwz r26,0(r28)
	r26.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r27,r1,80
	r27.s64 = ctx.r1.s64 + 80;
	// ori r11,r11,27680
	r11.u64 = r11.u64 | 27680;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r10,368(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 368);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x821a4ab0
	if (cr6.getEQ()) goto loc_821A4AB0;
	// li r27,0
	r27.s64 = 0;
loc_821A4AB0:
	// lwz r11,60(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 60);
	// lwz r10,56(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a4b28
	if (!cr6.getEQ()) goto loc_821A4B28;
	// addi r29,r28,32
	r29.s64 = r28.s64 + 32;
	// b 0x821a4b08
	goto loc_821A4B08;
loc_821A4AC8:
	// lwz r11,1768(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1768);
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r30,r31,14928
	r30.s64 = r31.s64 + 14928;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8240f8bc
	__imp__RtlEnterCriticalSection(ctx, base);
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// rlwinm. r11,r11,0,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a4b00
	if (cr0.getEQ()) goto loc_821A4B00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821996f0
	sub_821996F0(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r31,14828
	ctx.r4.s64 = r31.s64 + 14828;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821996f8
	sub_821996F8(ctx, base);
loc_821A4B00:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8240f8ac
	__imp__RtlLeaveCriticalSection(ctx, base);
loc_821A4B08:
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8240fadc
	__imp__KeWaitForSingleObject(ctx, base);
	// cmplwi cr6,r3,258
	cr6.compare<uint32_t>(ctx.r3.u32, 258, xer);
	// beq cr6,0x821a4ac8
	if (cr6.getEQ()) goto loc_821A4AC8;
loc_821A4B28:
	// addi r3,r28,32
	ctx.r3.s64 = r28.s64 + 32;
	// bl 0x8240facc
	__imp__KeResetEvent(ctx, base);
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a4b48
	if (cr6.getEQ()) goto loc_821A4B48;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x821a4750
	sub_821A4750(ctx, base);
	// b 0x821a4a88
	goto loc_821A4A88;
loc_821A4B48:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed17c
	return;
}

__attribute__((alias("__imp__sub_821A4B58"))) PPC_WEAK_FUNC(sub_821A4B58);
PPC_FUNC_IMPL(__imp__sub_821A4B58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lbz r10,268(r13)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r13.u32 + 268);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mulli r10,r10,80
	ctx.r10.s64 = ctx.r10.s64 * 80;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lis r3,-21846
	ctx.r3.s64 = -1431699456;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// ori r3,r3,43690
	ctx.r3.u64 = ctx.r3.u64 | 43690;
	// addi r31,r11,11316
	r31.s64 = r11.s64 + 11316;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r5,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r5.u32);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// bl 0x821a3a40
	sub_821A3A40(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,32
	ctx.r3.s64 = r31.s64 + 32;
	// bl 0x8240faec
	__imp__KeSetEvent(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A4BE0"))) PPC_WEAK_FUNC(sub_821A4BE0);
PPC_FUNC_IMPL(__imp__sub_821A4BE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219c2c0
	sub_8219C2C0(ctx, base);
	// lwz r11,13228(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13228);
	// lwz r8,13224(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 13224);
	// addi r30,r31,13352
	r30.s64 = r31.s64 + 13352;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r31,13372
	ctx.r10.s64 = r31.s64 + 13372;
	// li r9,5
	ctx.r9.s64 = 5;
	// stw r8,13216(r31)
	PPC_STORE_U32(r31.u32 + 13216, ctx.r8.u32);
	// stw r11,13220(r31)
	PPC_STORE_U32(r31.u32 + 13220, r11.u32);
	// mr r11,r30
	r11.u64 = r30.u64;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_821A4C2C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x821a4c2c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_821A4C2C;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d530
	sub_8219D530(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r31,13232
	ctx.r3.s64 = r31.s64 + 13232;
	// bl 0x8219d530
	sub_8219D530(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r31,13332
	ctx.r3.s64 = r31.s64 + 13332;
	// bl 0x8219d530
	sub_8219D530(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r31,13252
	ctx.r3.s64 = r31.s64 + 13252;
	// bl 0x8219d530
	sub_8219D530(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r31,13272
	ctx.r3.s64 = r31.s64 + 13272;
	// bl 0x8219d530
	sub_8219D530(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r31,13292
	ctx.r3.s64 = r31.s64 + 13292;
	// bl 0x8219d530
	sub_8219D530(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r31,13312
	ctx.r3.s64 = r31.s64 + 13312;
	// bl 0x8219d530
	sub_8219D530(ctx, base);
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r3,252
	ctx.r10.s64 = ctx.r3.s64 + 252;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x821a4cb0
	if (!cr6.getGT()) goto loc_821A4CB0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
loc_821A4CB0:
	// lis r11,-32768
	r11.s64 = -2147483648;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// lwz r11,13180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13180);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// lwz r11,12740(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// lwz r11,12740(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// lwz r8,12984(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 12984);
	// lwz r7,12988(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 12988);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x821a4d30
	if (!cr6.getGT()) goto loc_821A4D30;
	// addi r10,r3,16
	ctx.r10.s64 = ctx.r3.s64 + 16;
	// addi r11,r31,12748
	r11.s64 = r31.s64 + 12748;
loc_821A4CE8:
	// lwz r6,-4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// subf r6,r8,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r8.s64;
	// stw r6,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r6.u32);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subf r6,r7,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r7.s64;
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// subf r6,r8,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r8.s64;
	// stw r6,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r6.u32);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// subf r6,r7,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r7.s64;
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// lwz r6,12740(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// cmplw cr6,r9,r6
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, xer);
	// blt cr6,0x821a4ce8
	if (cr6.getLT()) goto loc_821A4CE8;
loc_821A4D30:
	// addi r11,r3,252
	r11.s64 = ctx.r3.s64 + 252;
	// stw r11,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A4D50"))) PPC_WEAK_FUNC(sub_821A4D50);
PPC_FUNC_IMPL(__imp__sub_821A4D50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a4d7c
	if (!cr0.getEQ()) goto loc_821A4D7C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// bl 0x8219cce0
	sub_8219CCE0(ctx, base);
loc_821A4D7C:
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,100
	ctx.r4.s64 = 100;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219ca10
	sub_8219CA10(ctx, base);
	// li r26,0
	r26.s64 = 0;
	// mr. r29,r3
	r29.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x821a4de4
	if (cr0.getEQ()) goto loc_821A4DE4;
	// lwz r11,13352(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13352);
	// addi r4,r29,-4
	ctx.r4.s64 = r29.s64 + -4;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821a4db0
	if (!cr0.getEQ()) goto loc_821A4DB0;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// b 0x821a4dc8
	goto loc_821A4DC8;
loc_821A4DB0:
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addis r7,r11,-16384
	ctx.r7.s64 = r11.s64 + -1073741824;
loc_821A4DC8:
	// lis r11,-32230
	r11.s64 = -2112225280;
	// ori r5,r30,1
	ctx.r5.u64 = r30.u64 | 1;
	// addi r6,r11,19288
	ctx.r6.s64 = r11.s64 + 19288;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d128
	sub_8219D128(ctx, base);
	// addi r28,r3,4
	r28.s64 = ctx.r3.s64 + 4;
	// b 0x821a4de8
	goto loc_821A4DE8;
loc_821A4DE4:
	// lwz r28,84(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_821A4DE8:
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219ca10
	sub_8219CA10(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bne 0x821a4e08
	if (!cr0.getEQ()) goto loc_821A4E08;
	// lwz r30,84(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_821A4E08:
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// stw r26,13216(r31)
	PPC_STORE_U32(r31.u32 + 13216, r26.u32);
	// stw r26,13220(r31)
	PPC_STORE_U32(r31.u32 + 13220, r26.u32);
	// rlwinm. r27,r11,27,31,31
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// beq 0x821a4e34
	if (cr0.getEQ()) goto loc_821A4E34;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219c2f8
	sub_8219C2F8(ctx, base);
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// andi. r11,r11,223
	r11.u64 = r11.u64 & 223;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stb r11,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, r11.u8);
	// b 0x821a4edc
	goto loc_821A4EDC;
loc_821A4E34:
	// addi r3,r31,13352
	ctx.r3.s64 = r31.s64 + 13352;
	// bl 0x8219c6f0
	sub_8219C6F0(ctx, base);
	// addi r3,r31,13232
	ctx.r3.s64 = r31.s64 + 13232;
	// bl 0x8219c6f0
	sub_8219C6F0(ctx, base);
	// addi r3,r31,13332
	ctx.r3.s64 = r31.s64 + 13332;
	// bl 0x8219c6f0
	sub_8219C6F0(ctx, base);
	// addi r3,r31,13252
	ctx.r3.s64 = r31.s64 + 13252;
	// bl 0x8219c6f0
	sub_8219C6F0(ctx, base);
	// addi r3,r31,13272
	ctx.r3.s64 = r31.s64 + 13272;
	// bl 0x8219c6f0
	sub_8219C6F0(ctx, base);
	// addi r3,r31,13292
	ctx.r3.s64 = r31.s64 + 13292;
	// bl 0x8219c6f0
	sub_8219C6F0(ctx, base);
	// addi r3,r31,13312
	ctx.r3.s64 = r31.s64 + 13312;
	// bl 0x8219c6f0
	sub_8219C6F0(ctx, base);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x821a4e94
	if (cr6.getEQ()) goto loc_821A4E94;
	// addi r8,r31,13372
	ctx.r8.s64 = r31.s64 + 13372;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219cf88
	sub_8219CF88(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_821A4E94:
	// lwz r11,21516(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21516);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a4eac
	if (!cr6.getEQ()) goto loc_821A4EAC;
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// rlwinm. r11,r11,0,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a4edc
	if (!cr0.getEQ()) goto loc_821A4EDC;
loc_821A4EAC:
	// rlwinm r11,r29,12,20,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 12) & 0xFFF;
	// subf r9,r29,r28
	ctx.r9.s64 = r28.s64 - r29.s64;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// clrlwi r10,r29,3
	ctx.r10.u64 = r29.u32 & 0x1FFFFFFF;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// addi r8,r31,13372
	ctx.r8.s64 = r31.s64 + 13372;
	// li r7,1
	ctx.r7.s64 = 1;
	// srawi r6,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r9.s32 >> 2;
	// add r5,r11,r10
	ctx.r5.u64 = r11.u64 + ctx.r10.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219cf88
	sub_8219CF88(ctx, base);
loc_821A4EDC:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821A4EE8"))) PPC_WEAK_FUNC(sub_821A4EE8);
PPC_FUNC_IMPL(__imp__sub_821A4EE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed12c
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,22280(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 22280);
	// rlwinm. r11,r11,0,23,23
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a502c
	if (!cr0.getEQ()) goto loc_821A502C;
	// lis r4,-19072
	ctx.r4.s64 = -1249902592;
	// li r3,200
	ctx.r3.s64 = 200;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,11796(r30)
	PPC_STORE_U32(r30.u32 + 11796, ctx.r3.u32);
	// bne 0x821a4f24
	if (!cr0.getEQ()) goto loc_821A4F24;
loc_821A4F1C:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x821a5030
	goto loc_821A5030;
loc_821A4F24:
	// lwz r8,22280(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 22280);
	// li r27,0
	r27.s64 = 0;
	// lwz r11,14880(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 14880);
	// lis r9,-16384
	ctx.r9.s64 = -1073741824;
	// lwz r10,14884(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 14884);
	// li r26,-1
	r26.s64 = -1;
	// stw r27,11004(r30)
	PPC_STORE_U32(r30.u32 + 11004, r27.u32);
	// stw r9,11020(r30)
	PPC_STORE_U32(r30.u32 + 11020, ctx.r9.u32);
	// stw r11,10948(r30)
	PPC_STORE_U32(r30.u32 + 10948, r11.u32);
	// stw r10,10956(r30)
	PPC_STORE_U32(r30.u32 + 10956, ctx.r10.u32);
	// rlwinm. r8,r8,0,5,5
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x4000000;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821a4f5c
	if (cr0.getEQ()) goto loc_821A4F5C;
	// li r26,2
	r26.s64 = 2;
	// stw r26,11312(r30)
	PPC_STORE_U32(r30.u32 + 11312, r26.u32);
loc_821A4F5C:
	// mr r29,r27
	r29.u64 = r27.u64;
	// lis r28,32512
	r28.s64 = 2130706432;
	// addi r31,r30,11356
	r31.s64 = r30.s64 + 11356;
	// lis r25,-32256
	r25.s64 = -2113929216;
loc_821A4F6C:
	// lis r11,256
	r11.s64 = 16777216;
	// lwz r10,22280(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 22280);
	// slw r11,r11,r29
	r11.u64 = r29.u8 & 0x20 ? 0 : (r11.u32 << (r29.u8 & 0x3F));
	// and. r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a5010
	if (cr0.getEQ()) goto loc_821A5010;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// bge cr6,0x821a4f90
	if (!cr6.getLT()) goto loc_821A4F90;
	// mr r26,r29
	r26.u64 = r29.u64;
	// stw r29,11312(r30)
	PPC_STORE_U32(r30.u32 + 11312, r29.u32);
loc_821A4F90:
	// addi r11,r30,10944
	r11.s64 = r30.s64 + 10944;
	// stw r29,-36(r31)
	PPC_STORE_U32(r31.u32 + -36, r29.u32);
	// addis r10,r28,16640
	ctx.r10.s64 = r28.s64 + 1090519040;
	// stw r28,-12(r31)
	PPC_STORE_U32(r31.u32 + -12, r28.u32);
	// addi r6,r31,-40
	ctx.r6.s64 = r31.s64 + -40;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// addi r8,r31,12
	ctx.r8.s64 = r31.s64 + 12;
	// stw r11,-40(r31)
	PPC_STORE_U32(r31.u32 + -40, r11.u32);
	// lis r11,-32230
	r11.s64 = -2112225280;
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r5,r11,19048
	ctx.r5.s64 = r11.s64 + 19048;
	// stw r10,-16(r31)
	PPC_STORE_U32(r31.u32 + -16, ctx.r10.u32);
	// lis r4,1
	ctx.r4.s64 = 65536;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r27,-8(r31)
	PPC_STORE_U8(r31.u32 + -8, r27.u8);
	// stw r27,-4(r31)
	PPC_STORE_U32(r31.u32 + -4, r27.u32);
	// stw r31,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r31.u32);
	// stw r31,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r31.u32);
	// bl 0x8235eb98
	sub_8235EB98(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r3.u32);
	// beq 0x821a4f1c
	if (cr0.getEQ()) goto loc_821A4F1C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r4,1596(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + 1596);
	// bl 0x8240fb1c
	__imp__ObReferenceObjectByHandle(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821a5010
	if (cr0.getLT()) goto loc_821A5010;
	// li r4,17
	ctx.r4.s64 = 17;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8240fb0c
	__imp__KeSetBasePriorityThread(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8240fafc
	__imp__ObDereferenceObject(ctx, base);
loc_821A5010:
	// lis r11,32512
	r11.s64 = 2130706432;
	// addi r28,r28,128
	r28.s64 = r28.s64 + 128;
	// ori r11,r11,768
	r11.u64 = r11.u64 | 768;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,80
	r31.s64 = r31.s64 + 80;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// blt cr6,0x821a4f6c
	if (cr6.getLT()) goto loc_821A4F6C;
loc_821A502C:
	// li r3,1
	ctx.r3.s64 = 1;
loc_821A5030:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed17c
	return;
}

__attribute__((alias("__imp__sub_821A5038"))) PPC_WEAK_FUNC(sub_821A5038);
PPC_FUNC_IMPL(__imp__sub_821A5038) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// lis r4,-20096
	ctx.r4.s64 = -1317011456;
	// lwz r3,11796(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 11796);
	// stw r11,10948(r31)
	PPC_STORE_U32(r31.u32 + 10948, r11.u32);
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// addi r30,r31,11348
	r30.s64 = r31.s64 + 11348;
	// li r29,6
	r29.s64 = 6;
loc_821A5064:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a5080
	if (cr6.getEQ()) goto loc_821A5080;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8240faec
	__imp__KeSetEvent(ctx, base);
loc_821A5080:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r30,r30,80
	r30.s64 = r30.s64 + 80;
	// bne 0x821a5064
	if (!cr0.getEQ()) goto loc_821A5064;
	// addi r31,r31,11364
	r31.s64 = r31.s64 + 11364;
	// li r30,6
	r30.s64 = 6;
loc_821A5094:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a50b0
	if (cr0.getEQ()) goto loc_821A50B0;
	// li r4,-1
	ctx.r4.s64 = -1;
	// bl 0x8235eba8
	sub_8235EBA8(ctx, base);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// bl 0x8235d388
	sub_8235D388(ctx, base);
loc_821A50B0:
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r31,r31,80
	r31.s64 = r31.s64 + 80;
	// bne 0x821a5094
	if (!cr0.getEQ()) goto loc_821A5094;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821A50C8"))) PPC_WEAK_FUNC(sub_821A50C8);
PPC_FUNC_IMPL(__imp__sub_821A50C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed12c
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x821a50f8
	if (!cr6.getGT()) goto loc_821A50F8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821A50F8:
	// li r11,8712
	r11.s64 = 8712;
	// li r10,6
	ctx.r10.s64 = 6;
	// lis r9,1
	ctx.r9.s64 = 65536;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// ori r9,r9,8192
	ctx.r9.u64 = ctx.r9.u64 | 8192;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// li r7,0
	ctx.r7.s64 = 0;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// lwz r10,148(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 148);
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bgt cr6,0x821a5160
	if (cr6.getGT()) goto loc_821A5160;
	// lwz r10,28(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// rlwinm r9,r28,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// li r9,8997
	ctx.r9.s64 = 8997;
	// rlwinm r8,r10,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// clrlwi r9,r10,3
	ctx.r9.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r8,512
	ctx.r10.s64 = ctx.r8.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// b 0x821a5224
	goto loc_821A5224;
loc_821A5160:
	// lis r9,-16384
	ctx.r9.s64 = -1073741824;
	// li r8,0
	ctx.r8.s64 = 0;
	// ori r25,r9,24832
	r25.u64 = ctx.r9.u64 | 24832;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// lis r9,-16384
	ctx.r9.s64 = -1073741824;
	// ori r30,r9,24576
	r30.u64 = ctx.r9.u64 | 24576;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// beq cr6,0x821a5210
	if (cr6.getEQ()) goto loc_821A5210;
	// rlwinm r27,r28,5,0,26
	r27.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 5) & 0xFFFFFFE0;
	// li r28,0
	r28.s64 = 0;
	// addi r29,r29,28
	r29.s64 = r29.s64 + 28;
	// mr r26,r10
	r26.u64 = ctx.r10.u64;
loc_821A5198:
	// li r8,3
	ctx.r8.s64 = 3;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// lis r7,-16383
	ctx.r7.s64 = -1073676288;
	// add r10,r27,r10
	ctx.r10.u64 = r27.u64 + ctx.r10.u64;
	// ori r7,r7,11521
	ctx.r7.u64 = ctx.r7.u64 | 11521;
	// lis r6,4
	ctx.r6.s64 = 262144;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// clrlwi r9,r10,3
	ctx.r9.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// ori r6,r6,805
	ctx.r6.u64 = ctx.r6.u64 | 805;
	// slw r5,r8,r28
	ctx.r5.u64 = r28.u8 & 0x20 ? 0 : (ctx.r8.u32 << (r28.u8 & 0x3F));
	// rlwinm r8,r10,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// addi r10,r8,512
	ctx.r10.s64 = ctx.r8.s64 + 512;
	// stwu r5,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	r11.u32 = ea;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r6,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// ble cr6,0x821a5200
	if (!cr6.getGT()) goto loc_821A5200;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A5200:
	// addic. r26,r26,-1
	xer.ca = r26.u32 > 0;
	r26.s64 = r26.s64 + -1;
	cr0.compare<int32_t>(r26.s32, 0, xer);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// bne 0x821a5198
	if (!cr0.getEQ()) goto loc_821A5198;
loc_821A5210:
	// stwu r30,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r30.u32);
	r11.u32 = ea;
	// lwz r10,12700(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12700);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r25,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r25.u32);
	r11.u32 = ea;
	// lwz r10,12704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12704);
loc_821A5224:
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// li r9,21
	ctx.r9.s64 = 21;
	// ori r10,r10,23296
	ctx.r10.u64 = ctx.r10.u64 | 23296;
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,57,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 57) & 0xFFFFFFFFFFFFFFFF;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r11,r11,8
	r11.u64 = r11.u64 | 8;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// or r11,r11,r12
	r11.u64 = r11.u64 | r12.u64;
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,56,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 56) & 0xFFFFFFFFFFFFFFFF;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// or r11,r11,r12
	r11.u64 = r11.u64 | r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed17c
	return;
}

__attribute__((alias("__imp__sub_821A5278"))) PPC_WEAK_FUNC(sub_821A5278);
PPC_FUNC_IMPL(__imp__sub_821A5278) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// li r9,0
	ctx.r9.s64 = 0;
	// bne cr6,0x821a537c
	if (!cr6.getEQ()) goto loc_821A537C;
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// stw r9,12708(r31)
	PPC_STORE_U32(r31.u32 + 12708, ctx.r9.u32);
	// rlwinm. r11,r10,0,28,28
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a5368
	if (!cr0.getEQ()) goto loc_821A5368;
	// rlwinm. r11,r10,0,29,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a5368
	if (!cr0.getEQ()) goto loc_821A5368;
	// lbz r11,12179(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 12179);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821a5368
	if (!cr0.getEQ()) goto loc_821A5368;
	// rlwinm. r11,r10,0,27,27
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a52cc
	if (cr0.getEQ()) goto loc_821A52CC;
	// li r11,1
	r11.s64 = 1;
	// b 0x821a535c
	goto loc_821A535C;
loc_821A52CC:
	// rlwinm. r11,r10,0,26,26
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a5354
	if (cr0.getEQ()) goto loc_821A5354;
	// lwz r11,12432(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12432);
	// lwz r8,12720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 12720);
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// beq cr6,0x821a52ec
	if (cr6.getEQ()) goto loc_821A52EC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5354
	if (!cr6.getEQ()) goto loc_821A5354;
loc_821A52EC:
	// lwz r11,12436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12436);
	// lwz r8,12724(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 12724);
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// beq cr6,0x821a5304
	if (cr6.getEQ()) goto loc_821A5304;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5354
	if (!cr6.getEQ()) goto loc_821A5354;
loc_821A5304:
	// lwz r11,12440(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12440);
	// lwz r8,12728(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 12728);
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// beq cr6,0x821a531c
	if (cr6.getEQ()) goto loc_821A531C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5354
	if (!cr6.getEQ()) goto loc_821A5354;
loc_821A531C:
	// lwz r11,12444(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12444);
	// lwz r8,12732(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 12732);
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// beq cr6,0x821a5334
	if (cr6.getEQ()) goto loc_821A5334;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5354
	if (!cr6.getEQ()) goto loc_821A5354;
loc_821A5334:
	// lwz r11,12448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12448);
	// lwz r8,12736(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 12736);
	// cmplw cr6,r8,r11
	cr6.compare<uint32_t>(ctx.r8.u32, r11.u32, xer);
	// beq cr6,0x821a534c
	if (cr6.getEQ()) goto loc_821A534C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5354
	if (!cr6.getEQ()) goto loc_821A5354;
loc_821A534C:
	// li r11,1
	r11.s64 = 1;
	// b 0x821a5358
	goto loc_821A5358;
loc_821A5354:
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_821A5358:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
loc_821A535C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne 0x821a536c
	if (!cr0.getEQ()) goto loc_821A536C;
loc_821A5368:
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_821A536C:
	// li r8,-1
	ctx.r8.s64 = -1;
	// rlwimi r11,r10,0,24,30
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFE) | (r11.u64 & 0xFFFFFFFFFFFFFF01);
	// stw r8,12700(r31)
	PPC_STORE_U32(r31.u32 + 12700, ctx.r8.u32);
	// b 0x821a5390
	goto loc_821A5390;
loc_821A537C:
	// li r10,1
	ctx.r10.s64 = 1;
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// stw r4,12700(r31)
	PPC_STORE_U32(r31.u32 + 12700, ctx.r4.u32);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,12708(r31)
	PPC_STORE_U32(r31.u32 + 12708, ctx.r10.u32);
loc_821A5390:
	// stb r11,10940(r31)
	PPC_STORE_U8(r31.u32 + 10940, r11.u8);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// stw r9,12704(r31)
	PPC_STORE_U32(r31.u32 + 12704, ctx.r9.u32);
	// stw r4,10932(r31)
	PPC_STORE_U32(r31.u32 + 10932, ctx.r4.u32);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// stw r9,10936(r31)
	PPC_STORE_U32(r31.u32 + 10936, ctx.r9.u32);
	// ble cr6,0x821a53b8
	if (!cr6.getGT()) goto loc_821A53B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821A53B8:
	// lis r11,-16384
	r11.s64 = -1073741824;
	// ori r11,r11,24576
	r11.u64 = r11.u64 | 24576;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,12700(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12700);
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A53E8"))) PPC_WEAK_FUNC(sub_821A53E8);
PPC_FUNC_IMPL(__imp__sub_821A53E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed120
	// stfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -96, f31.u64);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// stw r27,12740(r31)
	PPC_STORE_U32(r31.u32 + 12740, r27.u32);
	// mr r29,r26
	r29.u64 = r26.u64;
	// mr r28,r26
	r28.u64 = r26.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x821a54a8
	if (cr6.getEQ()) goto loc_821A54A8;
	// addi r9,r31,12988
	ctx.r9.s64 = r31.s64 + 12988;
	// addi r11,r22,12
	r11.s64 = r22.s64 + 12;
	// addi r10,r31,12748
	ctx.r10.s64 = r31.s64 + 12748;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
loc_821A543C:
	// lwz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// cmplw cr6,r29,r7
	cr6.compare<uint32_t>(r29.u32, ctx.r7.u32, xer);
	// bgt cr6,0x821a544c
	if (cr6.getGT()) goto loc_821A544C;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
loc_821A544C:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r28,r7
	cr6.compare<uint32_t>(r28.u32, ctx.r7.u32, xer);
	// bgt cr6,0x821a545c
	if (cr6.getGT()) goto loc_821A545C;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
loc_821A545C:
	// lwz r7,-12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -12);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r7,-4(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4, ctx.r7.u32);
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// lwz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// lwz r7,-12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -12);
	// rlwinm r7,r7,0,0,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFE0;
	// stw r7,-4(r9)
	PPC_STORE_U32(ctx.r9.u32 + -4, ctx.r7.u32);
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// rlwinm r7,r7,0,0,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFE0;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// bne 0x821a543c
	if (!cr0.getEQ()) goto loc_821A543C;
loc_821A54A8:
	// rlwinm. r11,r23,0,30,30
	r11.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a55d0
	if (cr0.getEQ()) goto loc_821A55D0;
	// lbz r11,10943(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// rlwinm. r10,r23,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(r23.u32 | (r23.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ori r11,r11,32
	r11.u64 = r11.u64 | 32;
	// stb r11,10943(r31)
	PPC_STORE_U8(r31.u32 + 10943, r11.u8);
	// beq 0x821a54cc
	if (cr0.getEQ()) goto loc_821A54CC;
	// ori r11,r11,16
	r11.u64 = r11.u64 | 16;
	// stb r11,10943(r31)
	PPC_STORE_U8(r31.u32 + 10943, r11.u8);
loc_821A54CC:
	// lhz r11,10368(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 10368);
	// clrlwi. r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a54e0
	if (!cr0.getEQ()) goto loc_821A54E0;
	// li r3,16
	ctx.r3.s64 = 16;
	// b 0x821a54ec
	goto loc_821A54EC;
loc_821A54E0:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// li r3,8
	ctx.r3.s64 = 8;
	// bne cr6,0x821a54f8
	if (!cr6.getEQ()) goto loc_821A54F8;
loc_821A54EC:
	// li r10,32
	ctx.r10.s64 = 32;
	// li r30,80
	r30.s64 = 80;
	// b 0x821a5500
	goto loc_821A5500;
loc_821A54F8:
	// li r10,16
	ctx.r10.s64 = 16;
	// li r30,40
	r30.s64 = 40;
loc_821A5500:
	// add r11,r10,r29
	r11.u64 = ctx.r10.u64 + r29.u64;
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// andc r5,r11,r9
	ctx.r5.u64 = r11.u64 & ~ctx.r9.u64;
	// twllei r10,0
	// divwu r4,r5,r10
	ctx.r4.u32 = ctx.r5.u32 / ctx.r10.u32;
	// beq cr6,0x821a5560
	if (cr6.getEQ()) goto loc_821A5560;
	// addi r8,r31,13104
	ctx.r8.s64 = r31.s64 + 13104;
	// addi r11,r31,12984
	r11.s64 = r31.s64 + 12984;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
loc_821A552C:
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// twllei r3,0
	// divwu r7,r7,r3
	ctx.r7.u32 = ctx.r7.u32 / ctx.r3.u32;
	// divwu r6,r6,r10
	ctx.r6.u32 = ctx.r6.u32 / ctx.r10.u32;
	// mullw r7,r7,r4
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r4.s32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// twllei r10,0
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bne 0x821a552c
	if (!cr0.getEQ()) goto loc_821A552C;
loc_821A5560:
	// lwz r10,10368(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10368);
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// stw r5,13164(r31)
	PPC_STORE_U32(r31.u32 + 13164, ctx.r5.u32);
	// twllei r30,0
	// rlwimi r10,r5,18,0,13
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r5.u32, 18) & 0xFFFC0000) | (ctx.r10.u64 & 0xFFFFFFFF0003FFFF);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// divwu r11,r11,r30
	r11.u32 = r11.u32 / r30.u32;
	// stw r10,10368(r31)
	PPC_STORE_U32(r31.u32 + 10368, ctx.r10.u32);
	// mullw r11,r11,r30
	r11.s64 = int64_t(r11.s32) * int64_t(r30.s32);
	// ld r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r10,r10,256
	ctx.r10.u64 = ctx.r10.u64 | 256;
	// std r10,16(r31)
	PPC_STORE_U64(r31.u32 + 16, ctx.r10.u64);
	// lbz r10,10943(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// stw r11,13168(r31)
	PPC_STORE_U32(r31.u32 + 13168, r11.u32);
	// rlwinm. r11,r10,0,26,26
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a55c4
	if (!cr0.getEQ()) goto loc_821A55C4;
	// lwz r11,11844(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 11844);
	// lis r10,4
	ctx.r10.s64 = 262144;
	// rlwinm r11,r11,0,12,14
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xE0000;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a55c4
	if (!cr6.getEQ()) goto loc_821A55C4;
	// ld r11,40(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 40);
	// li r12,-257
	r12.s64 = -257;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// b 0x821a55cc
	goto loc_821A55CC;
loc_821A55C4:
	// ld r11,40(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 40);
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
loc_821A55CC:
	// std r11,40(r31)
	PPC_STORE_U64(r31.u32 + 40, r11.u64);
loc_821A55D0:
	// clrlwi. r11,r23,31
	r11.u64 = r23.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r29,13172(r31)
	PPC_STORE_U32(r31.u32 + 13172, r29.u32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r28,13176(r31)
	PPC_STORE_U32(r31.u32 + 13176, r28.u32);
	// stw r23,13180(r31)
	PPC_STORE_U32(r31.u32 + 13180, r23.u32);
	// addi r30,r11,-18980
	r30.s64 = r11.s64 + -18980;
	// bne 0x821a56ec
	if (!cr0.getEQ()) goto loc_821A56EC;
	// lbz r11,10943(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a56ec
	if (!cr0.getEQ()) goto loc_821A56EC;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x821a5654
	if (cr6.getEQ()) goto loc_821A5654;
	// addi r10,r31,12756
	ctx.r10.s64 = r31.s64 + 12756;
	// addi r11,r31,12988
	r11.s64 = r31.s64 + 12988;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
loc_821A5614:
	// lwz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lwz r9,-4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subf r7,r7,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r7.s64;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// bgt cr6,0x821a5638
	if (cr6.getGT()) goto loc_821A5638;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
loc_821A5638:
	// cmplw cr6,r6,r7
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r7.u32, xer);
	// bgt cr6,0x821a5644
	if (cr6.getGT()) goto loc_821A5644;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
loc_821A5644:
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne 0x821a5614
	if (!cr0.getEQ()) goto loc_821A5614;
loc_821A5654:
	// addi r4,r31,12640
	ctx.r4.s64 = r31.s64 + 12640;
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// li r5,28
	ctx.r5.s64 = 28;
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// addi r10,r31,12668
	ctx.r10.s64 = r31.s64 + 12668;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r7,8(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// stw r7,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r7.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// bl 0x82195ac8
	sub_82195AC8(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,-18956
	ctx.r4.s64 = r11.s64 + -18956;
	// bl 0x82195288
	sub_82195288(ctx, base);
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,63
	ctx.r4.s64 = 63;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82197390
	sub_82197390(ctx, base);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195b48
	sub_82195B48(ctx, base);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195288
	sub_82195288(ctx, base);
loc_821A56EC:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x821a5704
	if (cr6.getEQ()) goto loc_821A5704;
	// li r11,13184
	r11.s64 = 13184;
	// lvx128 v0,r0,r25
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r31,r11
	_mm_store_si128((__m128i*)(base + ((r31.u32 + r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x821a570c
	goto loc_821A570C;
loc_821A5704:
	// std r26,13184(r31)
	PPC_STORE_U64(r31.u32 + 13184, r26.u64);
	// std r26,13192(r31)
	PPC_STORE_U64(r31.u32 + 13192, r26.u64);
loc_821A570C:
	// lwz r10,12432(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12432);
	// stfs f31,13200(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r31.u32 + 13200, temp.u32);
	// stw r24,13204(r31)
	PPC_STORE_U32(r31.u32 + 13204, r24.u32);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// bne 0x821a5728
	if (!cr0.getEQ()) goto loc_821A5728;
	// lwz r9,12448(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12448);
loc_821A5728:
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// li r5,2
	ctx.r5.s64 = 2;
	// lwz r8,12436(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 12436);
	// ori r11,r11,32
	r11.u64 = r11.u64 | 32;
	// lwz r7,12440(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 12440);
	// lwz r6,12444(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 12444);
	// lwz r4,12448(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 12448);
	// stw r10,12720(r31)
	PPC_STORE_U32(r31.u32 + 12720, ctx.r10.u32);
	// stw r9,12716(r31)
	PPC_STORE_U32(r31.u32 + 12716, ctx.r9.u32);
	// stw r8,12724(r31)
	PPC_STORE_U32(r31.u32 + 12724, ctx.r8.u32);
	// stw r7,12728(r31)
	PPC_STORE_U32(r31.u32 + 12728, ctx.r7.u32);
	// stw r6,12732(r31)
	PPC_STORE_U32(r31.u32 + 12732, ctx.r6.u32);
	// stw r4,12736(r31)
	PPC_STORE_U32(r31.u32 + 12736, ctx.r4.u32);
	// stw r5,12712(r31)
	PPC_STORE_U32(r31.u32 + 12712, ctx.r5.u32);
	// stw r26,12708(r31)
	PPC_STORE_U32(r31.u32 + 12708, r26.u32);
	// stb r11,10940(r31)
	PPC_STORE_U8(r31.u32 + 10940, r11.u8);
	// rlwinm. r10,r11,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a57b4
	if (!cr0.getEQ()) goto loc_821A57B4;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// rlwinm. r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a57b4
	if (!cr0.getEQ()) goto loc_821A57B4;
	// lbz r11,12179(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 12179);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821a57b4
	if (!cr0.getEQ()) goto loc_821A57B4;
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r11,r11,0,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a579c
	if (cr0.getEQ()) goto loc_821A579C;
	// li r11,1
	r11.s64 = 1;
	// b 0x821a57a8
	goto loc_821A57A8;
loc_821A579C:
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
loc_821A57A8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne 0x821a57b8
	if (!cr0.getEQ()) goto loc_821A57B8;
loc_821A57B4:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_821A57B8:
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// li r9,-1
	ctx.r9.s64 = -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r26,12704(r31)
	PPC_STORE_U32(r31.u32 + 12704, r26.u32);
	// rlwimi r11,r10,0,24,30
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFE) | (r11.u64 & 0xFFFFFFFFFFFFFF01);
	// stw r26,10932(r31)
	PPC_STORE_U32(r31.u32 + 10932, r26.u32);
	// stw r26,10936(r31)
	PPC_STORE_U32(r31.u32 + 10936, r26.u32);
	// stw r9,12700(r31)
	PPC_STORE_U32(r31.u32 + 12700, ctx.r9.u32);
	// stb r11,10940(r31)
	PPC_STORE_U8(r31.u32 + 10940, r11.u8);
	// bl 0x821a4be0
	sub_821A4BE0(ctx, base);
	// addi r3,r31,13352
	ctx.r3.s64 = r31.s64 + 13352;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a5800
	if (!cr6.getGT()) goto loc_821A5800;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A5800:
	// lis r10,-31232
	ctx.r10.s64 = -2046820352;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// stw r9,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r9.u32);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a582c
	if (!cr6.getGT()) goto loc_821A582C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A582C:
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// li r9,25
	ctx.r9.s64 = 25;
	// ori r10,r10,17920
	ctx.r10.u64 = ctx.r10.u64 | 17920;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x821a6e90
	sub_821A6E90(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82195ac8
	sub_82195AC8(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195758
	sub_82195758(ctx, base);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x823ed170
	return;
}

__attribute__((alias("__imp__sub_821A5880"))) PPC_WEAK_FUNC(sub_821A5880);
PPC_FUNC_IMPL(__imp__sub_821A5880) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed120
	// stfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -96, f31.u64);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r24,r9
	r24.u64 = ctx.r9.u64;
	// lwz r11,10896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// li r22,0
	r22.s64 = 0;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bne cr6,0x821a58cc
	if (!cr6.getEQ()) goto loc_821A58CC;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821a5a2c
	if (cr6.getEQ()) goto loc_821A5A2C;
	// addi r5,r31,12744
	ctx.r5.s64 = r31.s64 + 12744;
loc_821A58CC:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821a5a2c
	if (cr6.getEQ()) goto loc_821A5A2C;
	// lwz r11,12740(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// mr r27,r22
	r27.u64 = r22.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x821a5a2c
	if (!cr6.getGT()) goto loc_821A5A2C;
	// mr r28,r22
	r28.u64 = r22.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
loc_821A58EC:
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r7,8(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r6,12(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// lwz r10,12716(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12716);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// stw r7,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r7.u32);
	// stw r6,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r6.u32);
	// lwz r11,36(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// rlwinm r11,r11,14,18,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 14) & 0x3FFF;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x821a5938
	if (!cr6.getEQ()) goto loc_821A5938;
	// addi r11,r11,7
	r11.s64 = r11.s64 + 7;
	// rlwinm r11,r11,0,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF8;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
loc_821A5938:
	// lwz r11,36(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// rlwinm r11,r11,29,17,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 29) & 0x7FFF;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a595c
	if (!cr6.getEQ()) goto loc_821A595C;
	// addi r11,r11,7
	r11.s64 = r11.s64 + 7;
	// rlwinm r11,r11,0,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFF8;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
loc_821A595C:
	// li r11,3
	r11.s64 = 3;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// slw r4,r11,r28
	ctx.r4.u64 = r28.u8 & 0x20 ? 0 : (r11.u32 << (r28.u8 & 0x3F));
	// bl 0x821a5278
	sub_821A5278(ctx, base);
	// lbz r11,10943(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// not r11,r11
	r11.u64 = ~r11.u64;
	// rlwinm r4,r11,28,30,30
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 28) & 0x2;
	// bl 0x82194750
	sub_82194750(ctx, base);
	// lwz r11,12740(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// bge cr6,0x821a59e8
	if (!cr6.getLT()) goto loc_821A59E8;
	// lwz r11,12432(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12432);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a59a4
	if (cr6.getEQ()) goto loc_821A59A4;
	// ori r4,r26,256
	ctx.r4.u64 = r26.u64 | 256;
loc_821A59A4:
	// lwz r11,12448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12448);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a59b4
	if (cr6.getEQ()) goto loc_821A59B4;
	// ori r4,r4,512
	ctx.r4.u64 = ctx.r4.u64 | 512;
loc_821A59B4:
	// lwz r11,13204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13204);
	// addi r10,r31,13184
	ctx.r10.s64 = r31.s64 + 13184;
	// li r9,0
	ctx.r9.s64 = 0;
	// lfs f1,13200(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 13200);
	ctx.f1.f64 = double(temp.f32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r22.u32);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82197c48
	sub_82197C48(ctx, base);
	// b 0x821a5a14
	goto loc_821A5A14;
loc_821A59E8:
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// stw r23,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r23.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r24.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82197c48
	sub_82197C48(ctx, base);
loc_821A5A14:
	// lwz r11,12740(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// blt cr6,0x821a58ec
	if (cr6.getLT()) goto loc_821A58EC;
loc_821A5A2C:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a5278
	sub_821A5278(ctx, base);
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82194750
	sub_82194750(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// lwz r11,13180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13180);
	// lwz r10,11312(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 11312);
	// lis r9,256
	ctx.r9.s64 = 16777216;
	// rlwinm. r11,r11,0,2,7
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// slw r27,r9,r10
	r27.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// beq 0x821a5a68
	if (cr0.getEQ()) goto loc_821A5A68;
	// mr r27,r11
	r27.u64 = r11.u64;
loc_821A5A68:
	// lwz r11,12740(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x821a5ba0
	if (cr6.getEQ()) goto loc_821A5BA0;
	// lis r29,-32768
	r29.s64 = -2147483648;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x821a5278
	sub_821A5278(ctx, base);
	// addi r30,r31,13352
	r30.s64 = r31.s64 + 13352;
	// lis r28,16384
	r28.s64 = 1073741824;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821a5aa0
	if (!cr0.getEQ()) goto loc_821A5AA0;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// b 0x821a5ab8
	goto loc_821A5AB8;
loc_821A5AA0:
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r6,r28,r11
	ctx.r6.s64 = r11.s64 - r28.s64;
loc_821A5AB8:
	// lis r11,-32230
	r11.s64 = -2112225280;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// addi r5,r11,19288
	ctx.r5.s64 = r11.s64 + 19288;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d9d0
	sub_8219D9D0(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a5278
	sub_821A5278(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r3,8
	ctx.r10.s64 = ctx.r3.s64 + 8;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x821a5afc
	if (!cr6.getGT()) goto loc_821A5AFC;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
loc_821A5AFC:
	// lis r11,-30720
	r11.s64 = -2013265920;
	// stw r29,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r29.u32);
	// addi r10,r3,8
	ctx.r10.s64 = ctx.r3.s64 + 8;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r10,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r10.u32);
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r3,12
	ctx.r10.s64 = ctx.r3.s64 + 12;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x821a5b2c
	if (!cr6.getGT()) goto loc_821A5B2C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
loc_821A5B2C:
	// lis r11,-30464
	r11.s64 = -1996488704;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// lwz r11,13232(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13232);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821a5b48
	if (!cr0.getEQ()) goto loc_821A5B48;
	// mr r11,r22
	r11.u64 = r22.u64;
	// b 0x821a5b60
	goto loc_821A5B60;
loc_821A5B48:
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
loc_821A5B60:
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// lwz r11,13332(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13332);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821a5b78
	if (!cr0.getEQ()) goto loc_821A5B78;
	// mr r11,r22
	r11.u64 = r22.u64;
	// b 0x821a5b90
	goto loc_821A5B90;
loc_821A5B78:
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
loc_821A5B90:
	// addi r10,r3,12
	ctx.r10.s64 = ctx.r3.s64 + 12;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// stw r10,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r10.u32);
	// b 0x821a5ba4
	goto loc_821A5BA4;
loc_821A5BA0:
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821A5BA4:
	// addi r3,r31,13352
	ctx.r3.s64 = r31.s64 + 13352;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a5bc4
	if (!cr6.getGT()) goto loc_821A5BC4;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A5BC4:
	// lis r10,-30976
	ctx.r10.s64 = -2030043136;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// lbz r10,10943(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// stw r9,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r9.u32);
	// stw r22,12716(r31)
	PPC_STORE_U32(r31.u32 + 12716, r22.u32);
	// stw r22,12708(r31)
	PPC_STORE_U32(r31.u32 + 12708, r22.u32);
	// andi. r11,r11,223
	r11.u64 = r11.u64 & 223;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// andi. r10,r10,207
	ctx.r10.u64 = ctx.r10.u64 & 207;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stb r11,10940(r31)
	PPC_STORE_U8(r31.u32 + 10940, r11.u8);
	// stb r10,10943(r31)
	PPC_STORE_U8(r31.u32 + 10943, ctx.r10.u8);
	// rlwinm. r10,r11,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821a5cc8
	if (!cr0.getEQ()) goto loc_821A5CC8;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// rlwinm. r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a5cc8
	if (!cr0.getEQ()) goto loc_821A5CC8;
	// lbz r11,12179(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 12179);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821a5cc8
	if (!cr0.getEQ()) goto loc_821A5CC8;
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r11,r11,0,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a5c28
	if (cr0.getEQ()) goto loc_821A5C28;
	// li r11,1
	r11.s64 = 1;
	// b 0x821a5cbc
	goto loc_821A5CBC;
loc_821A5C28:
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a5cb4
	if (cr0.getEQ()) goto loc_821A5CB4;
	// lwz r11,12432(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12432);
	// lwz r10,12720(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12720);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x821a5c4c
	if (cr6.getEQ()) goto loc_821A5C4C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5cb4
	if (!cr6.getEQ()) goto loc_821A5CB4;
loc_821A5C4C:
	// lwz r11,12436(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12436);
	// lwz r10,12724(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12724);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x821a5c64
	if (cr6.getEQ()) goto loc_821A5C64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5cb4
	if (!cr6.getEQ()) goto loc_821A5CB4;
loc_821A5C64:
	// lwz r11,12440(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12440);
	// lwz r10,12728(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12728);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x821a5c7c
	if (cr6.getEQ()) goto loc_821A5C7C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5cb4
	if (!cr6.getEQ()) goto loc_821A5CB4;
loc_821A5C7C:
	// lwz r11,12444(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12444);
	// lwz r10,12732(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12732);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x821a5c94
	if (cr6.getEQ()) goto loc_821A5C94;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5cb4
	if (!cr6.getEQ()) goto loc_821A5CB4;
loc_821A5C94:
	// lwz r11,12448(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12448);
	// lwz r10,12736(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12736);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x821a5cac
	if (cr6.getEQ()) goto loc_821A5CAC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a5cb4
	if (!cr6.getEQ()) goto loc_821A5CB4;
loc_821A5CAC:
	// li r11,1
	r11.s64 = 1;
	// b 0x821a5cb8
	goto loc_821A5CB8;
loc_821A5CB4:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_821A5CB8:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
loc_821A5CBC:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne 0x821a5ccc
	if (!cr0.getEQ()) goto loc_821A5CCC;
loc_821A5CC8:
	// mr r11,r22
	r11.u64 = r22.u64;
loc_821A5CCC:
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// li r9,-1
	ctx.r9.s64 = -1;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,12740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// rlwimi r11,r10,0,24,30
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFE) | (r11.u64 & 0xFFFFFFFFFFFFFF01);
	// stw r22,12704(r31)
	PPC_STORE_U32(r31.u32 + 12704, r22.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r22,10932(r31)
	PPC_STORE_U32(r31.u32 + 10932, r22.u32);
	// stw r22,10936(r31)
	PPC_STORE_U32(r31.u32 + 10936, r22.u32);
	// stw r9,12700(r31)
	PPC_STORE_U32(r31.u32 + 12700, ctx.r9.u32);
	// stb r11,10940(r31)
	PPC_STORE_U8(r31.u32 + 10940, r11.u8);
	// bl 0x821a4d50
	sub_821A4D50(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r4,r11,-18980
	ctx.r4.s64 = r11.s64 + -18980;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195ac8
	sub_82195AC8(ctx, base);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x821a5d30
	if (cr6.getEQ()) goto loc_821A5D30;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195758
	sub_82195758(ctx, base);
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x821a5d34
	goto loc_821A5D34;
loc_821A5D30:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821A5D34:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// lfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x823ed170
	return;
}

__attribute__((alias("__imp__sub_821A5D40"))) PPC_WEAK_FUNC(sub_821A5D40);
PPC_FUNC_IMPL(__imp__sub_821A5D40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a5d70
	if (!cr0.getEQ()) goto loc_821A5D70;
	// li r11,1
	r11.s64 = 1;
	// stw r11,12712(r31)
	PPC_STORE_U32(r31.u32 + 12712, r11.u32);
	// bl 0x821a4be0
	sub_821A4BE0(ctx, base);
	// b 0x821a5d74
	goto loc_821A5D74;
loc_821A5D70:
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821A5D74:
	// addi r3,r31,13352
	ctx.r3.s64 = r31.s64 + 13352;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a5d94
	if (!cr6.getGT()) goto loc_821A5D94;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A5D94:
	// lis r10,-32000
	ctx.r10.s64 = -2097152000;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
	// not r9,r30
	ctx.r9.u64 = ~r30.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// rlwimi r11,r9,6,25,25
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 6) & 0x40) | (r11.u64 & 0xFFFFFFFFFFFFFFBF);
	// lbz r9,10943(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// lwz r7,10548(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 10548);
	// ori r10,r10,64
	ctx.r10.u64 = ctx.r10.u64 | 64;
	// stw r8,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r8.u32);
	// rlwinm r8,r7,28,29,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 28) & 0x7;
	// stb r11,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, r11.u8);
	// stb r10,10940(r31)
	PPC_STORE_U8(r31.u32 + 10940, ctx.r10.u8);
	// stw r8,12696(r31)
	PPC_STORE_U32(r31.u32 + 12696, ctx.r8.u32);
	// rlwinm. r9,r9,0,26,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x821a5e10
	if (cr0.getEQ()) goto loc_821A5E10;
	// li r12,1
	r12.s64 = 1;
	// ld r11,40(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 40);
	// lwz r10,10368(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 10368);
	// rldicr r12,r12,37,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 37) & 0xFFFFFFFFFFFFFFFF;
	// lwz r9,13168(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 13168);
	// clrlwi r10,r10,18
	ctx.r10.u64 = ctx.r10.u32 & 0x3FFF;
	// or r11,r11,r12
	r11.u64 = r11.u64 | r12.u64;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// std r11,40(r31)
	PPC_STORE_U64(r31.u32 + 40, r11.u64);
	// beq cr6,0x821a5e10
	if (cr6.getEQ()) goto loc_821A5E10;
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,57,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 57) & 0xFFFFFFFFFFFFFFFF;
	// or r11,r11,r12
	r11.u64 = r11.u64 | r12.u64;
	// std r11,40(r31)
	PPC_STORE_U64(r31.u32 + 40, r11.u64);
loc_821A5E10:
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// li r10,-1
	ctx.r10.s64 = -1;
	// li r4,1
	ctx.r4.s64 = 1;
	// ori r11,r11,8
	r11.u64 = r11.u64 | 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// stw r10,12692(r31)
	PPC_STORE_U32(r31.u32 + 12692, ctx.r10.u32);
	// bl 0x82195758
	sub_82195758(ctx, base);
	// lwz r11,13180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13180);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a5fb8
	if (!cr0.getEQ()) goto loc_821A5FB8;
	// lbz r11,10943(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a5fb8
	if (cr0.getEQ()) goto loc_821A5FB8;
	// lwz r11,12740(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12740);
	// li r28,0
	r28.s64 = 0;
	// mr r27,r28
	r27.u64 = r28.u64;
	// mr r26,r28
	r26.u64 = r28.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x821a5eac
	if (!cr6.getGT()) goto loc_821A5EAC;
	// rotlwi r7,r11,0
	ctx.r7.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r10,r31,12756
	ctx.r10.s64 = r31.s64 + 12756;
	// addi r11,r31,12988
	r11.s64 = r31.s64 + 12988;
loc_821A5E6C:
	// lwz r8,-4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// lwz r9,-4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// subf r8,r8,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r8.s64;
	// cmplw cr6,r27,r9
	cr6.compare<uint32_t>(r27.u32, ctx.r9.u32, xer);
	// bgt cr6,0x821a5e90
	if (cr6.getGT()) goto loc_821A5E90;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
loc_821A5E90:
	// cmplw cr6,r26,r8
	cr6.compare<uint32_t>(r26.u32, ctx.r8.u32, xer);
	// bgt cr6,0x821a5e9c
	if (cr6.getGT()) goto loc_821A5E9C;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
loc_821A5E9C:
	// addic. r7,r7,-1
	xer.ca = ctx.r7.u32 > 0;
	ctx.r7.s64 = ctx.r7.s64 + -1;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// bne 0x821a5e6c
	if (!cr0.getEQ()) goto loc_821A5E6C;
loc_821A5EAC:
	// addi r4,r31,12640
	ctx.r4.s64 = r31.s64 + 12640;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// li r5,28
	ctx.r5.s64 = 28;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// addi r10,r31,12668
	ctx.r10.s64 = r31.s64 + 12668;
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// addi r4,r9,-18980
	ctx.r4.s64 = ctx.r9.s64 + -18980;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r7,8(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// stw r7,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r7.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// bl 0x82195ac8
	sub_82195AC8(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,-18956
	ctx.r4.s64 = r11.s64 + -18956;
	// bl 0x82195288
	sub_82195288(ctx, base);
	// lwz r11,13172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13172);
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lwz r11,13176(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13176);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x821a5278
	sub_821A5278(ctx, base);
	// addi r30,r31,13184
	r30.s64 = r31.s64 + 13184;
	// lwz r8,13204(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 13204);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lfs f1,13200(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 13200);
	ctx.f1.f64 = double(temp.f32);
	// li r4,48
	ctx.r4.s64 = 48;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// bl 0x82197390
	sub_82197390(ctx, base);
	// lbz r11,10943(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// li r29,15
	r29.s64 = 15;
	// rlwinm. r11,r11,0,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a5f5c
	if (!cr0.getEQ()) goto loc_821A5F5C;
	// li r29,63
	r29.s64 = 63;
loc_821A5F5C:
	// li r4,2
	ctx.r4.s64 = 2;
	// stw r28,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r28.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// stw r27,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r27.u32);
	// stw r26,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r26.u32);
	// bl 0x821a5278
	sub_821A5278(ctx, base);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r8,13204(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 13204);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lfs f1,13200(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 13200);
	ctx.f1.f64 = double(temp.f32);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82197390
	sub_82197390(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a5278
	sub_821A5278(ctx, base);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195b48
	sub_82195B48(ctx, base);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195288
	sub_82195288(ctx, base);
loc_821A5FB8:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821A5FC0"))) PPC_WEAK_FUNC(sub_821A5FC0);
PPC_FUNC_IMPL(__imp__sub_821A5FC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// clrlwi r11,r11,25
	r11.u64 = r11.u32 & 0x7F;
	// stb r11,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, r11.u8);
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// addi r3,r31,13352
	ctx.r3.s64 = r31.s64 + 13352;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a6004
	if (!cr6.getGT()) goto loc_821A6004;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A6004:
	// lis r10,-31488
	ctx.r10.s64 = -2063597568;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// stw r9,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r9.u32);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a6070
	if (!cr0.getEQ()) goto loc_821A6070;
	// lwz r11,11312(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 11312);
	// lis r10,256
	ctx.r10.s64 = 16777216;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// slw r5,r10,r11
	ctx.r5.u64 = r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (r11.u8 & 0x3F));
	// bl 0x821a4d50
	sub_821A4D50(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a6058
	if (cr0.getEQ()) goto loc_821A6058;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195758
	sub_82195758(ctx, base);
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x821a6074
	goto loc_821A6074;
loc_821A6058:
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// rlwinm. r11,r11,0,25,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a6070
	if (!cr0.getEQ()) goto loc_821A6070;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195758
	sub_82195758(ctx, base);
loc_821A6070:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821A6074:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A6088"))) PPC_WEAK_FUNC(sub_821A6088);
PPC_FUNC_IMPL(__imp__sub_821A6088) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed128
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r24,0
	r24.s64 = 0;
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// lwz r11,10896(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10896);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// rlwinm. r10,r10,0,26,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821a621c
	if (cr0.getEQ()) goto loc_821A621C;
	// addi r25,r31,13232
	r25.s64 = r31.s64 + 13232;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8219c6f0
	sub_8219C6F0(ctx, base);
	// addi r26,r31,13332
	r26.s64 = r31.s64 + 13332;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8219c6f0
	sub_8219C6F0(ctx, base);
	// lwz r11,13180(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13180);
	// lwz r10,11312(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 11312);
	// lis r9,256
	ctx.r9.s64 = 16777216;
	// rlwinm. r11,r11,0,2,7
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// slw r29,r9,r10
	r29.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// beq 0x821a60e4
	if (cr0.getEQ()) goto loc_821A60E4;
	// mr r29,r11
	r29.u64 = r11.u64;
loc_821A60E4:
	// lis r28,-32768
	r28.s64 = -2147483648;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x821a5278
	sub_821A5278(ctx, base);
	// addi r30,r31,13352
	r30.s64 = r31.s64 + 13352;
	// lis r27,16384
	r27.s64 = 1073741824;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821a6110
	if (!cr0.getEQ()) goto loc_821A6110;
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x821a6128
	goto loc_821A6128;
loc_821A6110:
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// subf r6,r27,r11
	ctx.r6.s64 = r11.s64 - r27.s64;
loc_821A6128:
	// lis r11,-32230
	r11.s64 = -2112225280;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r5,r11,19288
	ctx.r5.s64 = r11.s64 + 19288;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d9d0
	sub_8219D9D0(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a5278
	sub_821A5278(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r10,r3,8
	ctx.r10.s64 = ctx.r3.s64 + 8;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x821a616c
	if (!cr6.getGT()) goto loc_821A616C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
loc_821A616C:
	// lis r11,-30720
	r11.s64 = -2013265920;
	// stw r28,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r28.u32);
	// addi r10,r3,8
	ctx.r10.s64 = ctx.r3.s64 + 8;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r10,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r10.u32);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r9,r10,12
	ctx.r9.s64 = ctx.r10.s64 + 12;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// ble cr6,0x821a61a0
	if (!cr6.getGT()) goto loc_821A61A0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_821A61A0:
	// lis r11,-30464
	r11.s64 = -1996488704;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a61cc
	if (cr0.getEQ()) goto loc_821A61CC;
	// rlwinm r8,r11,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r11,3
	ctx.r9.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r8,512
	r11.s64 = ctx.r8.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
loc_821A61CC:
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a61f4
	if (cr0.getEQ()) goto loc_821A61F4;
	// rlwinm r8,r11,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r11,3
	ctx.r9.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r8,512
	r11.s64 = ctx.r8.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
loc_821A61F4:
	// addi r9,r10,12
	ctx.r9.s64 = ctx.r10.s64 + 12;
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r9,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r9.u32);
	// bl 0x8219d530
	sub_8219D530(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8219d530
	sub_8219D530(ctx, base);
	// b 0x821a6224
	goto loc_821A6224;
loc_821A621C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821A6224:
	// addi r3,r31,13352
	ctx.r3.s64 = r31.s64 + 13352;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a6244
	if (!cr6.getGT()) goto loc_821A6244;
	// bl 0x8219d2d8
	sub_8219D2D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A6244:
	// lis r10,-31744
	ctx.r10.s64 = -2080374784;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,56,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 56) & 0xFFFFFFFFFFFFFFFF;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lbz r11,10940(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10940);
	// lbz r10,10941(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// stw r9,13360(r31)
	PPC_STORE_U32(r31.u32 + 13360, ctx.r9.u32);
	// ori r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 128;
	// stb r10,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, ctx.r10.u8);
	// andi. r11,r11,191
	r11.u64 = r11.u64 & 191;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stb r11,10940(r31)
	PPC_STORE_U8(r31.u32 + 10940, r11.u8);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r11,r11,8
	r11.u64 = r11.u64 | 8;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// or r11,r11,r12
	r11.u64 = r11.u64 | r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// lbz r11,10943(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a62e4
	if (cr0.getEQ()) goto loc_821A62E4;
	// addi r4,r31,12668
	ctx.r4.s64 = r31.s64 + 12668;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82195288
	sub_82195288(ctx, base);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// li r12,1
	r12.s64 = 1;
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// rldicr r12,r12,37,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 37) & 0xFFFFFFFFFFFFFFFF;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// or r11,r11,r12
	r11.u64 = r11.u64 | r12.u64;
	// li r12,1
	r12.s64 = 1;
	// rldicr r12,r12,57,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 57) & 0xFFFFFFFFFFFFFFFF;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// or r11,r11,r12
	r11.u64 = r11.u64 | r12.u64;
	// lis r12,-17
	r12.s64 = -1114112;
	// ori r12,r12,65534
	r12.u64 = r12.u64 | 65534;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// rldicr r12,r12,37,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 37) & 0xFFFFFFFFFFFFFFFF;
	// ld r11,40(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 40);
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,40(r31)
	PPC_STORE_U64(r31.u32 + 40, r11.u64);
loc_821A62E4:
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// rlwinm. r11,r11,0,25,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a62fc
	if (cr0.getEQ()) goto loc_821A62FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a5fc0
	sub_821A5FC0(ctx, base);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
loc_821A62FC:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed178
	return;
}

__attribute__((alias("__imp__sub_821A6308"))) PPC_WEAK_FUNC(sub_821A6308);
PPC_FUNC_IMPL(__imp__sub_821A6308) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lbz r11,10943(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 10943);
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// stb r11,10943(r3)
	PPC_STORE_U8(ctx.r3.u32 + 10943, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A6318"))) PPC_WEAK_FUNC(sub_821A6318);
PPC_FUNC_IMPL(__imp__sub_821A6318) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,32767
	r11.s64 = 2147418112;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bne cr6,0x821a634c
	if (!cr6.getEQ()) goto loc_821A634C;
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// b 0x821a636c
	goto loc_821A636C;
loc_821A634C:
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfd f0,-14368(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -14368);
	// fmul f31,f1,f0
	f31.f64 = ctx.f1.f64 * f0.f64;
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// fdiv f0,f31,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f31.f64 / ctx.f1.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(f0.f64));
loc_821A636C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A6388"))) PPC_WEAK_FUNC(sub_821A6388);
PPC_FUNC_IMPL(__imp__sub_821A6388) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x821a63b0
	if (!cr6.getEQ()) goto loc_821A63B0;
	// lis r3,-32038
	ctx.r3.s64 = -2099642368;
	// ori r3,r3,23
	ctx.r3.u64 = ctx.r3.u64 | 23;
	// b 0x821a641c
	goto loc_821A641C;
loc_821A63B0:
	// lbz r11,4(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,107
	cr6.compare<int32_t>(r11.s32, 107, xer);
	// bgt cr6,0x821a63f8
	if (cr6.getGT()) goto loc_821A63F8;
	// beq cr6,0x821a63e8
	if (cr6.getEQ()) goto loc_821A63E8;
	// cmpwi cr6,r11,97
	cr6.compare<int32_t>(r11.s32, 97, xer);
	// beq cr6,0x821a63e8
	if (cr6.getEQ()) goto loc_821A63E8;
	// cmpwi cr6,r11,98
	cr6.compare<int32_t>(r11.s32, 98, xer);
	// ble cr6,0x821a6418
	if (!cr6.getGT()) goto loc_821A6418;
	// cmpwi cr6,r11,100
	cr6.compare<int32_t>(r11.s32, 100, xer);
	// ble cr6,0x821a63e8
	if (!cr6.getGT()) goto loc_821A63E8;
	// addi r11,r11,-102
	r11.s64 = r11.s64 + -102;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bgt cr6,0x821a6418
	if (cr6.getGT()) goto loc_821A6418;
loc_821A63E8:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r4,r11,-14360
	ctx.r4.s64 = r11.s64 + -14360;
	// bl 0x8240f93c
	__imp__sprintf(ctx, base);
	// b 0x821a6418
	goto loc_821A6418;
loc_821A63F8:
	// cmpwi cr6,r11,109
	cr6.compare<int32_t>(r11.s32, 109, xer);
	// beq cr6,0x821a640c
	if (cr6.getEQ()) goto loc_821A640C;
	// cmpwi cr6,r11,116
	cr6.compare<int32_t>(r11.s32, 116, xer);
	// bne cr6,0x821a6418
	if (!cr6.getEQ()) goto loc_821A6418;
	// b 0x821a63e8
	goto loc_821A63E8;
loc_821A640C:
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x821a92d0
	sub_821A92D0(ctx, base);
loc_821A6418:
	// lis r3,730
	ctx.r3.s64 = 47841280;
loc_821A641C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A6430"))) PPC_WEAK_FUNC(sub_821A6430);
PPC_FUNC_IMPL(__imp__sub_821A6430) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,16544(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16544);
	// lwz r10,21644(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 21644);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821a6460
	if (!cr6.getEQ()) goto loc_821A6460;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x821a6480
	goto loc_821A6480;
loc_821A6460:
	// lwz r11,21632(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21632);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// beq cr6,0x821a6498
	if (cr6.getEQ()) goto loc_821A6498;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a647c
	if (!cr6.getEQ()) goto loc_821A647C;
	// li r11,1
	r11.s64 = 1;
	// stw r11,21632(r30)
	PPC_STORE_U32(r30.u32 + 21632, r11.u32);
loc_821A647C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821A6480:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_821A6498:
	// lwz r11,21636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21636);
	// lwz r10,21640(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 21640);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x821a647c
	if (cr6.getEQ()) goto loc_821A647C;
	// addi r11,r11,5403
	r11.s64 = r11.s64 + 5403;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r11,r30
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// bl 0x8219efc8
	sub_8219EFC8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821a647c
	if (!cr0.getEQ()) goto loc_821A647C;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// li r5,480
	ctx.r5.s64 = 480;
	// addi r31,r11,-10804
	r31.s64 = r11.s64 + -10804;
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// addi r4,r31,484
	ctx.r4.s64 = r31.s64 + 484;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// lwz r11,21636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21636);
	// addi r4,r31,484
	ctx.r4.s64 = r31.s64 + 484;
	// addi r11,r11,5403
	r11.s64 = r11.s64 + 5403;
	// li r6,0
	ctx.r6.s64 = 0;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwzx r3,r10,r30
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// stw r11,-4(r31)
	PPC_STORE_U32(r31.u32 + -4, r11.u32);
	// bl 0x821a0680
	sub_821A0680(ctx, base);
	// lwz r11,21636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 21636);
	// lwz r10,21628(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 21628);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bne cr6,0x821a6530
	if (!cr6.getEQ()) goto loc_821A6530;
	// li r11,0
	r11.s64 = 0;
	// b 0x821a6534
	goto loc_821A6534;
loc_821A6530:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_821A6534:
	// lwz r10,16544(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16544);
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,21636(r30)
	PPC_STORE_U32(r30.u32 + 21636, r11.u32);
	// stw r10,21644(r30)
	PPC_STORE_U32(r30.u32 + 21644, ctx.r10.u32);
	// b 0x821a6480
	goto loc_821A6480;
}

__attribute__((alias("__imp__sub_821A6548"))) PPC_WEAK_FUNC(sub_821A6548);
PPC_FUNC_IMPL(__imp__sub_821A6548) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, f30.u64);
	// stfd f31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,32767
	r11.s64 = 2147418112;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// cmplwi cr6,r31,18
	cr6.compare<uint32_t>(r31.u32, 18, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bgt cr6,0x821a69ec
	if (cr6.getGT()) goto loc_821A69EC;
	// lis r12,-32254
	r12.s64 = -2113798144;
	// addi r12,r12,-14352
	r12.s64 = r12.s64 + -14352;
	// rlwinm r0,r31,1,0,30
	r0.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r0,r12,r0
	r0.u64 = PPC_LOAD_U16(r12.u32 + r0.u32);
	// lis r12,-32230
	r12.s64 = -2112225280;
	// addi r12,r12,26020
	r12.s64 = r12.s64 + 26020;
	// add r12,r12,r0
	r12.u64 = r12.u64 + r0.u64;
	// mtctr r12
	ctr.u64 = r12.u64;
	// nop 
	// bctr 
	switch (r31.u64) {
	case 0:
		goto loc_821A65A4;
	case 1:
		goto loc_821A65C4;
	case 2:
		goto loc_821A65F0;
	case 3:
		goto loc_821A6608;
	case 4:
		goto loc_821A666C;
	case 5:
		goto loc_821A6688;
	case 6:
		goto loc_821A66C8;
	case 7:
		goto loc_821A676C;
	case 8:
		goto loc_821A67E8;
	case 9:
		goto loc_821A6838;
	case 10:
		goto loc_821A6898;
	case 11:
		goto loc_821A68CC;
	case 12:
		goto loc_821A6910;
	case 13:
		goto loc_821A6940;
	case 14:
		goto loc_821A696C;
	case 15:
		goto loc_821A6998;
	case 16:
		goto loc_821A69CC;
	case 17:
		goto loc_821A675C;
	case 18:
		goto loc_821A676C;
	default:
		__builtin_unreachable();
	}
loc_821A65A4:
	// lwz r11,21556(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21556);
	// lfs f0,21572(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 21572);
	f0.f64 = double(temp.f32);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * f0.f64));
	// b 0x821a69f0
	goto loc_821A69F0;
loc_821A65C4:
	// lwz r11,21556(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21556);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// lfs f0,21568(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 21568);
	f0.f64 = double(temp.f32);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fdivs f1,f0,f13
	ctx.f1.f64 = double(float(f0.f64 / ctx.f13.f64));
	// b 0x821a69f0
	goto loc_821A69F0;
loc_821A65F0:
	// lwz r11,16544(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16544);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
loc_821A65FC:
	// fcfid f0,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(f0.s64);
	// frsp f1,f0
	ctx.f1.f64 = double(float(f0.f64));
	// b 0x821a69f0
	goto loc_821A69F0;
loc_821A6608:
	// lwz r11,21592(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21592);
	// lwz r10,21556(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21556);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_821A6620:
	// fcfid f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(ctx.f13.s64);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fdivs f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 / ctx.f13.f64));
loc_821A6634:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,2944(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2944);
	f0.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// lfs f12,2688(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f13,f12
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// bge cr6,0x821a6658
	if (!cr6.getLT()) goto loc_821A6658;
	// fmr f0,f12
	f0.f64 = ctx.f12.f64;
	// b 0x821a6664
	goto loc_821A6664;
loc_821A6658:
	// fcmpu cr6,f13,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f13.f64, f0.f64);
	// bgt cr6,0x821a6664
	if (cr6.getGT()) goto loc_821A6664;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_821A6664:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// b 0x821a69f0
	goto loc_821A69F0;
loc_821A666C:
	// lwz r11,21596(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21596);
	// lwz r10,21556(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21556);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// b 0x821a6620
	goto loc_821A6620;
loc_821A6688:
	// lwz r11,21556(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21556);
	// lwz r10,21596(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21596);
	// lwz r9,21592(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21592);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fdivs f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 / f0.f64));
	// b 0x821a6634
	goto loc_821A6634;
loc_821A66C8:
	// lwz r11,21600(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21600);
	// lwz r10,10896(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 10896);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// clrlwi r9,r9,29
	ctx.r9.u64 = ctx.r9.u32 & 0x7;
	// clrlwi r8,r11,29
	ctx.r8.u64 = r11.u32 & 0x7;
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// clrlwi r9,r7,29
	ctx.r9.u64 = ctx.r7.u32 & 0x7;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwbrx r31,r7,r10
	r31.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32));
	// lwbrx r8,r8,r10
	ctx.r8.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32));
	// lwbrx r30,r9,r10
	r30.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32));
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// beq cr6,0x821a69ec
	if (cr6.getEQ()) goto loc_821A69EC;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// stw r11,21600(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21600, r11.u32);
	// beq cr6,0x821a69ec
	if (cr6.getEQ()) goto loc_821A69EC;
	// cmpldi cr6,r30,0
	cr6.compare<uint64_t>(r30.u64, 0, xer);
	// beq cr6,0x821a69ec
	if (cr6.getEQ()) goto loc_821A69EC;
	// li r11,1
	r11.s64 = 1;
	// cmpld cr6,r10,r30
	cr6.compare<uint64_t>(ctx.r10.u64, r30.u64, xer);
	// rldicr r11,r11,32,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// bgt cr6,0x821a6740
	if (cr6.getGT()) goto loc_821A6740;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
loc_821A6740:
	// cmpld cr6,r31,r10
	cr6.compare<uint64_t>(r31.u64, ctx.r10.u64, xer);
	// bgt cr6,0x821a674c
	if (cr6.getGT()) goto loc_821A674C;
	// add r31,r31,r11
	r31.u64 = r31.u64 + r11.u64;
loc_821A674C:
	// subf r3,r10,r31
	ctx.r3.s64 = r31.s64 - ctx.r10.s64;
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// subf r3,r30,r31
	ctx.r3.s64 = r31.s64 - r30.s64;
	// b 0x821a6824
	goto loc_821A6824;
loc_821A675C:
	// lwz r11,21648(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21648);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x821a65fc
	goto loc_821A65FC;
loc_821A676C:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// cmpwi cr6,r31,7
	cr6.compare<int32_t>(r31.s32, 7, xer);
	// addi r11,r11,-10804
	r11.s64 = r11.s64 + -10804;
	// ld r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 4);
	// ld r9,484(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 484);
	// subf r30,r10,r9
	r30.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// ld r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 12);
	// ld r11,492(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 492);
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// bne cr6,0x821a67b8
	if (!cr6.getEQ()) goto loc_821A67B8;
	// ld r11,88(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// subf r3,r11,r3
	ctx.r3.s64 = ctx.r3.s64 - r11.s64;
loc_821A67B8:
	// cmpldi cr6,r3,0
	cr6.compare<uint64_t>(ctx.r3.u64, 0, xer);
	// beq cr6,0x821a69ec
	if (cr6.getEQ()) goto loc_821A69EC;
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// fdiv f0,f31,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f31.f64 / ctx.f1.f64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// lfs f0,2776(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	f0.f64 = double(temp.f32);
	// fsubs f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 - ctx.f13.f64));
	// b 0x821a6634
	goto loc_821A6634;
loc_821A67E8:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r11,r11,-10804
	r11.s64 = r11.s64 + -10804;
	// ld r10,124(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 124);
	// ld r9,604(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 604);
	// subf r31,r10,r9
	r31.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// beq cr6,0x821a69ec
	if (cr6.getEQ()) goto loc_821A69EC;
	// ld r10,132(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 132);
	// ld r11,612(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 612);
loc_821A6818:
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_821A6824:
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// fdiv f0,f31,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f31.f64 / ctx.f1.f64;
loc_821A6830:
	// frsp f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f0.f64));
	// b 0x821a6634
	goto loc_821A6634;
loc_821A6838:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r31,r11,-10804
	r31.s64 = r11.s64 + -10804;
	// ld r11,124(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 124);
	// ld r10,604(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 604);
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// cmpldi cr6,r3,0
	cr6.compare<uint64_t>(ctx.r3.u64, 0, xer);
	// beq cr6,0x821a69ec
	if (cr6.getEQ()) goto loc_821A69EC;
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// ld r11,132(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 132);
	// ld r10,612(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 612);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// ld r11,140(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 140);
	// ld r10,620(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 620);
	// fsub f30,f31,f1
	ctx.fpscr.disableFlushMode();
	f30.f64 = f31.f64 - ctx.f1.f64;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x823ef0a0
	sub_823EF0A0(ctx, base);
	// fsub f0,f30,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f30.f64 - ctx.f1.f64;
	// fdiv f0,f0,f31
	f0.f64 = f0.f64 / f31.f64;
	// b 0x821a6830
	goto loc_821A6830;
loc_821A6898:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r11,r11,-10804
	r11.s64 = r11.s64 + -10804;
	// ld r10,124(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 124);
	// ld r9,604(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 604);
	// subf r31,r10,r9
	r31.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// beq cr6,0x821a69ec
	if (cr6.getEQ()) goto loc_821A69EC;
	// ld r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 140);
	// ld r11,620(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 620);
	// b 0x821a6818
	goto loc_821A6818;
loc_821A68CC:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r11,r11,-10804
	r11.s64 = r11.s64 + -10804;
	// ld r10,444(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 444);
	// ld r9,924(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 924);
	// subf r8,r10,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r10.s64;
	// ld r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 4);
	// ld r9,484(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 484);
	// subf r4,r10,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r10.s64;
	// ld r10,436(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 436);
	// ld r11,916(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 916);
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
loc_821A6908:
	// bl 0x821a6318
	sub_821A6318(ctx, base);
	// b 0x821a69f0
	goto loc_821A69F0;
loc_821A6910:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r11,r11,-10804
	r11.s64 = r11.s64 + -10804;
	// ld r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 4);
	// ld r9,484(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 484);
	// subf r4,r10,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r10.s64;
	// ld r10,452(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 452);
	// ld r11,932(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 932);
loc_821A6938:
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// b 0x821a6908
	goto loc_821A6908;
loc_821A6940:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r11,r11,-10804
	r11.s64 = r11.s64 + -10804;
	// ld r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 4);
	// ld r9,484(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 484);
	// subf r4,r10,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r10.s64;
	// ld r10,236(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 236);
	// ld r11,716(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 716);
	// b 0x821a6938
	goto loc_821A6938;
loc_821A696C:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r11,r11,-10804
	r11.s64 = r11.s64 + -10804;
	// ld r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 4);
	// ld r9,484(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 484);
	// subf r4,r10,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r10.s64;
	// ld r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 92);
	// ld r11,572(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 572);
	// b 0x821a6938
	goto loc_821A6938;
loc_821A6998:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r11,r11,-10804
	r11.s64 = r11.s64 + -10804;
	// ld r10,460(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 460);
	// ld r9,940(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 940);
loc_821A69B4:
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// ld r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 4);
	// ld r11,484(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 484);
	// rldicr r3,r9,1,62
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// subf r4,r10,r11
	ctx.r4.s64 = r11.s64 - ctx.r10.s64;
	// b 0x821a6908
	goto loc_821A6908;
loc_821A69CC:
	// bl 0x821a6430
	sub_821A6430(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a69ec
	if (cr0.getEQ()) goto loc_821A69EC;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r11,r11,-10804
	r11.s64 = r11.s64 + -10804;
	// ld r10,468(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 468);
	// ld r9,948(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 948);
	// b 0x821a69b4
	goto loc_821A69B4;
loc_821A69EC:
	// lfs f1,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
loc_821A69F0:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f31,-32(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A6A10"))) PPC_WEAK_FUNC(sub_821A6A10);
PPC_FUNC_IMPL(__imp__sub_821A6A10) {
	PPC_FUNC_PROLOGUE();
	// b 0x8219d958
	sub_8219D958(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_821A6A18"))) PPC_WEAK_FUNC(sub_821A6A18);
PPC_FUNC_IMPL(__imp__sub_821A6A18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r5,40
	ctx.r5.s64 = 40;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lis r11,-32230
	r11.s64 = -2112225280;
	// lbz r10,10943(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// lis r30,-32256
	r30.s64 = -2113929216;
	// lwz r9,16544(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16544);
	// addi r11,r11,27152
	r11.s64 = r11.s64 + 27152;
	// ori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 | 1;
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// li r9,6274
	ctx.r9.s64 = 6274;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// stb r10,10943(r31)
	PPC_STORE_U8(r31.u32 + 10943, ctx.r10.u8);
	// lwz r10,1756(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 1756);
	// stw r3,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r3.u32);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a6aa8
	if (cr0.getEQ()) goto loc_821A6AA8;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,82
	ctx.r3.s64 = 82;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r10,1756(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 1756);
loc_821A6AA8:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x821a6b00
	if (!cr6.getEQ()) goto loc_821A6B00;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1772(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1772);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x821a6ae0
	if (cr6.getEQ()) goto loc_821A6AE0;
	// rotlwi r11,r9,0
	r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a6b00
	if (cr0.getEQ()) goto loc_821A6B00;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a6b00
	if (cr0.getEQ()) goto loc_821A6B00;
	// b 0x821a6af0
	goto loc_821A6AF0;
loc_821A6AE0:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a6b00
	if (cr0.getEQ()) goto loc_821A6B00;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
loc_821A6AF0:
	// lwz r4,16544(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 16544);
	// li r3,46
	ctx.r3.s64 = 46;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821A6B00:
	// lbz r11,10943(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stb r11,10943(r31)
	PPC_STORE_U8(r31.u32 + 10943, r11.u8);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A6B28"))) PPC_WEAK_FUNC(sub_821A6B28);
PPC_FUNC_IMPL(__imp__sub_821A6B28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1768(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1768);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x821a6ce0
	if (cr6.getEQ()) goto loc_821A6CE0;
	// rotlwi r31,r10,0
	r31.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 60);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a6ce0
	if (cr6.getEQ()) goto loc_821A6CE0;
	// ld r11,10880(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 10880);
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x821a6ce0
	if (cr6.getEQ()) goto loc_821A6CE0;
	// cmplwi cr6,r3,34
	cr6.compare<uint32_t>(ctx.r3.u32, 34, xer);
	// bgt cr6,0x821a6c04
	if (cr6.getGT()) goto loc_821A6C04;
	// beq cr6,0x821a6bfc
	if (cr6.getEQ()) goto loc_821A6BFC;
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// blt cr6,0x821a6bec
	if (cr6.getLT()) goto loc_821A6BEC;
	// beq cr6,0x821a6bec
	if (cr6.getEQ()) goto loc_821A6BEC;
	// cmplwi cr6,r3,16
	cr6.compare<uint32_t>(ctx.r3.u32, 16, xer);
	// beq cr6,0x821a6bc0
	if (cr6.getEQ()) goto loc_821A6BC0;
	// cmplwi cr6,r3,17
	cr6.compare<uint32_t>(ctx.r3.u32, 17, xer);
	// bne cr6,0x821a6ce0
	if (!cr6.getEQ()) goto loc_821A6CE0;
	// li r9,1
	ctx.r9.s64 = 1;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// cmplwi cr6,r4,6
	cr6.compare<uint32_t>(ctx.r4.u32, 6, xer);
	// lwz r10,-9796(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -9796);
	// slw r8,r9,r4
	ctx.r8.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r4.u8 & 0x3F));
	// or r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 | ctx.r10.u64;
	// stw r10,-9796(r11)
	PPC_STORE_U32(r11.u32 + -9796, ctx.r10.u32);
	// bne cr6,0x821a6ce0
	if (!cr6.getEQ()) goto loc_821A6CE0;
	// li r11,0
	r11.s64 = 0;
	// stw r9,21608(r31)
	PPC_STORE_U32(r31.u32 + 21608, ctx.r9.u32);
	// stw r11,21600(r31)
	PPC_STORE_U32(r31.u32 + 21600, r11.u32);
	// stw r11,21604(r31)
	PPC_STORE_U32(r31.u32 + 21604, r11.u32);
	// b 0x821a6ce0
	goto loc_821A6CE0;
loc_821A6BC0:
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r4,6
	cr6.compare<uint32_t>(ctx.r4.u32, 6, xer);
	// slw r9,r11,r4
	ctx.r9.u64 = ctx.r4.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r4.u8 & 0x3F));
	// lis r11,-31991
	r11.s64 = -2096562176;
	// lwz r10,-9796(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -9796);
	// andc r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r9.u64;
	// stw r10,-9796(r11)
	PPC_STORE_U32(r11.u32 + -9796, ctx.r10.u32);
	// bne cr6,0x821a6ce0
	if (!cr6.getEQ()) goto loc_821A6CE0;
	// li r11,0
	r11.s64 = 0;
loc_821A6BE4:
	// stw r11,21608(r31)
	PPC_STORE_U32(r31.u32 + 21608, r11.u32);
	// b 0x821a6ce0
	goto loc_821A6CE0;
loc_821A6BEC:
	// li r11,0
	r11.s64 = 0;
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// stw r11,-9796(r10)
	PPC_STORE_U32(ctx.r10.u32 + -9796, r11.u32);
	// b 0x821a6be4
	goto loc_821A6BE4;
loc_821A6BFC:
	// stw r4,21656(r31)
	PPC_STORE_U32(r31.u32 + 21656, ctx.r4.u32);
	// b 0x821a6ce0
	goto loc_821A6CE0;
loc_821A6C04:
	// cmplwi cr6,r3,224
	cr6.compare<uint32_t>(ctx.r3.u32, 224, xer);
	// beq cr6,0x821a6ca8
	if (cr6.getEQ()) goto loc_821A6CA8;
	// cmplwi cr6,r3,225
	cr6.compare<uint32_t>(ctx.r3.u32, 225, xer);
	// beq cr6,0x821a6ca8
	if (cr6.getEQ()) goto loc_821A6CA8;
	// cmplwi cr6,r3,226
	cr6.compare<uint32_t>(ctx.r3.u32, 226, xer);
	// beq cr6,0x821a6ca8
	if (cr6.getEQ()) goto loc_821A6CA8;
	// cmplwi cr6,r3,255
	cr6.compare<uint32_t>(ctx.r3.u32, 255, xer);
	// bne cr6,0x821a6ce0
	if (!cr6.getEQ()) goto loc_821A6CE0;
	// lis r28,-31991
	r28.s64 = -2096562176;
	// lwz r11,-9796(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + -9796);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a6c64
	if (cr0.getEQ()) goto loc_821A6C64;
	// lwz r11,21556(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21556);
	// lfs f0,21572(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 21572);
	f0.f64 = double(temp.f32);
	// lis r3,2
	ctx.r3.s64 = 131072;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// lfs f0,14108(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14108);
	f0.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * f0.f64));
	// bl 0x8235eab8
	sub_8235EAB8(ctx, base);
loc_821A6C64:
	// lis r11,-32019
	r11.s64 = -2098397184;
	// li r29,17
	r29.s64 = 17;
	// addi r11,r11,29400
	r11.s64 = r11.s64 + 29400;
	// addi r30,r11,8
	r30.s64 = r11.s64 + 8;
loc_821A6C74:
	// lwz r11,-8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + -8);
	// lwz r10,-9796(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + -9796);
	// and. r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a6c98
	if (cr0.getEQ()) goto loc_821A6C98;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x821a6548
	sub_821A6548(ctx, base);
	// lwz r3,-4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + -4);
	// bl 0x8235eab8
	sub_8235EAB8(ctx, base);
loc_821A6C98:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r30,r30,12
	r30.s64 = r30.s64 + 12;
	// bne 0x821a6c74
	if (!cr0.getEQ()) goto loc_821A6C74;
	// b 0x821a6ce0
	goto loc_821A6CE0;
loc_821A6CA8:
	// lwz r11,21516(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21516);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821a6ce0
	if (cr6.getEQ()) goto loc_821A6CE0;
	// bl 0x8235eaa8
	sub_8235EAA8(ctx, base);
	// lwz r11,10888(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10888);
	// cmplw cr6,r11,r3
	cr6.compare<uint32_t>(r11.u32, ctx.r3.u32, xer);
	// bne cr6,0x821a6ce0
	if (!cr6.getEQ()) goto loc_821A6CE0;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x821a6cdc
	if (!cr6.getGT()) goto loc_821A6CDC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821A6CDC:
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
loc_821A6CE0:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821A6CE8"))) PPC_WEAK_FUNC(sub_821A6CE8);
PPC_FUNC_IMPL(__imp__sub_821A6CE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r31,-32256
	r31.s64 = -2113929216;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lis r30,-32256
	r30.s64 = -2113929216;
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x821a6d2c
	if (cr6.getEQ()) goto loc_821A6D2C;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x821a6d58
	if (cr0.getEQ()) goto loc_821A6D58;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x821a6d58
	if (cr0.getEQ()) goto loc_821A6D58;
	// b 0x821a6d40
	goto loc_821A6D40;
loc_821A6D2C:
	// lwz r10,1756(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 1756);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x821a6d58
	if (cr0.getEQ()) goto loc_821A6D58;
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
loc_821A6D40:
	// lis r11,-32230
	r11.s64 = -2112225280;
	// li r3,28
	ctx.r3.s64 = 28;
	// addi r4,r11,25480
	ctx.r4.s64 = r11.s64 + 25480;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,1772(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1772);
loc_821A6D58:
	// lis r10,-32230
	ctx.r10.s64 = -2112225280;
	// li r9,2
	ctx.r9.s64 = 2;
	// addi r10,r10,27432
	ctx.r10.s64 = ctx.r10.s64 + 27432;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// lis r9,-31991
	ctx.r9.s64 = -2096562176;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// stw r10,-9796(r9)
	PPC_STORE_U32(ctx.r9.u32 + -9796, ctx.r10.u32);
	// beq cr6,0x821a6da0
	if (cr6.getEQ()) goto loc_821A6DA0;
	// rotlwi r11,r8,0
	r11.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a6dc4
	if (cr0.getEQ()) goto loc_821A6DC4;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a6dc4
	if (cr0.getEQ()) goto loc_821A6DC4;
	// b 0x821a6db4
	goto loc_821A6DB4;
loc_821A6DA0:
	// lwz r11,1756(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1756);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a6dc4
	if (cr0.getEQ()) goto loc_821A6DC4;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
loc_821A6DB4:
	// li r3,47
	ctx.r3.s64 = 47;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821A6DC4:
	// lis r11,-32230
	r11.s64 = -2112225280;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// addi r11,r11,25352
	r11.s64 = r11.s64 + 25352;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lwz r11,1756(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1756);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a6df8
	if (cr0.getEQ()) goto loc_821A6DF8;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// li r3,66
	ctx.r3.s64 = 66;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821A6DF8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821A6E00"))) PPC_WEAK_FUNC(sub_821A6E00);
PPC_FUNC_IMPL(__imp__sub_821A6E00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a6e34
	if (!cr6.getGT()) goto loc_821A6E34;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A6E34:
	// lis r10,-16382
	ctx.r10.s64 = -1073610752;
	// ori r9,r10,22528
	ctx.r9.u64 = ctx.r10.u64 | 22528;
	// lis r10,-32768
	ctx.r10.s64 = -2147483648;
	// ori r8,r10,3
	ctx.r8.u64 = ctx.r10.u64 | 3;
	// rlwinm r10,r30,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 12) & 0xFFF;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// clrlwi r9,r30,3
	ctx.r9.u64 = r30.u32 & 0x1FFFFFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// lis r9,-8531
	ctx.r9.s64 = -559087616;
	// ori r9,r9,48879
	ctx.r9.u64 = ctx.r9.u64 | 48879;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A6E90"))) PPC_WEAK_FUNC(sub_821A6E90);
PPC_FUNC_IMPL(__imp__sub_821A6E90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,70
	ctx.r4.s64 = 70;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a6eb8
	if (cr0.getEQ()) goto loc_821A6EB8;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
loc_821A6EB8:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A6ED0"))) PPC_WEAK_FUNC(sub_821A6ED0);
PPC_FUNC_IMPL(__imp__sub_821A6ED0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,21632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21632);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x821a6f94
	if (!cr6.getEQ()) goto loc_821A6F94;
	// lwz r11,21640(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21640);
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x821a6f08
	if (!cr6.getEQ()) goto loc_821A6F08;
	// li r11,0
	r11.s64 = 0;
	// b 0x821a6f0c
	goto loc_821A6F0C;
loc_821A6F08:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_821A6F0C:
	// lwz r10,21636(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21636);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x821a6f94
	if (cr6.getEQ()) goto loc_821A6F94;
	// rlwinm r30,r11,1,0,30
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x821a6f28
	if (!cr6.getEQ()) goto loc_821A6F28;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
loc_821A6F28:
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x821a6f44
	if (!cr6.getGT()) goto loc_821A6F44;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821A6F44:
	// lis r10,-16382
	ctx.r10.s64 = -1073610752;
	// lis r9,-32768
	ctx.r9.s64 = -2147483648;
	// ori r10,r10,22528
	ctx.r10.u64 = ctx.r10.u64 | 22528;
	// ori r8,r9,3
	ctx.r8.u64 = ctx.r9.u64 | 3;
	// rlwinm r9,r30,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r7,-8531
	ctx.r7.s64 = -559087616;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// ori r7,r7,48879
	ctx.r7.u64 = ctx.r7.u64 | 48879;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// lwz r10,21628(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21628);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r8,r10,12,20,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r9,r10,3
	ctx.r9.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r10,r8,512
	ctx.r10.s64 = ctx.r8.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// ori r10,r10,2
	ctx.r10.u64 = ctx.r10.u64 | 2;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
loc_821A6F94:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A6FB0"))) PPC_WEAK_FUNC(sub_821A6FB0);
PPC_FUNC_IMPL(__imp__sub_821A6FB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-384(r1)
	ea = -384 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r9,16704(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16704);
	// rlwinm. r11,r9,0,21,21
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x400;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a715c
	if (cr0.getEQ()) goto loc_821A715C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1756(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1756);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a6fec
	if (cr0.getEQ()) goto loc_821A6FEC;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// b 0x821a6ff0
	goto loc_821A6FF0;
loc_821A6FEC:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_821A6FF0:
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,16708(r31)
	PPC_STORE_U32(r31.u32 + 16708, ctx.r3.u32);
loc_821A7000:
	// stw r30,16704(r31)
	PPC_STORE_U32(r31.u32 + 16704, r30.u32);
loc_821A7004:
	// lwz r11,21564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21564);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// lwz r11,21560(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21560);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// mftb r11
	r11.u64 = __rdtsc();
	// ld r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// cmpdi cr6,r10,0
	cr6.compare<int64_t>(ctx.r10.s64, 0, xer);
	// beq cr6,0x821a7038
	if (cr6.getEQ()) goto loc_821A7038;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,21556(r31)
	PPC_STORE_U32(r31.u32 + 21556, r11.u32);
loc_821A7038:
	// ld r9,21576(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 21576);
	// ld r7,21584(r31)
	ctx.r7.u64 = PPC_LOAD_U64(r31.u32 + 21584);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,21632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21632);
	// std r30,21576(r31)
	PPC_STORE_U64(r31.u32 + 21576, r30.u64);
	// stw r9,21592(r31)
	PPC_STORE_U32(r31.u32 + 21592, ctx.r9.u32);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// stw r10,21564(r31)
	PPC_STORE_U32(r31.u32 + 21564, ctx.r10.u32);
	// stw r8,21560(r31)
	PPC_STORE_U32(r31.u32 + 21560, ctx.r8.u32);
	// stw r7,21596(r31)
	PPC_STORE_U32(r31.u32 + 21596, ctx.r7.u32);
	// std r30,21584(r31)
	PPC_STORE_U64(r31.u32 + 21584, r30.u64);
	// bne cr6,0x821a71c0
	if (!cr6.getEQ()) goto loc_821A71C0;
	// li r5,240
	ctx.r5.s64 = 240;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// stw r30,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r30.u32);
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// li r11,8
	r11.s64 = 8;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// li r11,13
	r11.s64 = 13;
	// stw r11,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r11.u32);
	// li r11,37
	r11.s64 = 37;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// li r11,19
	r11.s64 = 19;
	// stw r11,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r11.u32);
	// stw r11,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r11.u32);
	// li r11,6
	r11.s64 = 6;
	// stw r11,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, r11.u32);
	// li r11,25
	r11.s64 = 25;
	// stw r11,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, r11.u32);
	// li r11,26
	r11.s64 = 26;
	// stw r11,328(r1)
	PPC_STORE_U32(ctx.r1.u32 + 328, r11.u32);
	// li r11,200
	r11.s64 = 200;
	// stw r11,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r11.u32);
	// li r11,30
	r11.s64 = 30;
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// bne 0x821a70ec
	if (!cr0.getEQ()) goto loc_821A70EC;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219e840
	sub_8219E840(ctx, base);
loc_821A70EC:
	// addi r29,r31,21612
	r29.s64 = r31.s64 + 21612;
	// li r28,4
	r28.s64 = 4;
loc_821A70F4:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821a7110
	if (!cr6.getEQ()) goto loc_821A7110;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219e928
	sub_8219E928(ctx, base);
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
loc_821A7110:
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// bne 0x821a70f4
	if (!cr0.getEQ()) goto loc_821A70F4;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219e9c0
	sub_8219E9C0(ctx, base);
	// lwz r11,21628(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21628);
	// stw r30,21636(r31)
	PPC_STORE_U32(r31.u32 + 21636, r30.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r30,21640(r31)
	PPC_STORE_U32(r31.u32 + 21640, r30.u32);
	// bne cr6,0x821a7150
	if (!cr6.getEQ()) goto loc_821A7150;
	// lis r4,-23936
	ctx.r4.s64 = -1568669696;
	// li r3,32
	ctx.r3.s64 = 32;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// stw r3,21628(r31)
	PPC_STORE_U32(r31.u32 + 21628, ctx.r3.u32);
loc_821A7150:
	// li r11,2
	r11.s64 = 2;
	// stw r11,21632(r31)
	PPC_STORE_U32(r31.u32 + 21632, r11.u32);
	// b 0x821a7218
	goto loc_821A7218;
loc_821A715C:
	// rlwinm. r11,r9,0,23,23
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821a7004
	if (cr0.getEQ()) goto loc_821A7004;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1756(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1756);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a7180
	if (cr0.getEQ()) goto loc_821A7180;
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// b 0x821a7184
	goto loc_821A7184;
loc_821A7180:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_821A7184:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// rlwinm r4,r9,20,4,11
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 20) & 0xFF00000;
	// addi r3,r11,-14312
	ctx.r3.s64 = r11.s64 + -14312;
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,16708(r31)
	PPC_STORE_U32(r31.u32 + 16708, ctx.r3.u32);
	// blt 0x821a7000
	if (cr0.getLT()) goto loc_821A7000;
	// lwz r11,16704(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16704);
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwimi r11,r10,10,23,23
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 10) & 0x100) | (r11.u64 & 0xFFFFFFFFFFFFFEFF);
	// rlwimi r11,r10,10,21,21
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 10) & 0x400) | (r11.u64 & 0xFFFFFFFFFFFFFBFF);
	// stw r11,16704(r31)
	PPC_STORE_U32(r31.u32 + 16704, r11.u32);
	// b 0x821a7004
	goto loc_821A7004;
loc_821A71C0:
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x821a7218
	if (!cr6.getEQ()) goto loc_821A7218;
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a71e0
	if (!cr0.getEQ()) goto loc_821A71E0;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219e840
	sub_8219E840(ctx, base);
loc_821A71E0:
	// lwz r11,21640(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21640);
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x821a71f0
	if (cr6.getEQ()) goto loc_821A71F0;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_821A71F0:
	// lwz r11,21636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21636);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x821a7218
	if (cr6.getEQ()) goto loc_821A7218;
	// addi r11,r30,5403
	r11.s64 = r30.s64 + 5403;
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwzx r4,r11,r31
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// bl 0x8219efe0
	sub_8219EFE0(ctx, base);
	// stw r30,21640(r31)
	PPC_STORE_U32(r31.u32 + 21640, r30.u32);
loc_821A7218:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a6a18
	sub_821A6A18(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1772(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1772);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a724c
	if (cr0.getEQ()) goto loc_821A724C;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a724c
	if (cr0.getEQ()) goto loc_821A724C;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821A724C:
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821A7258"))) PPC_WEAK_FUNC(sub_821A7258);
PPC_FUNC_IMPL(__imp__sub_821A7258) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r5,255
	ctx.r5.s64 = 255;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,81
	ctx.r3.s64 = ctx.r1.s64 + 81;
	// stb r11,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r11.u8);
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// li r5,256
	ctx.r5.s64 = 256;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823f0600
	sub_823F0600(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1756(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1756);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a72c0
	if (cr0.getEQ()) goto loc_821A72C0;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,84
	ctx.r3.s64 = 84;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x821a72c4
	goto loc_821A72C4;
loc_821A72C0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821A72C4:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821a72d4
	if (cr6.getEQ()) goto loc_821A72D4;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x821a72dc
	goto loc_821A72DC;
loc_821A72D4:
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
loc_821A72DC:
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A72F0"))) PPC_WEAK_FUNC(sub_821A72F0);
PPC_FUNC_IMPL(__imp__sub_821A72F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r7,12(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
	// stw r7,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r7.u32);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x821a7364
	if (!cr6.getEQ()) goto loc_821A7364;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a7364
	if (!cr6.getEQ()) goto loc_821A7364;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x821a7364
	if (!cr6.getEQ()) goto loc_821A7364;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a7364
	if (!cr6.getEQ()) goto loc_821A7364;
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
loc_821A7364:
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r8,92(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// subf r29,r9,r8
	r29.s64 = ctx.r8.s64 - ctx.r9.s64;
	// mr r27,r11
	r27.u64 = r11.u64;
	// bne 0x821a7388
	if (!cr0.getEQ()) goto loc_821A7388;
	// lwz r27,21524(r26)
	r27.u64 = PPC_LOAD_U32(r26.u32 + 21524);
loc_821A7388:
	// lwz r30,20(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// bne 0x821a73d0
	if (!cr0.getEQ()) goto loc_821A73D0;
	// lwz r11,13572(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 13572);
	// mullw r9,r27,r29
	ctx.r9.s64 = int64_t(r27.s32) * int64_t(r29.s32);
	// divwu r30,r9,r10
	r30.u32 = ctx.r9.u32 / ctx.r10.u32;
	// twllei r10,0
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a73cc
	if (!cr0.getEQ()) goto loc_821A73CC;
	// bl 0x8240fb2c
	__imp__VdQueryVideoFlags(ctx, base);
	// clrlwi. r11,r3,31
	r11.u64 = ctx.r3.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a73cc
	if (!cr0.getEQ()) goto loc_821A73CC;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// bgt cr6,0x821a73cc
	if (cr6.getGT()) goto loc_821A73CC;
	// lwz r11,21528(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 21528);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x821a73d0
	if (!cr6.getGT()) goto loc_821A73D0;
loc_821A73CC:
	// lwz r30,21528(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 21528);
loc_821A73D0:
	// li r5,56
	ctx.r5.s64 = 56;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// stw r27,16(r28)
	PPC_STORE_U32(r28.u32 + 16, r27.u32);
	// stw r30,20(r28)
	PPC_STORE_U32(r28.u32 + 20, r30.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// stw r9,4(r28)
	PPC_STORE_U32(r28.u32 + 4, ctx.r9.u32);
	// stw r8,8(r28)
	PPC_STORE_U32(r28.u32 + 8, ctx.r8.u32);
	// stw r11,12(r28)
	PPC_STORE_U32(r28.u32 + 12, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821A7418"))) PPC_WEAK_FUNC(sub_821A7418);
PPC_FUNC_IMPL(__imp__sub_821A7418) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r11,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, r11.u32);
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// stw r11,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, r11.u32);
	// lwz r11,12(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// stw r11,12(r5)
	PPC_STORE_U32(ctx.r5.u32 + 12, r11.u32);
	// lwz r11,16(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// stw r11,16(r5)
	PPC_STORE_U32(ctx.r5.u32 + 16, r11.u32);
	// lwz r11,20(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// stw r11,20(r5)
	PPC_STORE_U32(ctx.r5.u32 + 20, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// lwz r10,24(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x821a7464
	if (!cr6.getEQ()) goto loc_821A7464;
	// stw r11,24(r5)
	PPC_STORE_U32(ctx.r5.u32 + 24, r11.u32);
	// stw r11,40(r5)
	PPC_STORE_U32(ctx.r5.u32 + 40, r11.u32);
	// b 0x821a7470
	goto loc_821A7470;
loc_821A7464:
	// li r10,3
	ctx.r10.s64 = 3;
	// stw r10,24(r5)
	PPC_STORE_U32(ctx.r5.u32 + 24, ctx.r10.u32);
	// stw r10,40(r5)
	PPC_STORE_U32(ctx.r5.u32 + 40, ctx.r10.u32);
loc_821A7470:
	// stw r11,28(r5)
	PPC_STORE_U32(ctx.r5.u32 + 28, r11.u32);
	// stw r11,32(r5)
	PPC_STORE_U32(ctx.r5.u32 + 32, r11.u32);
	// stw r11,36(r5)
	PPC_STORE_U32(ctx.r5.u32 + 36, r11.u32);
	// stw r11,44(r5)
	PPC_STORE_U32(ctx.r5.u32 + 44, r11.u32);
	// stw r11,48(r5)
	PPC_STORE_U32(ctx.r5.u32 + 48, r11.u32);
	// stw r11,52(r5)
	PPC_STORE_U32(ctx.r5.u32 + 52, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A7490"))) PPC_WEAK_FUNC(sub_821A7490);
PPC_FUNC_IMPL(__imp__sub_821A7490) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// bl 0x821a72f0
	sub_821A72F0(ctx, base);
	// lwz r11,21524(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 21524);
	// addi r3,r1,152
	ctx.r3.s64 = ctx.r1.s64 + 152;
	// lwz r10,21528(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21528);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r5,56
	ctx.r5.s64 = 56;
	// sth r30,144(r1)
	PPC_STORE_U16(ctx.r1.u32 + 144, r30.u16);
	// sth r29,146(r1)
	PPC_STORE_U16(ctx.r1.u32 + 146, r29.u16);
	// sth r11,148(r1)
	PPC_STORE_U16(ctx.r1.u32 + 148, r11.u16);
	// sth r10,150(r1)
	PPC_STORE_U16(ctx.r1.u32 + 150, ctx.r10.u16);
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x8240fb3c
	__imp__VdCallGraphicsNotificationRoutines(ctx, base);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821A74F0"))) PPC_WEAK_FUNC(sub_821A74F0);
PPC_FUNC_IMPL(__imp__sub_821A74F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// vrfim v11,v1
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_round_ps(_mm_load_ps(ctx.v1.f32), _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC));
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r11,r11,-14272
	r11.s64 = r11.s64 + -14272;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32254
	r11.s64 = -2113798144;
	// vspltw v12,v0,0
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v10,v0,1
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xAA));
	// addi r11,r11,-14288
	r11.s64 = r11.s64 + -14288;
	// vspltw v7,v0,2
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x55));
	// vspltw v5,v0,3
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// vsubfp v0,v1,v11
	_mm_store_ps(ctx.v0.f32, _mm_sub_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v11.f32)));
	// vexptefp v11,v11
	ctx.v11.f32[0] = exp2f(ctx.v11.f32[0]);
	ctx.v11.f32[1] = exp2f(ctx.v11.f32[1]);
	ctx.v11.f32[2] = exp2f(ctx.v11.f32[2]);
	ctx.v11.f32[3] = exp2f(ctx.v11.f32[3]);
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltw v8,v13,1
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xAA));
	// vspltw v9,v13,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// vspltw v6,v13,2
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0x55));
	// vspltw v4,v13,3
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0x0));
	// vmaddfp v10,v0,v10,v12
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v13,v0,v0
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v9,v0,v8,v9
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v0,v0,v13
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// vmaddfp v10,v13,v7,v10
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v7.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v9,v13,v6,v9
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v6.f32)), _mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v13,v13,v13
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v13.f32)));
	// vmaddfp v10,v0,v5,v10
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v5.f32)), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v0,v0,v4,v9
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v4.f32)), _mm_load_ps(ctx.v9.f32)));
	// vmaddfp v0,v13,v0,v10
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v10.f32)));
	// vor v13,v0,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vor v10,v0,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vrefp v0,v0
	_mm_store_ps(ctx.v0.f32, _mm_div_ps(_mm_set1_ps(1), _mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v13,v13,v0,v12
	_mm_store_ps(ctx.v13.f32, _mm_xor_ps(_mm_sub_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v12.f32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x80000000)))));
	// vor v8,v0,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vmaddfp v0,v0,v13,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v13,v10,v0,v12
	_mm_store_ps(ctx.v13.f32, _mm_xor_ps(_mm_sub_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v12.f32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x80000000)))));
	// vcmpeqfp v12,v0,v0
	_mm_store_ps(ctx.v12.f32, _mm_cmpeq_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v0,v0,v13,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v0.f32)));
	// vsel v0,v8,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8))));
	// vmulfp128 v1,v11,v0
	_mm_store_ps(ctx.v1.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v0.f32)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A7590"))) PPC_WEAK_FUNC(sub_821A7590);
PPC_FUNC_IMPL(__imp__sub_821A7590) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r11,292
	r11.s64 = 19136512;
	// stfs f1,-48(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// addi r9,r1,-48
	ctx.r9.s64 = ctx.r1.s64 + -48;
	// ori r10,r11,16237
	ctx.r10.u64 = r11.u64 | 16237;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,6588(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 6588);
	f0.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,2776(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f13,-32(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r11,r11,2160
	r11.s64 = r11.s64 + 2160;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r11,r11,2176
	r11.s64 = r11.s64 + 2176;
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r11,r11,2144
	r11.s64 = r11.s64 + 2144;
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,-48(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// rlwimi r11,r10,30,1,31
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 30) & 0x7FFFFFFF) | (r11.u64 & 0xFFFFFFFF80000000);
	// stw r11,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, r11.u32);
	// lfs f13,-48(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f1
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f1.f64));
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f13,-20188(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -20188);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fctiwz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f13.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// lwa r11,-48(r1)
	r11.s64 = int32_t(PPC_LOAD_U32(ctx.r1.u32 + -48));
	// std r11,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, r11.u64);
	// lfd f13,-48(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fnmsubs f0,f13,f0,f1
	f0.f64 = double(float(-(ctx.f13.f64 * f0.f64 - ctx.f1.f64)));
	// stfs f0,-28(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// fmuls f13,f0,f0
	ctx.f13.f64 = double(float(f0.f64 * f0.f64));
	// stfs f13,-24(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -24, temp.u32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f0,-20(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -20, temp.u32);
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// vspltw v13,v0,1
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xAA));
	// vmulfp128 v0,v0,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v0,v0,v13
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// vspltw v9,v0,3
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// vmsum4fp128 v10,v0,v10
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v10.f32), 0xFF));
	// vmulfp128 v13,v9,v13
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v13.f32)));
	// stvx128 v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// vmulfp128 v0,v0,v13
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v13,v0,v13
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// vmsum4fp128 v0,v0,v12
	_mm_store_ps(ctx.v0.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v12.f32), 0xFF));
	// vmsum4fp128 v13,v13,v11
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v11.f32), 0xFF));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-48
	r11.s64 = ctx.r1.s64 + -48;
	// lfs f0,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	f0.f64 = double(temp.f32);
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,-48(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// fadds f1,f0,f13
	ctx.f1.f64 = double(float(f0.f64 + ctx.f13.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A7690"))) PPC_WEAK_FUNC(sub_821A7690);
PPC_FUNC_IMPL(__imp__sub_821A7690) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister f0{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	PPCVRegister vTemp{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f1,20(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 20, temp.u32);
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// stfs f2,28(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 28, temp.u32);
	// addi r9,r1,-28
	ctx.r9.s64 = ctx.r1.s64 + -28;
	// vspltisw v13,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_set1_epi32(int(0x0)));
	// addi r8,r1,20
	ctx.r8.s64 = ctx.r1.s64 + 20;
	// vspltisw v11,-1
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_set1_epi32(int(0xFFFFFFFF)));
	// addi r7,r1,-20
	ctx.r7.s64 = ctx.r1.s64 + -20;
	// vspltisw v4,-9
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_set1_epi32(int(0xFFFFFFF7)));
	// lfs f0,2688(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	f0.f64 = double(temp.f32);
	// addi r11,r1,-24
	r11.s64 = ctx.r1.s64 + -24;
	// stfs f0,-32(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// stfs f0,-28(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// vslw v3,v11,v11
	ctx.v3.u32[0] = ctx.v11.u32[0] << (ctx.v11.u8[0] & 0x1F);
	ctx.v3.u32[1] = ctx.v11.u32[1] << (ctx.v11.u8[4] & 0x1F);
	ctx.v3.u32[2] = ctx.v11.u32[2] << (ctx.v11.u8[8] & 0x1F);
	ctx.v3.u32[3] = ctx.v11.u32[3] << (ctx.v11.u8[12] & 0x1F);
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,-20
	ctx.r10.s64 = ctx.r1.s64 + -20;
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,28
	ctx.r9.s64 = ctx.r1.s64 + 28;
	// stfs f0,-24(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -24, temp.u32);
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v10,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,-24
	r11.s64 = ctx.r1.s64 + -24;
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v10,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// stfs f0,-20(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -20, temp.u32);
	// lvlx v10,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,-20(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -20, temp.u32);
	// stfs f0,-24(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -24, temp.u32);
	// lvlx v9,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// lvlx v8,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lvlx v7,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v8,v10,4,3
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// vrlimi128 v7,v9,4,3
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 57), 4));
	// addi r11,r11,-14240
	r11.s64 = r11.s64 + -14240;
	// vor v12,v8,v8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// vor v10,v7,v7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// vor v8,v0,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltw v7,v9,0
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xFF));
	// lis r11,-32254
	r11.s64 = -2113798144;
	// vspltw v5,v9,1
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xAA));
	// vrlimi128 v10,v12,3,2
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// vupkd3d128 v12,v13,4
	temp.f32 = 3.0f;
	temp.s32 += ctx.v13.s16[1];
	vTemp.f32[3] = temp.f32;
	temp.f32 = 3.0f;
	temp.s32 += ctx.v13.s16[0];
	vTemp.f32[2] = temp.f32;
	vTemp.f32[1] = 0.0f;
	vTemp.f32[0] = 1.0f;
	ctx.v12 = vTemp;
	// addi r11,r11,-14256
	r11.s64 = r11.s64 + -14256;
	// vspltw v29,v9,2
	_mm_store_si128((__m128i*)v29.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0x55));
	// vspltw v6,v12,3
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0x0));
	// vandc v12,v8,v3
	// vor v0,v10,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vslw v10,v11,v4
	ctx.v10.u32[0] = ctx.v11.u32[0] << (ctx.v4.u8[0] & 0x1F);
	ctx.v10.u32[1] = ctx.v11.u32[1] << (ctx.v4.u8[4] & 0x1F);
	ctx.v10.u32[2] = ctx.v11.u32[2] << (ctx.v4.u8[8] & 0x1F);
	ctx.v10.u32[3] = ctx.v11.u32[3] << (ctx.v4.u8[12] & 0x1F);
	// vlogefp v2,v12
	ctx.fpscr.enableFlushModeUnconditional();
	ctx.v2.f32[0] = log2f(ctx.v12.f32[0]);
	ctx.v2.f32[1] = log2f(ctx.v12.f32[1]);
	ctx.v2.f32[2] = log2f(ctx.v12.f32[2]);
	ctx.v2.f32[3] = log2f(ctx.v12.f32[3]);
	// vsel v10,v12,v6,v10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v12.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v6.u8))));
	// vsubfp v12,v10,v6
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v6.f32)));
	// vrfim v10,v2
	_mm_store_ps(ctx.v10.f32, _mm_round_ps(_mm_load_ps(ctx.v2.f32), _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC));
	// vmaddfp v5,v12,v5,v7
	_mm_store_ps(ctx.v5.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v5.f32)), _mm_load_ps(ctx.v7.f32)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32254
	r11.s64 = -2113798144;
	// vspltw v30,v7,1
	_mm_store_si128((__m128i*)v30.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0xAA));
	// vspltw v31,v7,0
	_mm_store_si128((__m128i*)v31.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0xFF));
	// addi r11,r11,-14272
	r11.s64 = r11.s64 + -14272;
	// vspltw v28,v7,2
	_mm_store_si128((__m128i*)v28.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0x55));
	// vspltw v27,v7,3
	_mm_store_si128((__m128i*)v27.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0x0));
	// vmulfp128 v1,v10,v0
	_mm_store_ps(ctx.v1.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v10,v12,v12
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v31,v12,v30,v31
	_mm_store_ps(v31.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v30.f32)), _mm_load_ps(v31.f32)));
	// vmaddfp v30,v10,v29,v5
	_mm_store_ps(v30.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(v29.f32)), _mm_load_ps(ctx.v5.f32)));
	// vspltw v29,v9,3
	_mm_store_si128((__m128i*)v29.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0x0));
	// lvx128 v9,r0,r11
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32254
	r11.s64 = -2113798144;
	// vmulfp128 v2,v12,v10
	_mm_store_ps(ctx.v2.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v10.f32)));
	// vspltw v5,v9,0
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xFF));
	// addi r11,r11,-14288
	r11.s64 = r11.s64 + -14288;
	// vmulfp128 v26,v10,v10
	_mm_store_ps(v26.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v10,v10,v28,v31
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(v28.f32)), _mm_load_ps(v31.f32)));
	// vspltw v25,v9,1
	_mm_store_si128((__m128i*)v25.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xAA));
	// vmulfp128 v12,v12,v0
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32)));
	// lvx128 v7,r0,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltw v31,v7,0
	_mm_store_si128((__m128i*)v31.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0xFF));
	// vspltw v28,v7,1
	_mm_store_si128((__m128i*)v28.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0xAA));
	// vmaddfp v10,v2,v27,v10
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(v27.f32)), _mm_load_ps(ctx.v10.f32)));
	// vspltw v24,v7,2
	_mm_store_si128((__m128i*)v24.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0x55));
	// vmaddfp v30,v2,v29,v30
	_mm_store_ps(v30.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(v29.f32)), _mm_load_ps(v30.f32)));
	// vspltw v29,v9,2
	_mm_store_si128((__m128i*)v29.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0x55));
	// vspltw v9,v9,3
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0x0));
	// vand v3,v8,v3
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v3.u8)));
	// vspltw v7,v7,3
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0x0));
	// vslw v4,v11,v4
	ctx.v4.u32[0] = ctx.v11.u32[0] << (ctx.v4.u8[0] & 0x1F);
	ctx.v4.u32[1] = ctx.v11.u32[1] << (ctx.v4.u8[4] & 0x1F);
	ctx.v4.u32[2] = ctx.v11.u32[2] << (ctx.v4.u8[8] & 0x1F);
	ctx.v4.u32[3] = ctx.v11.u32[3] << (ctx.v4.u8[12] & 0x1F);
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// vmaddfp v10,v26,v10,v30
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v26.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(v30.f32)));
	// vcmpgtfp v30,v13,v0
	_mm_store_ps(v30.f32, _mm_cmpgt_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v12,v12,v10,v1
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(ctx.v1.f32)));
	// vrfim v10,v12
	_mm_store_ps(ctx.v10.f32, _mm_round_ps(_mm_load_ps(ctx.v12.f32), _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC));
	// vsubfp v12,v12,v10
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v10.f32)));
	// vexptefp v2,v10
	ctx.v2.f32[0] = exp2f(ctx.v10.f32[0]);
	ctx.v2.f32[1] = exp2f(ctx.v10.f32[1]);
	ctx.v2.f32[2] = exp2f(ctx.v10.f32[2]);
	ctx.v2.f32[3] = exp2f(ctx.v10.f32[3]);
	// vmulfp128 v10,v12,v12
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v12.f32)));
	// vmaddfp v1,v12,v25,v5
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v25.f32)), _mm_load_ps(ctx.v5.f32)));
	// vmaddfp v31,v12,v28,v31
	_mm_store_ps(v31.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v28.f32)), _mm_load_ps(v31.f32)));
	// vmulfp128 v12,v12,v10
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v1,v10,v29,v1
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(v29.f32)), _mm_load_ps(ctx.v1.f32)));
	// vmaddfp v31,v10,v24,v31
	_mm_store_ps(v31.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(v24.f32)), _mm_load_ps(v31.f32)));
	// vmulfp128 v10,v10,v10
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v10.f32)));
	// vmaddfp v9,v12,v9,v1
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v1.f32)));
	// vmaddfp v12,v12,v7,v31
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v7.f32)), _mm_load_ps(v31.f32)));
	// vctsxs v1,v0,0
	_mm_store_si128((__m128i*)ctx.v1.s32, _mm_vctsxs(_mm_load_ps(ctx.v0.f32)));
	// vcmpeqfp v7,v0,v13
	_mm_store_ps(ctx.v7.f32, _mm_cmpeq_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// vmaddfp v12,v10,v12,v9
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v9.f32)));
	// vspltisw v10,1
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_set1_epi32(int(0x1)));
	// vrfiz v9,v0
	_mm_store_ps(ctx.v9.f32, _mm_round_ps(_mm_load_ps(ctx.v0.f32), _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC));
	// vand v1,v1,v10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v10.u8)));
	// vsrw v10,v4,v10
	ctx.v10.u32[0] = ctx.v4.u32[0] >> (ctx.v10.u8[0] & 0x1F);
	ctx.v10.u32[1] = ctx.v4.u32[1] >> (ctx.v10.u8[4] & 0x1F);
	ctx.v10.u32[2] = ctx.v4.u32[2] >> (ctx.v10.u8[8] & 0x1F);
	ctx.v10.u32[3] = ctx.v4.u32[3] >> (ctx.v10.u8[12] & 0x1F);
	// vslw v11,v1,v11
	ctx.v11.u32[0] = ctx.v1.u32[0] << (ctx.v11.u8[0] & 0x1F);
	ctx.v11.u32[1] = ctx.v1.u32[1] << (ctx.v11.u8[4] & 0x1F);
	ctx.v11.u32[2] = ctx.v1.u32[2] << (ctx.v11.u8[8] & 0x1F);
	ctx.v11.u32[3] = ctx.v1.u32[3] << (ctx.v11.u8[12] & 0x1F);
	// vor v29,v12,v12
	_mm_store_si128((__m128i*)v29.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vcmpeqfp v31,v0,v9
	_mm_store_ps(v31.f32, _mm_cmpeq_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v9.f32)));
	// vor v1,v12,v12
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vcmpeqfp v9,v8,v13
	_mm_store_ps(ctx.v9.f32, _mm_cmpeq_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v13.f32)));
	// vcmpgtfp v8,v13,v8
	_mm_store_ps(ctx.v8.f32, _mm_cmpgt_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v8.f32)));
	// vrefp v0,v12
	_mm_store_ps(ctx.v0.f32, _mm_div_ps(_mm_set1_ps(1), _mm_load_ps(ctx.v12.f32)));
	// vand v12,v3,v11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// vor v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vandc v11,v8,v31
	// vandc v8,v9,v30
	// vor v9,v9,v7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsel v13,v10,v13,v8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v10.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8))));
	// vnmsubfp v10,v29,v0,v5
	_mm_store_ps(ctx.v10.f32, _mm_xor_ps(_mm_sub_ps(_mm_mul_ps(_mm_load_ps(v29.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v5.f32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x80000000)))));
	// vor v11,v11,v9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v9.u8)));
	// vor v9,v0,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vsel v13,v13,v6,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v6.u8))));
	// vmaddfp v0,v0,v10,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v10,v1,v0,v5
	_mm_store_ps(ctx.v10.f32, _mm_xor_ps(_mm_sub_ps(_mm_mul_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v5.f32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x80000000)))));
	// vcmpeqfp v8,v0,v0
	_mm_store_ps(ctx.v8.f32, _mm_cmpeq_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp v0,v0,v10,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(ctx.v0.f32)));
	// vsel v0,v9,v0,v8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v9.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8))));
	// vmulfp128 v0,v2,v0
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v0.f32)));
	// vor v0,v0,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vsel v0,v0,v13,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v13.u8))));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f1,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A78C0"))) PPC_WEAK_FUNC(sub_821A78C0);
PPC_FUNC_IMPL(__imp__sub_821A78C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f1,20(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 20, temp.u32);
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// vspltisw v0,1
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x1)));
	// addi r9,r1,-24
	ctx.r9.s64 = ctx.r1.s64 + -24;
	// addi r8,r1,20
	ctx.r8.s64 = ctx.r1.s64 + 20;
	// lfs f0,2688(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	f0.f64 = double(temp.f32);
	// addi r11,r1,-28
	r11.s64 = ctx.r1.s64 + -28;
	// stfs f0,-32(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// vcfsx v12,v0,1
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_cvtepi32_ps(_mm_load_si128((__m128i*)ctx.v0.u32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x3F000000)))));
	// stfs f0,-28(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,-24(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -24, temp.u32);
	// lvlx v11,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// vrlimi128 v11,v13,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v10,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// vrlimi128 v13,v11,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// vrsqrtefp v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v0.f32, _mm_div_ps(_mm_set1_ps(1), _mm_sqrt_ps(_mm_load_ps(ctx.v13.f32))));
	// vor v11,v13,v13
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vmulfp128 v10,v13,v12
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v9,v0,v0
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32)));
	// vcmpeqfp v8,v0,v0
	_mm_store_ps(ctx.v8.f32, _mm_cmpeq_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v12,v10,v9,v12
	_mm_store_ps(ctx.v12.f32, _mm_xor_ps(_mm_sub_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v12.f32)), _mm_castsi128_ps(_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v0,v0,v12,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v0.f32)));
	// vcmpeqfp v12,v12,v12
	_mm_store_ps(ctx.v12.f32, _mm_cmpeq_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v0,v13,v0
	_mm_store_ps(ctx.v0.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// vxor v13,v12,v8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vsel v0,v0,v11,v13
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v11.u8))));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f1,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A7948"))) PPC_WEAK_FUNC(sub_821A7948);
PPC_FUNC_IMPL(__imp__sub_821A7948) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-992(r1)
	ea = -992 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mullw. r11,r30,r29
	r11.s64 = int64_t(r30.s32) * int64_t(r29.s32);
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x821a7978
	if (!cr0.getGT()) goto loc_821A7978;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x823ee498
	sub_823EE498(ctx, base);
loc_821A7978:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x821a79a0
	if (!cr6.getGT()) goto loc_821A79A0;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// li r10,-1
	ctx.r10.s64 = -1;
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x821a79a0
	if (cr0.getEQ()) goto loc_821A79A0;
	// mtctr r30
	ctr.u64 = r30.u64;
loc_821A7994:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x821a7994
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_821A7994;
loc_821A79A0:
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// li r28,0
	r28.s64 = 0;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// li r3,1
	ctx.r3.s64 = 1;
	// stwx r28,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, r28.u32);
	// stwx r28,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, r28.u32);
	// beq cr6,0x821a7a0c
	if (cr6.getEQ()) goto loc_821A7A0C;
	// addi r11,r30,2
	r11.s64 = r30.s64 + 2;
	// addi r10,r30,-3
	ctx.r10.s64 = r30.s64 + -3;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, ctx.r3.u32);
	// stwx r3,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r3.u32);
loc_821A7A0C:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x821a7a3c
	if (!cr6.getGT()) goto loc_821A7A3C;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_821A7A1C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r9,-1
	cr6.compare<int32_t>(ctx.r9.s32, -1, xer);
	// bne cr6,0x821a7a30
	if (!cr6.getEQ()) goto loc_821A7A30;
	// li r9,2
	ctx.r9.s64 = 2;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_821A7A30:
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne 0x821a7a1c
	if (!cr0.getEQ()) goto loc_821A7A1C;
loc_821A7A3C:
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// ble cr6,0x821a7d98
	if (!cr6.getGT()) goto loc_821A7D98;
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// subf r4,r31,r11
	ctx.r4.s64 = r11.s64 - r31.s64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lfs f12,14108(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 14108);
	ctx.f12.f64 = double(temp.f32);
	// mr r31,r29
	r31.u64 = r29.u64;
	// li r6,4
	ctx.r6.s64 = 4;
	// lfd f10,28168(r10)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r10.u32 + 28168);
	// lfs f11,2688(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	ctx.f11.f64 = double(temp.f32);
loc_821A7A70:
	// li r7,512
	ctx.r7.s64 = 512;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x821a7b3c
	if (!cr6.getGT()) goto loc_821A7B3C;
	// addi r9,r1,240
	ctx.r9.s64 = ctx.r1.s64 + 240;
	// rlwinm r8,r29,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r4,r5
	ctx.r10.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_821A7A8C:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne 0x821a7a8c
	if (!cr0.getEQ()) goto loc_821A7A8C;
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
loc_821A7AAC:
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r27,r10,7
	r27.s64 = ctx.r10.s64 + 7;
	// slw r27,r3,r27
	r27.u64 = r27.u8 & 0x20 ? 0 : (ctx.r3.u32 << (r27.u8 & 0x3F));
	// extsw r27,r27
	r27.s64 = r27.s32;
	// ble cr6,0x821a7af0
	if (!cr6.getGT()) goto loc_821A7AF0;
	// std r27,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, r27.u64);
	// lfd f13,176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fadd f0,f0,f10
	f0.f64 = f0.f64 + ctx.f10.f64;
	// b 0x821a7b08
	goto loc_821A7B08;
loc_821A7AF0:
	// std r27,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r27.u64);
	// lfd f13,152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fsub f0,f0,f10
	f0.f64 = f0.f64 - ctx.f10.f64;
loc_821A7B08:
	// fctiwz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, f0.u32);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subfic r8,r10,2
	xer.ca = ctx.r10.u32 <= 2;
	ctx.r8.s64 = 2 - ctx.r10.s64;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r27,r1,192
	r27.s64 = ctx.r1.s64 + 192;
	// stwx r10,r11,r27
	PPC_STORE_U32(r11.u32 + r27.u32, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// slw r10,r10,r8
	ctx.r10.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r8.u8 & 0x3F));
	// subf r7,r10,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r10.s64;
	// bne 0x821a7aac
	if (!cr0.getEQ()) goto loc_821A7AAC;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x821a7c2c
	if (!cr6.getGT()) goto loc_821A7C2C;
loc_821A7B3C:
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// fmr f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f12.f64;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x821a7c00
	if (!cr6.getGT()) goto loc_821A7C00;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_821A7B54:
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// fabs f9,f0
	ctx.f9.u64 = f0.u64 & ~0x8000000000000000;
	// fcmpu cr6,f9,f11
	cr6.compare(ctx.f9.f64, ctx.f11.f64);
	// ble cr6,0x821a7bb8
	if (!cr6.getGT()) goto loc_821A7BB8;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r27,r1,192
	r27.s64 = ctx.r1.s64 + 192;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r10,r10,7
	ctx.r10.s64 = ctx.r10.s64 + 7;
	// lwax r27,r11,r27
	r27.s64 = int32_t(PPC_LOAD_U32(r11.u32 + r27.u32));
	// slw r10,r3,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r10.u8 & 0x3F));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r27,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, r27.u64);
	// std r10,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r10.u64);
	// lfd f9,168(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 168);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// lfd f8,160(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// fmuls f0,f8,f0
	f0.f64 = double(float(ctx.f8.f64 * f0.f64));
	// fsubs f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 - f0.f64));
	// fdivs f0,f9,f0
	f0.f64 = double(float(ctx.f9.f64 / f0.f64));
	// fabs f0,f0
	f0.u64 = f0.u64 & ~0x8000000000000000;
	// b 0x821a7bbc
	goto loc_821A7BBC;
loc_821A7BB8:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f12.f64;
loc_821A7BBC:
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// bge cr6,0x821a7bf0
	if (!cr6.getLT()) goto loc_821A7BF0;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// sraw r10,r6,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	ctx.r10.s64 = ctx.r6.s32 >> temp.u32;
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// blt cr6,0x821a7bf0
	if (cr6.getLT()) goto loc_821A7BF0;
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x821a7bf0
	if (!cr6.getLT()) goto loc_821A7BF0;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
loc_821A7BF0:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r9,r30
	cr6.compare<int32_t>(ctx.r9.s32, r30.s32, xer);
	// blt cr6,0x821a7b54
	if (cr6.getLT()) goto loc_821A7B54;
loc_821A7C00:
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// lwzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// sraw r9,r6,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	ctx.r9.s64 = ctx.r6.s32 >> temp.u32;
	// subf. r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// bgt 0x821a7b3c
	if (cr0.getGT()) goto loc_821A7B3C;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
loc_821A7C2C:
	// bge cr6,0x821a7d48
	if (!cr6.getLT()) goto loc_821A7D48;
loc_821A7C30:
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// fmr f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f12.f64;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x821a7d20
	if (!cr6.getGT()) goto loc_821A7D20;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_821A7C48:
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// fabs f9,f0
	ctx.f9.u64 = f0.u64 & ~0x8000000000000000;
	// fcmpu cr6,f9,f11
	cr6.compare(ctx.f9.f64, ctx.f11.f64);
	// ble cr6,0x821a7cb4
	if (!cr6.getGT()) goto loc_821A7CB4;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r27,r1,192
	r27.s64 = ctx.r1.s64 + 192;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r26,r10,7
	r26.s64 = ctx.r10.s64 + 7;
	// lwzx r10,r11,r27
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r10.u64);
	// slw r10,r3,r26
	ctx.r10.u64 = r26.u8 & 0x20 ? 0 : (ctx.r3.u32 << (r26.u8 & 0x3F));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r10.u64);
	// lfd f9,136(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// lfd f8,144(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// fmuls f0,f8,f0
	f0.f64 = double(float(ctx.f8.f64 * f0.f64));
	// fsubs f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 - f0.f64));
	// fdivs f0,f9,f0
	f0.f64 = double(float(ctx.f9.f64 / f0.f64));
	// fabs f0,f0
	f0.u64 = f0.u64 & ~0x8000000000000000;
	// b 0x821a7cb8
	goto loc_821A7CB8;
loc_821A7CB4:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f12.f64;
loc_821A7CB8:
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// sraw r27,r6,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	r27.s64 = ctx.r6.s32 >> temp.u32;
	// neg r27,r27
	r27.s64 = -r27.s64;
	// cmpw cr6,r7,r27
	cr6.compare<int32_t>(ctx.r7.s32, r27.s32, xer);
	// bgt cr6,0x821a7d10
	if (cr6.getGT()) goto loc_821A7D10;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x821a7cec
	if (!cr6.getEQ()) goto loc_821A7CEC;
	// addi r27,r1,192
	r27.s64 = ctx.r1.s64 + 192;
	// lwzx r27,r11,r27
	r27.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// bgt cr6,0x821a7d00
	if (cr6.getGT()) goto loc_821A7D00;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
loc_821A7CEC:
	// ble cr6,0x821a7d10
	if (!cr6.getGT()) goto loc_821A7D10;
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r10,-256
	cr6.compare<int32_t>(ctx.r10.s32, -256, xer);
	// ble cr6,0x821a7d10
	if (!cr6.getGT()) goto loc_821A7D10;
loc_821A7D00:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x821a7d10
	if (!cr6.getLT()) goto loc_821A7D10;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
loc_821A7D10:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r9,r30
	cr6.compare<int32_t>(ctx.r9.s32, r30.s32, xer);
	// blt cr6,0x821a7c48
	if (cr6.getLT()) goto loc_821A7C48;
loc_821A7D20:
	// rlwinm r11,r8,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// lwzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// sraw r9,r6,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r6.s32 < 0) & (((ctx.r6.s32 >> temp.u32) << temp.u32) != ctx.r6.s32);
	ctx.r9.s64 = ctx.r6.s32 >> temp.u32;
	// add. r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// blt 0x821a7c30
	if (cr0.getLT()) goto loc_821A7C30;
loc_821A7D48:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x821a7d8c
	if (!cr6.getGT()) goto loc_821A7D8C;
	// rlwinm r8,r29,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_821A7D60:
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r27,r1,192
	r27.s64 = ctx.r1.s64 + 192;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwzx r7,r11,r7
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// lwzx r27,r11,r27
	r27.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// subfic r7,r7,2
	xer.ca = ctx.r7.u32 <= 2;
	ctx.r7.s64 = 2 - ctx.r7.s64;
	// slw r7,r27,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (r27.u32 << (ctx.r7.u8 & 0x3F));
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// bne 0x821a7d60
	if (!cr0.getEQ()) goto loc_821A7D60;
loc_821A7D8C:
	// addic. r31,r31,-1
	xer.ca = r31.u32 > 0;
	r31.s64 = r31.s64 + -1;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne 0x821a7a70
	if (!cr0.getEQ()) goto loc_821A7A70;
loc_821A7D98:
	// addi r1,r1,992
	ctx.r1.s64 = ctx.r1.s64 + 992;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821A7DA0"))) PPC_WEAK_FUNC(sub_821A7DA0);
PPC_FUNC_IMPL(__imp__sub_821A7DA0) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stfd f30,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, f30.u64);
	// stfd f31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fabs f30,f1
	f30.u64 = ctx.f1.u64 & ~0x8000000000000000;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f30,92(r1)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lfs f13,-14160(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14160);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f30,f13
	cr6.compare(f30.f64, ctx.f13.f64);
	// bge cr6,0x821a7e30
	if (!cr6.getLT()) goto loc_821A7E30;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-14164(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14164);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmuls f0,f1,f0
	f0.f64 = double(float(ctx.f1.f64 * f0.f64));
	// lfs f13,-14168(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14168);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f12,-14172(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14172);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmuls f0,f0,f0
	f0.f64 = double(float(f0.f64 * f0.f64));
	// fmadds f12,f0,f13,f12
	ctx.f12.f64 = double(float(f0.f64 * ctx.f13.f64 + ctx.f12.f64));
	// lfs f13,-14176(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14176);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmadds f12,f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f13.f64));
	// lfs f13,-14180(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14180);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmadds f12,f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f13.f64));
	// lfs f13,-14184(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14184);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmadds f12,f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f13.f64));
	// lfs f13,-14188(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14188);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmadds f12,f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f13.f64));
	// lfs f13,2776(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f1,f12,f0,f13
	ctx.f1.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f13.f64));
	// b 0x821a7f00
	goto loc_821A7F00;
loc_821A7E30:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fdivs f31,f13,f30
	ctx.fpscr.disableFlushMode();
	f31.f64 = double(float(ctx.f13.f64 / f30.f64));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// addi r8,r1,92
	ctx.r8.s64 = ctx.r1.s64 + 92;
	// lfs f0,2688(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	f0.f64 = double(temp.f32);
	// addi r11,r1,84
	r11.s64 = ctx.r1.s64 + 84;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx v13,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v12,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// vor v1,v0,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// bl 0x821a74f0
	sub_821A74F0(ctx, base);
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// stvx128 v1,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821a78c0
	sub_821A78C0(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,-14192(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14192);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f13,-14196(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14196);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmsubs f13,f31,f0,f13
	ctx.f13.f64 = double(float(f31.f64 * f0.f64 - ctx.f13.f64));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	f0.f64 = double(temp.f32);
	// fdivs f12,f0,f1
	ctx.f12.f64 = double(float(f0.f64 / ctx.f1.f64));
	// lfs f0,-14200(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14200);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmadds f13,f13,f31,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// lfs f0,-14204(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14204);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmsubs f13,f13,f31,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f31.f64 - f0.f64));
	// lfs f0,-14208(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14208);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmadds f13,f13,f31,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// lfs f0,-14212(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14212);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmsubs f13,f13,f31,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f31.f64 - f0.f64));
	// lfs f0,-14216(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14216);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmadds f13,f13,f31,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// lfs f0,-14220(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14220);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmadds f13,f13,f31,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// lfs f0,-14224(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14224);
	f0.f64 = double(temp.f32);
	// fmadds f0,f13,f31,f0
	f0.f64 = double(float(ctx.f13.f64 * f31.f64 + f0.f64));
	// fmuls f1,f12,f0
	ctx.f1.f64 = double(float(ctx.f12.f64 * f0.f64));
loc_821A7F00:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f30,-24(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// lfd f31,-16(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A7F18"))) PPC_WEAK_FUNC(sub_821A7F18);
PPC_FUNC_IMPL(__imp__sub_821A7F18) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stfd f31,-16(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -16, f31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,2688(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// bne cr6,0x821a7f44
	if (!cr6.getEQ()) goto loc_821A7F44;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f1,2776(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	ctx.f1.f64 = double(temp.f32);
	// b 0x821a7f5c
	goto loc_821A7F5C;
loc_821A7F44:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,14032(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14032);
	f0.f64 = double(temp.f32);
	// fmuls f31,f1,f0
	f31.f64 = double(float(ctx.f1.f64 * f0.f64));
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x821a7590
	sub_821A7590(ctx, base);
	// fdivs f1,f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64 / f31.f64));
loc_821A7F5C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f31,-16(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A7F70"))) PPC_WEAK_FUNC(sub_821A7F70);
PPC_FUNC_IMPL(__imp__sub_821A7F70) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// addi r12,r1,-8
	r12.s64 = ctx.r1.s64 + -8;
	// bl 0x823ed548
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,2776(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	f0.f64 = double(temp.f32);
	// fabs f13,f31
	ctx.f13.u64 = f31.u64 & ~0x8000000000000000;
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bge cr6,0x821a7fe8
	if (!cr6.getLT()) goto loc_821A7FE8;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fabs f1,f31
	ctx.f1.u64 = f31.u64 & ~0x8000000000000000;
	// lfs f2,3908(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3908);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x821a7690
	sub_821A7690(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f29,f1
	ctx.fpscr.disableFlushMode();
	f29.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// lfs f30,13968(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 13968);
	f30.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f2,3060(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3060);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x821a7690
	sub_821A7690(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,14492(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14492);
	f0.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmuls f0,f1,f0
	f0.f64 = double(float(ctx.f1.f64 * f0.f64));
	// fmsubs f13,f29,f30,f0
	ctx.f13.f64 = double(float(f29.f64 * f30.f64 - f0.f64));
	// lfs f0,2780(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2780);
	f0.f64 = double(temp.f32);
loc_821A7FE0:
	// fadds f1,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f13.f64 + f0.f64));
	// b 0x821a8050
	goto loc_821A8050;
loc_821A7FE8:
	// fabs f0,f31
	ctx.fpscr.disableFlushMode();
	f0.u64 = f31.u64 & ~0x8000000000000000;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f30,3060(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3060);
	f30.f64 = double(temp.f32);
	// fcmpu cr6,f0,f30
	cr6.compare(f0.f64, f30.f64);
	// bge cr6,0x821a8048
	if (!cr6.getLT()) goto loc_821A8048;
	// fmr f2,f30
	ctx.f2.f64 = f30.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x821a7690
	sub_821A7690(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f28,f1
	ctx.fpscr.disableFlushMode();
	f28.f64 = ctx.f1.f64;
	// fabs f1,f31
	ctx.f1.u64 = f31.u64 & ~0x8000000000000000;
	// lfs f29,14492(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14492);
	f29.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f2,3908(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3908);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x821a7690
	sub_821A7690(ctx, base);
	// fmuls f13,f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64 * f30.f64));
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fabs f12,f31
	ctx.f12.u64 = f31.u64 & ~0x8000000000000000;
	// lfs f0,2948(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2948);
	f0.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmsubs f13,f28,f29,f13
	ctx.f13.f64 = double(float(f28.f64 * f29.f64 - ctx.f13.f64));
	// fnmsubs f13,f12,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * f0.f64 - ctx.f13.f64)));
	// lfs f0,2952(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2952);
	f0.f64 = double(temp.f32);
	// b 0x821a7fe0
	goto loc_821A7FE0;
loc_821A8048:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f1,2688(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	ctx.f1.f64 = double(temp.f32);
loc_821A8050:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// addi r12,r1,-8
	r12.s64 = ctx.r1.s64 + -8;
	// bl 0x823ed594
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A8068"))) PPC_WEAK_FUNC(sub_821A8068);
PPC_FUNC_IMPL(__imp__sub_821A8068) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// addi r12,r1,-8
	r12.s64 = ctx.r1.s64 + -8;
	// bl 0x823ed548
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,2776(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	f0.f64 = double(temp.f32);
	// fabs f13,f31
	ctx.f13.u64 = f31.u64 & ~0x8000000000000000;
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bge cr6,0x821a80e0
	if (!cr6.getLT()) goto loc_821A80E0;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fabs f1,f31
	ctx.f1.u64 = f31.u64 & ~0x8000000000000000;
	// lfs f2,3908(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3908);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x821a7690
	sub_821A7690(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f29,f1
	ctx.fpscr.disableFlushMode();
	f29.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// lfs f30,16032(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16032);
	f30.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f2,3060(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3060);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x821a7690
	sub_821A7690(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,14436(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14436);
	f0.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmuls f0,f1,f0
	f0.f64 = double(float(ctx.f1.f64 * f0.f64));
	// fmsubs f13,f29,f30,f0
	ctx.f13.f64 = double(float(f29.f64 * f30.f64 - f0.f64));
	// lfs f0,2772(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2772);
	f0.f64 = double(temp.f32);
loc_821A80D8:
	// fadds f1,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f13.f64 + f0.f64));
	// b 0x821a8148
	goto loc_821A8148;
loc_821A80E0:
	// fabs f0,f31
	ctx.fpscr.disableFlushMode();
	f0.u64 = f31.u64 & ~0x8000000000000000;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f2,3060(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3060);
	ctx.f2.f64 = double(temp.f32);
	// fcmpu cr6,f0,f2
	cr6.compare(f0.f64, ctx.f2.f64);
	// bge cr6,0x821a8140
	if (!cr6.getLT()) goto loc_821A8140;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x821a7690
	sub_821A7690(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f28,f1
	ctx.fpscr.disableFlushMode();
	f28.f64 = ctx.f1.f64;
	// fabs f1,f31
	ctx.f1.u64 = f31.u64 & ~0x8000000000000000;
	// lfs f29,14436(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14436);
	f29.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f30,3908(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3908);
	f30.f64 = double(temp.f32);
	// fmr f2,f30
	ctx.f2.f64 = f30.f64;
	// bl 0x821a7690
	sub_821A7690(ctx, base);
	// fmuls f13,f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64 * f30.f64));
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fabs f12,f31
	ctx.f12.u64 = f31.u64 & ~0x8000000000000000;
	// lfs f0,28808(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 28808);
	f0.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmsubs f13,f28,f29,f13
	ctx.f13.f64 = double(float(f28.f64 * f29.f64 - ctx.f13.f64));
	// fnmsubs f13,f12,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * f0.f64 - ctx.f13.f64)));
	// lfs f0,14492(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14492);
	f0.f64 = double(temp.f32);
	// b 0x821a80d8
	goto loc_821A80D8;
loc_821A8140:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f1,2688(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	ctx.f1.f64 = double(temp.f32);
loc_821A8148:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// addi r12,r1,-8
	r12.s64 = ctx.r1.s64 + -8;
	// bl 0x823ed594
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A8160"))) PPC_WEAK_FUNC(sub_821A8160);
PPC_FUNC_IMPL(__imp__sub_821A8160) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed12c
	// addi r12,r1,-64
	r12.s64 = ctx.r1.s64 + -64;
	// bl 0x823ed530
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// fmr f25,f1
	ctx.fpscr.disableFlushMode();
	f25.f64 = ctx.f1.f64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f30,f2
	f30.f64 = ctx.f2.f64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// lfs f23,2688(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2688);
	f23.f64 = double(temp.f32);
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// lfs f0,2692(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2692);
	f0.f64 = double(temp.f32);
	// mullw r31,r27,r25
	r31.s64 = int64_t(r27.s32) * int64_t(r25.s32);
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x821a81b0
	if (cr6.getEQ()) goto loc_821A81B0;
	// fmr f29,f23
	f29.f64 = f23.f64;
	// b 0x821a81b4
	goto loc_821A81B4;
loc_821A81B0:
	// fmr f29,f0
	ctx.fpscr.disableFlushMode();
	f29.f64 = f0.f64;
loc_821A81B4:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// lfs f24,2776(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	f24.f64 = double(temp.f32);
	// beq cr6,0x821a84f4
	if (cr6.getEQ()) goto loc_821A84F4;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// beq cr6,0x821a845c
	if (cr6.getEQ()) goto loc_821A845C;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// beq cr6,0x821a83dc
	if (cr6.getEQ()) goto loc_821A83DC;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// beq cr6,0x821a8378
	if (cr6.getEQ()) goto loc_821A8378;
	// cmpwi cr6,r3,5
	cr6.compare<int32_t>(ctx.r3.s32, 5, xer);
	// beq cr6,0x821a82fc
	if (cr6.getEQ()) goto loc_821A82FC;
	// cmpwi cr6,r3,6
	cr6.compare<int32_t>(ctx.r3.s32, 6, xer);
	// beq cr6,0x821a825c
	if (cr6.getEQ()) goto loc_821A825C;
	// cmpwi cr6,r3,7
	cr6.compare<int32_t>(ctx.r3.s32, 7, xer);
	// bne cr6,0x821a825c
	if (!cr6.getEQ()) goto loc_821A825C;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821a821c
	if (cr6.getEQ()) goto loc_821A821C;
	// mr r11,r28
	r11.u64 = r28.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x821a821c
	if (cr0.getEQ()) goto loc_821A821C;
	// mtctr r31
	ctr.u64 = r31.u64;
loc_821A8210:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x821a8210
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_821A8210;
loc_821A821C:
	// subf r11,r27,r31
	r11.s64 = r31.s64 - r27.s64;
	// add r10,r31,r27
	ctx.r10.u64 = r31.u64 + r27.u64;
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x821a8554
	if (!cr6.getLT()) goto loc_821A8554;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf. r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// add r10,r9,r28
	ctx.r10.u64 = ctx.r9.u64 + r28.u64;
	// lis r9,16256
	ctx.r9.s64 = 1065353216;
	// beq 0x821a8554
	if (cr0.getEQ()) goto loc_821A8554;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_821A824C:
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x821a824c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_821A824C;
	// b 0x821a8554
	goto loc_821A8554;
loc_821A825C:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821a8554
	if (cr6.getEQ()) goto loc_821A8554;
	// rlwinm r11,r31,31,1,31
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// subf r6,r27,r11
	ctx.r6.s64 = r11.s64 - r27.s64;
	// subf r7,r11,r27
	ctx.r7.s64 = r27.s64 - r11.s64;
loc_821A8278:
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// blt cr6,0x821a82a8
	if (cr6.getLT()) goto loc_821A82A8;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x821a82a8
	if (!cr6.getLT()) goto loc_821A82A8;
	// add r8,r7,r10
	ctx.r8.u64 = ctx.r7.u64 + ctx.r10.u64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fadds f0,f0,f29
	f0.f64 = double(float(f0.f64 + f29.f64));
	// b 0x821a82dc
	goto loc_821A82DC;
loc_821A82A8:
	// add r8,r11,r27
	ctx.r8.u64 = r11.u64 + r27.u64;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bge cr6,0x821a82e4
	if (!cr6.getLT()) goto loc_821A82E4;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x821a82e4
	if (cr6.getLT()) goto loc_821A82E4;
	// subf r8,r10,r11
	ctx.r8.s64 = r11.s64 - ctx.r10.s64;
	// add r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 + r27.u64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f29
	f0.f64 = double(float(f0.f64 - f29.f64));
loc_821A82DC:
	// stfs f0,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// b 0x821a82e8
	goto loc_821A82E8;
loc_821A82E4:
	// stfs f23,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f23.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
loc_821A82E8:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// blt cr6,0x821a8278
	if (cr6.getLT()) goto loc_821A8278;
	// b 0x821a8554
	goto loc_821A8554;
loc_821A82FC:
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821a8554
	if (cr6.getEQ()) goto loc_821A8554;
	// clrldi r11,r31,32
	r11.u64 = r31.u64 & 0xFFFFFFFF;
	// fdivs f30,f24,f30
	ctx.fpscr.disableFlushMode();
	f30.f64 = double(float(f24.f64 / f30.f64));
	// mr r30,r28
	r30.u64 = r28.u64;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f28,f13,f0
	f28.f64 = double(float(ctx.f13.f64 * f0.f64));
loc_821A8328:
	// clrldi r11,r29,32
	r11.u64 = r29.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f28
	f0.f64 = double(float(f0.f64 - f28.f64));
	// fadds f0,f0,f29
	f0.f64 = double(float(f0.f64 + f29.f64));
	// fmuls f31,f0,f25
	f31.f64 = double(float(f0.f64 * f25.f64));
	// fmuls f1,f30,f31
	ctx.f1.f64 = double(float(f30.f64 * f31.f64));
	// bl 0x821a7f18
	sub_821A7F18(ctx, base);
	// fmr f27,f1
	ctx.fpscr.disableFlushMode();
	f27.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x821a7f18
	sub_821A7F18(ctx, base);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// fmuls f0,f27,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f27.f64 * ctx.f1.f64));
	// stfs f0,0(r30)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// blt cr6,0x821a8328
	if (cr6.getLT()) goto loc_821A8328;
	// b 0x821a8554
	goto loc_821A8554;
loc_821A8378:
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821a8554
	if (cr6.getEQ()) goto loc_821A8554;
	// clrldi r11,r31,32
	r11.u64 = r31.u64 & 0xFFFFFFFF;
	// mr r30,r28
	r30.u64 = r28.u64;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f31,f13,f0
	f31.f64 = double(float(ctx.f13.f64 * f0.f64));
loc_821A83A0:
	// clrldi r11,r29,32
	r11.u64 = r29.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f31
	f0.f64 = double(float(f0.f64 - f31.f64));
	// fadds f0,f0,f29
	f0.f64 = double(float(f0.f64 + f29.f64));
	// fmuls f1,f0,f25
	ctx.f1.f64 = double(float(f0.f64 * f25.f64));
	// bl 0x821a7f70
	sub_821A7F70(ctx, base);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// stfs f1,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// blt cr6,0x821a83a0
	if (cr6.getLT()) goto loc_821A83A0;
	// b 0x821a8554
	goto loc_821A8554;
loc_821A83DC:
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821a8554
	if (cr6.getEQ()) goto loc_821A8554;
	// clrldi r11,r31,32
	r11.u64 = r31.u64 & 0xFFFFFFFF;
	// fdivs f31,f24,f30
	ctx.fpscr.disableFlushMode();
	f31.f64 = double(float(f24.f64 / f30.f64));
	// mr r30,r28
	r30.u64 = r28.u64;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f28,3060(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3060);
	f28.f64 = double(temp.f32);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f30,f13,f0
	f30.f64 = double(float(ctx.f13.f64 * f0.f64));
loc_821A8410:
	// clrldi r11,r29,32
	r11.u64 = r29.u64 & 0xFFFFFFFF;
	// fmr f1,f28
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f28.f64;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f30
	f0.f64 = double(float(f0.f64 - f30.f64));
	// fadds f0,f0,f29
	f0.f64 = double(float(f0.f64 + f29.f64));
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// fmuls f0,f0,f25
	f0.f64 = double(float(f0.f64 * f25.f64));
	// fmuls f0,f0,f0
	f0.f64 = double(float(f0.f64 * f0.f64));
	// fneg f2,f0
	ctx.f2.u64 = f0.u64 ^ 0x8000000000000000;
	// bl 0x821a7690
	sub_821A7690(ctx, base);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// stfs f1,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// blt cr6,0x821a8410
	if (cr6.getLT()) goto loc_821A8410;
	// b 0x821a8554
	goto loc_821A8554;
loc_821A845C:
	// clrldi r11,r31,32
	r11.u64 = r31.u64 & 0xFFFFFFFF;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f27,f13,f0
	f27.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fsubs f28,f27,f29
	f28.f64 = double(float(f27.f64 - f29.f64));
	// beq cr6,0x821a8554
	if (cr6.getEQ()) goto loc_821A8554;
	// fdivs f26,f24,f28
	f26.f64 = double(float(f24.f64 / f28.f64));
	// mr r29,r28
	r29.u64 = r28.u64;
loc_821A848C:
	// clrldi r11,r30,32
	r11.u64 = r30.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f31,f0
	f31.f64 = double(float(f0.f64));
	// fsubs f0,f31,f28
	f0.f64 = double(float(f31.f64 - f28.f64));
	// fmuls f0,f0,f26
	f0.f64 = double(float(f0.f64 * f26.f64));
	// fnmsubs f1,f0,f0,f24
	ctx.f1.f64 = double(float(-(f0.f64 * f0.f64 - f24.f64)));
	// bl 0x821a78c0
	sub_821A78C0(ctx, base);
	// fmuls f1,f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64 * f30.f64));
	// bl 0x821a7da0
	sub_821A7DA0(ctx, base);
	// fmr f22,f1
	ctx.fpscr.disableFlushMode();
	f22.f64 = ctx.f1.f64;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// bl 0x821a7da0
	sub_821A7DA0(ctx, base);
	// fsubs f0,f31,f27
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f31.f64 - f27.f64));
	// fdivs f31,f22,f1
	f31.f64 = double(float(f22.f64 / ctx.f1.f64));
	// fadds f0,f0,f29
	f0.f64 = double(float(f0.f64 + f29.f64));
	// fmuls f1,f0,f25
	ctx.f1.f64 = double(float(f0.f64 * f25.f64));
	// bl 0x821a7f18
	sub_821A7F18(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// fmuls f0,f31,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f31.f64 * ctx.f1.f64));
	// stfs f0,0(r29)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r29.u32 + 0, temp.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// blt cr6,0x821a848c
	if (cr6.getLT()) goto loc_821A848C;
	// b 0x821a8554
	goto loc_821A8554;
loc_821A84F4:
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821a8554
	if (cr6.getEQ()) goto loc_821A8554;
	// clrldi r11,r31,32
	r11.u64 = r31.u64 & 0xFFFFFFFF;
	// mr r30,r28
	r30.u64 = r28.u64;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f31,f13,f0
	f31.f64 = double(float(ctx.f13.f64 * f0.f64));
loc_821A851C:
	// clrldi r11,r29,32
	r11.u64 = r29.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f31
	f0.f64 = double(float(f0.f64 - f31.f64));
	// fadds f0,f0,f29
	f0.f64 = double(float(f0.f64 + f29.f64));
	// fmuls f1,f0,f25
	ctx.f1.f64 = double(float(f0.f64 * f25.f64));
	// bl 0x821a8068
	sub_821A8068(ctx, base);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// stfs f1,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// blt cr6,0x821a851c
	if (cr6.getLT()) goto loc_821A851C;
loc_821A8554:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// beq cr6,0x821a8560
	if (cr6.getEQ()) goto loc_821A8560;
	// stfs f23,0(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f23.f64);
	PPC_STORE_U32(r28.u32 + 0, temp.u32);
loc_821A8560:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x821a85cc
	if (cr6.getEQ()) goto loc_821A85CC;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
loc_821A8570:
	// fmr f0,f23
	ctx.fpscr.disableFlushMode();
	f0.f64 = f23.f64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x821a85c0
	if (cr6.getEQ()) goto loc_821A85C0;
	// rlwinm r8,r27,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
loc_821A8588:
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// fadds f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// bne 0x821a8588
	if (!cr0.getEQ()) goto loc_821A8588;
	// fdivs f0,f24,f0
	f0.f64 = double(float(f24.f64 / f0.f64));
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_821A85A8:
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// bne 0x821a85a8
	if (!cr0.getEQ()) goto loc_821A85A8;
loc_821A85C0:
	// addic. r7,r7,-1
	xer.ca = ctx.r7.u32 > 0;
	ctx.r7.s64 = ctx.r7.s64 + -1;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne 0x821a8570
	if (!cr0.getEQ()) goto loc_821A8570;
loc_821A85CC:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// addi r12,r1,-64
	r12.s64 = ctx.r1.s64 + -64;
	// bl 0x823ed57c
	// b 0x823ed17c
	return;
}

__attribute__((alias("__imp__sub_821A85E0"))) PPC_WEAK_FUNC(sub_821A85E0);
PPC_FUNC_IMPL(__imp__sub_821A85E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-784(r1)
	ea = -784 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r8,1
	ctx.r8.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x821a861c
	if (!cr6.getEQ()) goto loc_821A861C;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
loc_821A861C:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f9,6580(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 6580);
	ctx.f9.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x821a8634
	if (!cr6.getLT()) goto loc_821A8634;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_821A8634:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f11,2776(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x821a8648
	if (!cr6.getGT()) goto loc_821A8648;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_821A8648:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lfs f12,-14112(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14112);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,12904(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12904);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f13,f0,f12,f13
	ctx.f13.f64 = double(float(-(f0.f64 * ctx.f12.f64 - ctx.f13.f64)));
	// fmadds f10,f13,f0,f11
	ctx.f10.f64 = double(float(ctx.f13.f64 * f0.f64 + ctx.f11.f64));
	// beq cr6,0x821a8700
	if (cr6.getEQ()) goto loc_821A8700;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x821a8700
	if (cr6.getEQ()) goto loc_821A8700;
	// lfs f0,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x821a8680
	if (!cr6.getLT()) goto loc_821A8680;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_821A8680:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x821a868c
	if (!cr6.getGT()) goto loc_821A868C;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_821A868C:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,28472(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 28472);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmadds f12,f0,f12,f13
	ctx.f12.f64 = double(float(f0.f64 * ctx.f12.f64 + ctx.f13.f64));
	// lfd f13,-14120(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -14120);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fcmpu cr6,f1,f13
	cr6.compare(ctx.f1.f64, ctx.f13.f64);
	// lfs f13,12888(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12888);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f13,f12,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * f0.f64 - ctx.f13.f64)));
	// ble cr6,0x821a86c4
	if (!cr6.getGT()) goto loc_821A86C4;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-14128(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -14128);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// blt cr6,0x821a86c8
	if (cr6.getLT()) goto loc_821A86C8;
loc_821A86C4:
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_821A86C8:
	// fcmpu cr6,f10,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f10.f64, ctx.f11.f64);
	// bge cr6,0x821a86d4
	if (!cr6.getLT()) goto loc_821A86D4;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_821A86D4:
	// fcmpu cr6,f13,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f13.f64, ctx.f11.f64);
	// bge cr6,0x821a86e0
	if (!cr6.getLT()) goto loc_821A86E0;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_821A86E0:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,3060(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3060);
	f0.f64 = double(temp.f32);
	// fdivs f0,f0,f1
	f0.f64 = double(float(f0.f64 / ctx.f1.f64));
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// blt cr6,0x821a86f8
	if (cr6.getLT()) goto loc_821A86F8;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_821A86F8:
	// fmuls f0,f0,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// b 0x821a8740
	goto loc_821A8740;
loc_821A8700:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-14136(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -14136);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// ble cr6,0x821a8720
	if (!cr6.getGT()) goto loc_821A8720;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-14144(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -14144);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// blt cr6,0x821a8724
	if (cr6.getLT()) goto loc_821A8724;
loc_821A8720:
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_821A8724:
	// fcmpu cr6,f10,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f10.f64, ctx.f11.f64);
	// bge cr6,0x821a8730
	if (!cr6.getLT()) goto loc_821A8730;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_821A8730:
	// fdivs f0,f11,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f11.f64 / ctx.f1.f64));
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// blt cr6,0x821a8740
	if (cr6.getLT()) goto loc_821A8740;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_821A8740:
	// fmuls f13,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f0.f64 * ctx.f10.f64));
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfs f0,-18868(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -18868);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f0,4(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 4, temp.u32);
	// lfs f0,-14152(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14152);
	f0.f64 = double(temp.f32);
	// lis r11,31
	r11.s64 = 2031616;
	// ori r10,r11,65535
	ctx.r10.u64 = r11.u64 | 65535;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f10,2692(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2692);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f0,f1,f0,f10
	f0.f64 = double(float(ctx.f1.f64 * f0.f64 + ctx.f10.f64));
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,648(r31)
	PPC_STORE_U32(r31.u32 + 648, r11.u32);
	// ble cr6,0x821a8790
	if (!cr6.getGT()) goto loc_821A8790;
	// stw r10,648(r31)
	PPC_STORE_U32(r31.u32 + 648, ctx.r10.u32);
loc_821A8790:
	// lfs f1,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// cmpwi cr6,r5,2
	cr6.compare<int32_t>(ctx.r5.s32, 2, xer);
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// bne cr6,0x821a87dc
	if (!cr6.getEQ()) goto loc_821A87DC;
	// lfs f0,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x821a87b0
	if (!cr6.getLT()) goto loc_821A87B0;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_821A87B0:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x821a87bc
	if (!cr6.getGT()) goto loc_821A87BC;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_821A87BC:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,3908(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3908);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f12,12272(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12272);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmadds f12,f0,f13,f12
	ctx.f12.f64 = double(float(f0.f64 * ctx.f13.f64 + ctx.f12.f64));
	// lfs f13,2780(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2780);
	ctx.f13.f64 = double(temp.f32);
	// b 0x821a885c
	goto loc_821A885C;
loc_821A87DC:
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// bne cr6,0x821a881c
	if (!cr6.getEQ()) goto loc_821A881C;
	// lfs f0,8(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x821a87f4
	if (!cr6.getLT()) goto loc_821A87F4;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_821A87F4:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x821a8800
	if (!cr6.getGT()) goto loc_821A8800;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_821A8800:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,14116(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14116);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f12,-14156(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14156);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f0,f13,f12
	ctx.f13.f64 = double(float(f0.f64 * ctx.f13.f64 + ctx.f12.f64));
	// fmadds f2,f13,f0,f10
	ctx.f2.f64 = double(float(ctx.f13.f64 * f0.f64 + ctx.f10.f64));
	// b 0x821a886c
	goto loc_821A886C;
loc_821A881C:
	// cmpwi cr6,r5,5
	cr6.compare<int32_t>(ctx.r5.s32, 5, xer);
	// bne cr6,0x821a8864
	if (!cr6.getEQ()) goto loc_821A8864;
	// lfs f0,8(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x821a8834
	if (!cr6.getLT()) goto loc_821A8834;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_821A8834:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x821a8840
	if (!cr6.getGT()) goto loc_821A8840;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_821A8840:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,12896(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12896);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f12,-17376(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17376);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmadds f12,f0,f13,f12
	ctx.f12.f64 = double(float(f0.f64 * ctx.f13.f64 + ctx.f12.f64));
	// lfs f13,12468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12468);
	ctx.f13.f64 = double(temp.f32);
loc_821A885C:
	// fmadds f2,f12,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f13.f64));
	// b 0x821a886c
	goto loc_821A886C;
loc_821A8864:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f2,2688(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	ctx.f2.f64 = double(temp.f32);
loc_821A886C:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// li r3,7
	ctx.r3.s64 = 7;
	// bne cr6,0x821a887c
	if (!cr6.getEQ()) goto loc_821A887C;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
loc_821A887C:
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,16
	ctx.r4.s64 = 16;
	// bl 0x821a8160
	sub_821A8160(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r4,r31,8
	ctx.r4.s64 = r31.s64 + 8;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821a7948
	sub_821A7948(ctx, base);
	// addi r1,r1,784
	ctx.r1.s64 = ctx.r1.s64 + 784;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A88C0"))) PPC_WEAK_FUNC(sub_821A88C0);
PPC_FUNC_IMPL(__imp__sub_821A88C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed100
	// stfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -168, f30.u64);
	// stfd f31,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, f31.u64);
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// mr r21,r8
	r21.u64 = ctx.r8.u64;
	// mr r15,r9
	r15.u64 = ctx.r9.u64;
	// mr r14,r10
	r14.u64 = ctx.r10.u64;
	// bl 0x8240fa4c
	__imp__VdQueryVideoMode(ctx, base);
	// lwz r29,120(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// cntlzw r11,r29
	r11.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// bne cr6,0x821a8924
	if (!cr6.getEQ()) goto loc_821A8924;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x821a8924
	if (!cr6.getEQ()) goto loc_821A8924;
	// mr r30,r21
	r30.u64 = r21.u64;
loc_821A8924:
	// lwz r20,420(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// rlwinm r25,r27,16,16,31
	r25.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 16) & 0xFFFF;
	// lwz r18,21532(r28)
	r18.u64 = PPC_LOAD_U32(r28.u32 + 21532);
	// rlwinm r19,r31,16,16,31
	r19.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 16) & 0xFFFF;
	// lwz r16,21528(r28)
	r16.u64 = PPC_LOAD_U32(r28.u32 + 21528);
	// clrlwi r17,r31,16
	r17.u64 = r31.u32 & 0xFFFF;
	// rlwinm r28,r30,16,16,31
	r28.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// clrlwi r24,r30,16
	r24.u64 = r30.u32 & 0xFFFF;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// clrlwi r27,r27,16
	r27.u64 = r27.u32 & 0xFFFF;
	// rlwinm r23,r26,16,16,31
	r23.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 16) & 0xFFFF;
	// clrlwi r22,r26,16
	r22.u64 = r26.u32 & 0xFFFF;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a8980
	if (!cr6.getEQ()) goto loc_821A8980;
	// cmplw cr6,r28,r23
	cr6.compare<uint32_t>(r28.u32, r23.u32, xer);
	// bne cr6,0x821a896c
	if (!cr6.getEQ()) goto loc_821A896C;
	// li r11,7
	r11.s64 = 7;
	// b 0x821a897c
	goto loc_821A897C;
loc_821A896C:
	// subfc r11,r23,r28
	xer.ca = r28.u32 >= r23.u32;
	r11.s64 = r28.s64 - r23.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
loc_821A897C:
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
loc_821A8980:
	// lwz r11,0(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821a89c4
	if (!cr6.getEQ()) goto loc_821A89C4;
	// divwu r11,r22,r10
	r11.u32 = r22.u32 / ctx.r10.u32;
	// twllei r10,0
	// cmplw cr6,r24,r11
	cr6.compare<uint32_t>(r24.u32, r11.u32, xer);
	// bne cr6,0x821a89b0
	if (!cr6.getEQ()) goto loc_821A89B0;
	// subfic r11,r29,0
	xer.ca = r29.u32 <= 0;
	r11.s64 = 0 - r29.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,7
	r11.s64 = r11.s64 + 7;
	// b 0x821a89c0
	goto loc_821A89C0;
loc_821A89B0:
	// subfc r11,r11,r24
	xer.ca = r24.u32 >= r11.u32;
	r11.s64 = r24.s64 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
loc_821A89C0:
	// stw r11,0(r15)
	PPC_STORE_U32(r15.u32 + 0, r11.u32);
loc_821A89C4:
	// clrldi r9,r23,32
	ctx.r9.u64 = r23.u64 & 0xFFFFFFFF;
	// clrldi r11,r28,32
	r11.u64 = r28.u64 & 0xFFFFFFFF;
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fdivs f30,f0,f13
	f30.f64 = double(float(f0.f64 / ctx.f13.f64));
	// lfs f0,12468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12468);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f13,-14088(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -14088);
	// fcmpu cr6,f30,f0
	cr6.compare(f30.f64, f0.f64);
	// blt cr6,0x821a8a14
	if (cr6.getLT()) goto loc_821A8A14;
	// fcmpu cr6,f30,f13
	cr6.compare(f30.f64, ctx.f13.f64);
	// bgt cr6,0x821a8a14
	if (cr6.getGT()) goto loc_821A8A14;
	// fmr f30,f0
	f30.f64 = f0.f64;
loc_821A8A14:
	// clrldi r11,r24,32
	r11.u64 = r24.u64 & 0xFFFFFFFF;
	// clrldi r9,r22,32
	ctx.r9.u64 = r22.u64 & 0xFFFFFFFF;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f11,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// lfd f12,88(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fmuls f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fdivs f31,f12,f10
	f31.f64 = double(float(ctx.f12.f64 / ctx.f10.f64));
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// blt cr6,0x821a8a70
	if (cr6.getLT()) goto loc_821A8A70;
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// bgt cr6,0x821a8a70
	if (cr6.getGT()) goto loc_821A8A70;
	// fmr f31,f0
	f31.f64 = f0.f64;
	// b 0x821a8ab4
	goto loc_821A8AB4;
loc_821A8A70:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,3908(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3908);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// blt cr6,0x821a8a90
	if (cr6.getLT()) goto loc_821A8A90;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-14096(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -14096);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// ble cr6,0x821a8ab0
	if (!cr6.getGT()) goto loc_821A8AB0;
loc_821A8A90:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,3060(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3060);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// blt cr6,0x821a8ab4
	if (cr6.getLT()) goto loc_821A8AB4;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f0,-14128(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -14128);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// bgt cr6,0x821a8ab4
	if (cr6.getGT()) goto loc_821A8AB4;
loc_821A8AB0:
	// fmr f31,f13
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f13.f64;
loc_821A8AB4:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r9,140(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// twllei r9,0
	// lwz r11,1584(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1584);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// divwu r11,r10,r9
	r11.u32 = ctx.r10.u32 / ctx.r9.u32;
	// rlwinm r26,r11,1,0,30
	r26.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r26,10
	cr6.compare<uint32_t>(r26.u32, 10, xer);
	// ble cr6,0x821a8adc
	if (!cr6.getGT()) goto loc_821A8ADC;
	// li r26,10
	r26.s64 = 10;
loc_821A8ADC:
	// li r11,7680
	r11.s64 = 7680;
	// twllei r28,0
	// divwu r11,r11,r28
	r11.u32 = r11.u32 / r28.u32;
	// addi r30,r11,-1
	r30.s64 = r11.s64 + -1;
	// li r11,6
	r11.s64 = 6;
	// cmplwi cr6,r30,6
	cr6.compare<uint32_t>(r30.u32, 6, xer);
	// bgt cr6,0x821a8afc
	if (cr6.getGT()) goto loc_821A8AFC;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_821A8AFC:
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f12,f0
	ctx.f12.f64 = double(float(f0.f64));
	// frsp f0,f13
	f0.f64 = double(float(ctx.f13.f64));
	// fmuls f13,f12,f30
	ctx.f13.f64 = double(float(ctx.f12.f64 * f30.f64));
	// fdivs f12,f0,f13
	ctx.f12.f64 = double(float(f0.f64 / ctx.f13.f64));
	// fctidz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f12.f64));
	// stfiwx f12,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f12.u32);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x821a8b54
	if (!cr6.getLT()) goto loc_821A8B54;
	// cmplwi cr6,r30,6
	cr6.compare<uint32_t>(r30.u32, 6, xer);
	// ble cr6,0x821a8b68
	if (!cr6.getGT()) goto loc_821A8B68;
	// li r30,6
	r30.s64 = 6;
	// b 0x821a8b68
	goto loc_821A8B68;
loc_821A8B54:
	// fdivs f0,f0,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 / ctx.f13.f64));
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_821A8B68:
	// lwz r31,436(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// li r5,1408
	ctx.r5.s64 = 1408;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r5,0(r15)
	ctx.r5.u64 = PPC_LOAD_U32(r15.u32 + 0);
	// mr r6,r14
	ctx.r6.u64 = r14.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r30,r31,652
	r30.s64 = r31.s64 + 652;
	// bl 0x821a85e0
	sub_821A85E0(ctx, base);
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// lwz r5,0(r20)
	ctx.r5.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r6,428(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// bl 0x821a85e0
	sub_821A85E0(ctx, base);
	// addi r6,r28,3
	ctx.r6.s64 = r28.s64 + 3;
	// rlwinm r11,r21,16,16,31
	r11.u64 = __builtin_rotateleft64(r21.u32 | (r21.u64 << 32), 16) & 0xFFFF;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rlwinm r5,r6,30,2,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// lwz r8,648(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// clrlwi r6,r21,16
	ctx.r6.u64 = r21.u32 & 0xFFFF;
	// addi r10,r11,31
	ctx.r10.s64 = r11.s64 + 31;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// li r11,0
	r11.s64 = 0;
	// li r7,1
	ctx.r7.s64 = 1;
	// rlwinm r6,r6,0,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFE;
	// subf r26,r23,r18
	r26.s64 = r18.s64 - r23.s64;
	// cntlzw r4,r29
	ctx.r4.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// stw r11,1348(r31)
	PPC_STORE_U32(r31.u32 + 1348, r11.u32);
	// subf r3,r22,r16
	ctx.r3.s64 = r16.s64 - r22.s64;
	// stw r7,1360(r31)
	PPC_STORE_U32(r31.u32 + 1360, ctx.r7.u32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// clrlwi r22,r17,20
	r22.u64 = r17.u32 & 0xFFF;
	// stw r6,1344(r31)
	PPC_STORE_U32(r31.u32 + 1344, ctx.r6.u32);
	// addi r6,r9,-1
	ctx.r6.s64 = ctx.r9.s64 + -1;
	// addi r23,r7,-1
	r23.s64 = ctx.r7.s64 + -1;
	// stw r11,1332(r31)
	PPC_STORE_U32(r31.u32 + 1332, r11.u32);
	// rlwinm r7,r4,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x1;
	// stw r11,1336(r31)
	PPC_STORE_U32(r31.u32 + 1336, r11.u32);
	// rlwimi r6,r23,8,20,23
	ctx.r6.u64 = (__builtin_rotateleft32(r23.u32, 8) & 0xF00) | (ctx.r6.u64 & 0xFFFFFFFFFFFFF0FF);
	// rlwinm r4,r28,16,4,15
	ctx.r4.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 16) & 0xFFF0000;
	// clrlwi r28,r24,20
	r28.u64 = r24.u32 & 0xFFF;
	// rlwinm r24,r19,16,4,15
	r24.u64 = __builtin_rotateleft64(r19.u32 | (r19.u64 << 32), 16) & 0xFFF0000;
	// or r4,r4,r28
	ctx.r4.u64 = ctx.r4.u64 | r28.u64;
	// clrlwi r5,r5,22
	ctx.r5.u64 = ctx.r5.u32 & 0x3FF;
	// or r28,r24,r22
	r28.u64 = r24.u64 | r22.u64;
	// subf r3,r27,r3
	ctx.r3.s64 = ctx.r3.s64 - r27.s64;
	// subf r26,r25,r26
	r26.s64 = r26.s64 - r25.s64;
	// stw r4,1356(r31)
	PPC_STORE_U32(r31.u32 + 1356, ctx.r4.u32);
	// rlwinm r10,r10,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// stw r5,1404(r31)
	PPC_STORE_U32(r31.u32 + 1404, ctx.r5.u32);
	// clrlwi r5,r3,20
	ctx.r5.u64 = ctx.r3.u32 & 0xFFF;
	// stw r28,1352(r31)
	PPC_STORE_U32(r31.u32 + 1352, r28.u32);
	// clrlwi r4,r27,20
	ctx.r4.u64 = r27.u32 & 0xFFF;
	// clrlwi r3,r25,20
	ctx.r3.u64 = r25.u32 & 0xFFF;
	// clrlwi r28,r26,20
	r28.u64 = r26.u32 & 0xFFF;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// stw r10,1340(r31)
	PPC_STORE_U32(r31.u32 + 1340, ctx.r10.u32);
	// stw r10,1320(r31)
	PPC_STORE_U32(r31.u32 + 1320, ctx.r10.u32);
	// stw r4,1304(r31)
	PPC_STORE_U32(r31.u32 + 1304, ctx.r4.u32);
	// andi. r6,r6,3847
	ctx.r6.u64 = ctx.r6.u64 & 3847;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// stw r5,1308(r31)
	PPC_STORE_U32(r31.u32 + 1308, ctx.r5.u32);
	// stw r3,1312(r31)
	PPC_STORE_U32(r31.u32 + 1312, ctx.r3.u32);
	// stw r28,1316(r31)
	PPC_STORE_U32(r31.u32 + 1316, r28.u32);
	// stw r6,1364(r31)
	PPC_STORE_U32(r31.u32 + 1364, ctx.r6.u32);
	// lwz r10,648(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 648);
	// rlwinm r10,r10,5,6,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0x3FFFFE0;
	// stw r10,1376(r31)
	PPC_STORE_U32(r31.u32 + 1376, ctx.r10.u32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// slw r7,r8,r7
	ctx.r7.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r8.u32 << (ctx.r7.u8 & 0x3F));
	// clrlwi r7,r7,6
	ctx.r7.u64 = ctx.r7.u32 & 0x3FFFFFF;
	// lfs f11,25740(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 25740);
	ctx.f11.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f0,15940(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15940);
	f0.f64 = double(temp.f32);
	// lis r10,256
	ctx.r10.s64 = 16777216;
	// stw r11,1372(r31)
	PPC_STORE_U32(r31.u32 + 1372, r11.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// stw r7,1392(r31)
	PPC_STORE_U32(r31.u32 + 1392, ctx.r7.u32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// stw r10,1388(r31)
	PPC_STORE_U32(r31.u32 + 1388, ctx.r10.u32);
	// lwz r11,648(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 648);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lfd f12,88(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmadds f12,f13,f11,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfs f13,2776(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2776);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fadds f10,f12,f13
	ctx.f10.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// lfs f12,2692(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2692);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f10,f10,f0,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 * f0.f64 + ctx.f12.f64));
	// fctidz f10,f10
	ctx.f10.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f10.f64));
	// stfiwx f10,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f10.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r11,9,12,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0xFFE00;
	// stw r11,1380(r31)
	PPC_STORE_U32(r31.u32 + 1380, r11.u32);
	// beq cr6,0x821a8da8
	if (cr6.getEQ()) goto loc_821A8DA8;
	// clrldi r10,r8,32
	ctx.r10.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// lfs f9,-14104(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14104);
	ctx.f9.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f8,-14108(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -14108);
	ctx.f8.f64 = double(temp.f32);
	// lfd f11,96(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lfd f10,88(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fmadds f9,f11,f9,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f9.f64 + ctx.f10.f64));
	// fmadds f11,f11,f8,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f10.f64));
	// fadds f10,f9,f13
	ctx.f10.f64 = double(float(ctx.f9.f64 + ctx.f13.f64));
	// fadds f13,f11,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 + ctx.f13.f64));
	// fmadds f11,f10,f0,f12
	ctx.f11.f64 = double(float(ctx.f10.f64 * f0.f64 + ctx.f12.f64));
	// fmadds f0,f13,f0,f12
	f0.f64 = double(float(ctx.f13.f64 * f0.f64 + ctx.f12.f64));
	// fctidz f13,f11
	ctx.f13.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f11.f64));
	// stfiwx f13,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f13.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&f0.f64));
	// rlwinm r11,r11,9,13,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x7FE00;
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, f0.u32);
	// stw r11,1396(r31)
	PPC_STORE_U32(r31.u32 + 1396, r11.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r11,9,13,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x7FE00;
	// stw r11,1400(r31)
	PPC_STORE_U32(r31.u32 + 1400, r11.u32);
	// b 0x821a8df4
	goto loc_821A8DF4;
loc_821A8DA8:
	// clrldi r11,r8,32
	r11.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// clrldi r10,r9,32
	ctx.r10.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f10,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lfd f9,88(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fmadds f11,f10,f11,f9
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 + ctx.f9.f64));
	// fadds f13,f11,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 + ctx.f13.f64));
	// fmadds f0,f13,f0,f12
	f0.f64 = double(float(ctx.f13.f64 * f0.f64 + ctx.f12.f64));
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&f0.f64));
	// stfiwx f0,0,r9
	PPC_STORE_U32(ctx.r9.u32, f0.u32);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r11,9,13,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0x7FE00;
	// stw r11,1396(r31)
	PPC_STORE_U32(r31.u32 + 1396, r11.u32);
loc_821A8DF4:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// lfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lfd f31,-160(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x823ed150
	return;
}

__attribute__((alias("__imp__sub_821A8E08"))) PPC_WEAK_FUNC(sub_821A8E08);
PPC_FUNC_IMPL(__imp__sub_821A8E08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-2464(r1)
	ea = -2464 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// bl 0x821a72f0
	sub_821A72F0(ctx, base);
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r6,128(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r5,132(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// subf r9,r6,r11
	ctx.r9.s64 = r11.s64 - ctx.r6.s64;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r10,21532(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 21532);
	// lwz r8,21524(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21524);
	// subf r3,r5,r11
	ctx.r3.s64 = r11.s64 - ctx.r5.s64;
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// beq cr6,0x821a8e84
	if (cr6.getEQ()) goto loc_821A8E84;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x821a8e68
	if (cr6.getLT()) goto loc_821A8E68;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bgt cr6,0x821a8e68
	if (cr6.getGT()) goto loc_821A8E68;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_821A8E68:
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// blt cr6,0x821a8e84
	if (cr6.getLT()) goto loc_821A8E84;
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bgt cr6,0x821a8e84
	if (cr6.getGT()) goto loc_821A8E84;
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// rlwinm r6,r8,31,1,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
loc_821A8E84:
	// subf r7,r11,r10
	ctx.r7.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// lwz r8,21528(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 21528);
	// clrlwi r4,r30,16
	ctx.r4.u64 = r30.u32 & 0xFFFF;
	// rlwinm r27,r11,16,0,15
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 16) & 0xFFFF0000;
	// subf r30,r10,r8
	r30.s64 = ctx.r8.s64 - ctx.r10.s64;
	// rlwinm r8,r7,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r7,r30,31,1,31
	ctx.r7.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 31) & 0x7FFFFFFF;
	// clrlwi r30,r29,16
	r30.u64 = r29.u32 & 0xFFFF;
	// clrlwi r11,r9,16
	r11.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwimi r30,r4,16,0,15
	r30.u64 = (__builtin_rotateleft32(ctx.r4.u32, 16) & 0xFFFF0000) | (r30.u64 & 0xFFFFFFFF0000FFFF);
	// clrlwi r4,r5,16
	ctx.r4.u64 = ctx.r5.u32 & 0xFFFF;
	// clrlwi r5,r3,16
	ctx.r5.u64 = ctx.r3.u32 & 0xFFFF;
	// clrlwi r28,r10,16
	r28.u64 = ctx.r10.u32 & 0xFFFF;
	// rlwimi r5,r11,16,0,15
	ctx.r5.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (ctx.r5.u64 & 0xFFFFFFFF0000FFFF);
	// lwz r11,152(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// clrlwi r10,r6,16
	ctx.r10.u64 = ctx.r6.u32 & 0xFFFF;
	// addi r9,r1,992
	ctx.r9.s64 = ctx.r1.s64 + 992;
	// addi r6,r1,172
	ctx.r6.s64 = ctx.r1.s64 + 172;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// clrlwi r29,r7,16
	r29.u64 = ctx.r7.u32 & 0xFFFF;
	// lwz r11,168(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// rlwimi r4,r10,16,0,15
	ctx.r4.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r4.u64 & 0xFFFFFFFF0000FFFF);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// addi r10,r1,156
	ctx.r10.s64 = ctx.r1.s64 + 156;
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// addi r9,r1,116
	ctx.r9.s64 = ctx.r1.s64 + 116;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// or r7,r28,r27
	ctx.r7.u64 = r28.u64 | r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// clrlwi r11,r8,16
	r11.u64 = ctx.r8.u32 & 0xFFFF;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// rlwimi r29,r11,16,0,15
	r29.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (r29.u64 & 0xFFFFFFFF0000FFFF);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// bl 0x821a88c0
	sub_821A88C0(ctx, base);
	// lis r5,-32768
	ctx.r5.s64 = -2147483648;
	// li r4,800
	ctx.r4.s64 = 800;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x8240fb5c
	__imp__RtlFillMemoryUlong(ctx, base);
	// li r4,220
	ctx.r4.s64 = 220;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// addi r6,r1,172
	ctx.r6.s64 = ctx.r1.s64 + 172;
	// lwz r8,140(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// addi r7,r1,992
	ctx.r7.s64 = ctx.r1.s64 + 992;
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// clrlwi r26,r10,16
	r26.u64 = ctx.r10.u32 & 0xFFFF;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// clrlwi r10,r9,16
	ctx.r10.u64 = ctx.r9.u32 & 0xFFFF;
	// stw r7,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r7.u32);
	// li r9,200
	ctx.r9.s64 = 200;
	// clrlwi r4,r8,16
	ctx.r4.u64 = ctx.r8.u32 & 0xFFFF;
	// addi r8,r1,192
	ctx.r8.s64 = ctx.r1.s64 + 192;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwimi r26,r11,16,0,15
	r26.u64 = (__builtin_rotateleft32(r11.u32, 16) & 0xFFFF0000) | (r26.u64 & 0xFFFFFFFF0000FFFF);
	// stw r9,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r9.u32);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// or r6,r5,r27
	ctx.r6.u64 = ctx.r5.u64 | r27.u64;
	// rlwimi r4,r10,16,0,15
	ctx.r4.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r4.u64 & 0xFFFFFFFF0000FFFF);
	// stw r8,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r8.u32);
	// addi r9,r1,156
	ctx.r9.s64 = ctx.r1.s64 + 156;
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8240fb4c
	__imp__VdInitializeScalerCommandBuffer(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// rlwinm r30,r11,2,0,29
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// addi r1,r1,2464
	ctx.r1.s64 = ctx.r1.s64 + 2464;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821A8FD8"))) PPC_WEAK_FUNC(sub_821A8FD8);
PPC_FUNC_IMPL(__imp__sub_821A8FD8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,14812(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 14812);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821a9018
	if (cr0.getEQ()) goto loc_821A9018;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// lwz r11,36(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// rlwinm r10,r10,2,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x2;
	// clrlwi r9,r11,19
	ctx.r9.u64 = r11.u32 & 0x1FFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r11,r11,19,19,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 19) & 0x1FFF;
	// add r30,r9,r10
	r30.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// b 0x821a9020
	goto loc_821A9020;
loc_821A9018:
	// lwz r30,13528(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 13528);
	// lwz r29,13532(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 13532);
loc_821A9020:
	// addi r28,r31,13708
	r28.s64 = r31.s64 + 13708;
	// li r5,56
	ctx.r5.s64 = 56;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// ori r11,r11,16
	r11.u64 = r11.u64 | 16;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,10942(r31)
	PPC_STORE_U8(r31.u32 + 10942, r11.u8);
	// bl 0x821a7490
	sub_821A7490(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821A9058"))) PPC_WEAK_FUNC(sub_821A9058);
PPC_FUNC_IMPL(__imp__sub_821A9058) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// bl 0x821a7418
	sub_821A7418(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x821a8fd8
	sub_821A8FD8(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A9088"))) PPC_WEAK_FUNC(sub_821A9088);
PPC_FUNC_IMPL(__imp__sub_821A9088) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-2192(r1)
	ea = -2192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// bl 0x8240fb9c
	__imp__KeEnterCriticalRegion(ctx, base);
	// lis r28,-32256
	r28.s64 = -2113929216;
	// lwz r3,1560(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// bl 0x8240f8bc
	__imp__RtlEnterCriticalSection(ctx, base);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x821a90c0
	if (cr6.getEQ()) goto loc_821A90C0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8240fb8c
	__imp__VdRetrainEDRAMWorker(ctx, base);
loc_821A90C0:
	// li r11,0
	r11.s64 = 0;
	// li r8,2048
	ctx.r8.s64 = 2048;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// li r5,4096
	ctx.r5.s64 = 4096;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x8240fb7c
	__imp__VdRetrainEDRAM(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a9158
	if (cr0.getEQ()) goto loc_821A9158;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x821a9100
	if (!cr6.getEQ()) goto loc_821A9100;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d958
	sub_8219D958(ctx, base);
loc_821A9100:
	// li r4,4096
	ctx.r4.s64 = 4096;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r8,2048
	ctx.r8.s64 = 2048;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// li r5,4096
	ctx.r5.s64 = 4096;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x8240fb7c
	__imp__VdRetrainEDRAM(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x8219d958
	sub_8219D958(ctx, base);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x821a9100
	if (!cr6.getEQ()) goto loc_821A9100;
loc_821A9158:
	// lwz r3,1560(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 1560);
	// bl 0x8240f8ac
	__imp__RtlLeaveCriticalSection(ctx, base);
	// bl 0x8240fb6c
	__imp__KeLeaveCriticalRegion(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,2192
	ctx.r1.s64 = ctx.r1.s64 + 2192;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821A9170"))) PPC_WEAK_FUNC(sub_821A9170);
PPC_FUNC_IMPL(__imp__sub_821A9170) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// li r11,1
	r11.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,-9521(r10)
	PPC_STORE_U8(ctx.r10.u32 + -9521, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A9188"))) PPC_WEAK_FUNC(sub_821A9188);
PPC_FUNC_IMPL(__imp__sub_821A9188) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// li r11,0
	r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,-9521(r10)
	PPC_STORE_U8(ctx.r10.u32 + -9521, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A91A0"))) PPC_WEAK_FUNC(sub_821A91A0);
PPC_FUNC_IMPL(__imp__sub_821A91A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x823f0660
	sub_823F0660(ctx, base);
	// lis r11,1
	r11.s64 = 65536;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// subf r11,r3,r11
	r11.s64 = r11.s64 - ctx.r3.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A91D8"))) PPC_WEAK_FUNC(sub_821A91D8);
PPC_FUNC_IMPL(__imp__sub_821A91D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x823f0660
	sub_823F0660(ctx, base);
	// lis r11,2
	r11.s64 = 131072;
	// ori r11,r11,32728
	r11.u64 = r11.u64 | 32728;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bge cr6,0x821a920c
	if (!cr6.getLT()) goto loc_821A920C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823f0660
	sub_823F0660(ctx, base);
	// extsw r11,r3
	r11.s64 = ctx.r3.s32;
loc_821A920C:
	// lis r10,-32018
	ctx.r10.s64 = -2098331648;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,-31940(r10)
	PPC_STORE_U32(ctx.r10.u32 + -31940, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A9230"))) PPC_WEAK_FUNC(sub_821A9230);
PPC_FUNC_IMPL(__imp__sub_821A9230) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r31,r11,-9784
	r31.s64 = r11.s64 + -9784;
	// li r5,260
	ctx.r5.s64 = 260;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823f0600
	sub_823F0600(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,262(r31)
	PPC_STORE_U8(r31.u32 + 262, r11.u8);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A9278"))) PPC_WEAK_FUNC(sub_821A9278);
PPC_FUNC_IMPL(__imp__sub_821A9278) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// li r11,1
	r11.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,-9523(r10)
	PPC_STORE_U8(ctx.r10.u32 + -9523, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A9290"))) PPC_WEAK_FUNC(sub_821A9290);
PPC_FUNC_IMPL(__imp__sub_821A9290) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// li r11,1
	r11.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,-9524(r10)
	PPC_STORE_U8(ctx.r10.u32 + -9524, r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A92A8"))) PPC_WEAK_FUNC(sub_821A92A8);
PPC_FUNC_IMPL(__imp__sub_821A92A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	// addi r11,r3,21680
	r11.s64 = ctx.r3.s64 + 21680;
	// li r9,-1
	ctx.r9.s64 = -1;
	// li r10,41
	ctx.r10.s64 = 41;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_821A92B8:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x821a92b8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_821A92B8;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A92D0"))) PPC_WEAK_FUNC(sub_821A92D0);
PPC_FUNC_IMPL(__imp__sub_821A92D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed12c
	// stwu r1,-688(r1)
	ea = -688 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r10,r10,-6440
	ctx.r10.s64 = ctx.r10.s64 + -6440;
	// addi r28,r11,6
	r28.s64 = r11.s64 + 6;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lis r25,-32038
	r25.s64 = -2099642368;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// li r5,54
	ctx.r5.s64 = 54;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,90
	ctx.r3.s64 = ctx.r1.s64 + 90;
	// ori r25,r25,7
	r25.u64 = r25.u64 | 7;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lhz r11,8(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 8);
	// sth r11,88(r1)
	PPC_STORE_U16(ctx.r1.u32 + 88, r11.u16);
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lis r10,-32229
	ctx.r10.s64 = -2112159744;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r10,-28304
	ctx.r10.s64 = ctx.r10.s64 + -28304;
	// addi r11,r11,-6456
	r11.s64 = r11.s64 + -6456;
	// li r5,51
	ctx.r5.s64 = 51;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,161
	ctx.r3.s64 = ctx.r1.s64 + 161;
	// stw r10,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r10.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r11,12(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 12);
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
	// stb r11,160(r1)
	PPC_STORE_U8(ctx.r1.u32 + 160, r11.u8);
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lis r10,-32229
	ctx.r10.s64 = -2112159744;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r10,-28280
	ctx.r10.s64 = ctx.r10.s64 + -28280;
	// addi r11,r11,-6468
	r11.s64 = r11.s64 + -6468;
	// li r5,54
	ctx.r5.s64 = 54;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,226
	ctx.r3.s64 = ctx.r1.s64 + 226;
	// stw r10,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lhz r11,8(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// stw r10,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r10.u32);
	// sth r11,224(r1)
	PPC_STORE_U16(ctx.r1.u32 + 224, r11.u16);
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lis r10,-32229
	ctx.r10.s64 = -2112159744;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r10,-28256
	ctx.r10.s64 = ctx.r10.s64 + -28256;
	// addi r11,r11,-6488
	r11.s64 = r11.s64 + -6488;
	// addi r9,r1,284
	ctx.r9.s64 = ctx.r1.s64 + 284;
	// stw r10,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, ctx.r10.u32);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// li r11,19
	r11.s64 = 19;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_821A93C8:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x821a93c8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_821A93C8;
	// li r5,45
	ctx.r5.s64 = 45;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,303
	ctx.r3.s64 = ctx.r1.s64 + 303;
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lis r10,-32229
	ctx.r10.s64 = -2112159744;
	// addi r4,r11,-6516
	ctx.r4.s64 = r11.s64 + -6516;
	// addi r11,r10,-28200
	r11.s64 = ctx.r10.s64 + -28200;
	// addi r3,r1,352
	ctx.r3.s64 = ctx.r1.s64 + 352;
	// li r5,27
	ctx.r5.s64 = 27;
	// stw r11,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, r11.u32);
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// li r5,37
	ctx.r5.s64 = 37;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,379
	ctx.r3.s64 = ctx.r1.s64 + 379;
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lis r10,-32229
	ctx.r10.s64 = -2112159744;
	// addi r4,r11,-6544
	ctx.r4.s64 = r11.s64 + -6544;
	// addi r11,r10,-28112
	r11.s64 = ctx.r10.s64 + -28112;
	// addi r3,r1,420
	ctx.r3.s64 = ctx.r1.s64 + 420;
	// li r5,25
	ctx.r5.s64 = 25;
	// stw r11,416(r1)
	PPC_STORE_U32(ctx.r1.u32 + 416, r11.u32);
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// li r5,39
	ctx.r5.s64 = 39;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,445
	ctx.r3.s64 = ctx.r1.s64 + 445;
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lis r10,-32229
	ctx.r10.s64 = -2112159744;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r10,-28016
	ctx.r10.s64 = ctx.r10.s64 + -28016;
	// addi r11,r11,-6560
	r11.s64 = r11.s64 + -6560;
	// li r5,49
	ctx.r5.s64 = 49;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,503
	ctx.r3.s64 = ctx.r1.s64 + 503;
	// stw r10,484(r1)
	PPC_STORE_U32(ctx.r1.u32 + 484, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,488(r1)
	PPC_STORE_U32(ctx.r1.u32 + 488, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,492(r1)
	PPC_STORE_U32(ctx.r1.u32 + 492, ctx.r10.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r10,496(r1)
	PPC_STORE_U32(ctx.r1.u32 + 496, ctx.r10.u32);
	// lhz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 12);
	// lbz r11,14(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 14);
	// sth r10,500(r1)
	PPC_STORE_U16(ctx.r1.u32 + 500, ctx.r10.u16);
	// stb r11,502(r1)
	PPC_STORE_U8(ctx.r1.u32 + 502, r11.u8);
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lis r10,-32229
	ctx.r10.s64 = -2112159744;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r10,-28040
	ctx.r10.s64 = ctx.r10.s64 + -28040;
	// addi r11,r11,-6576
	r11.s64 = r11.s64 + -6576;
	// li r5,51
	ctx.r5.s64 = 51;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,569
	ctx.r3.s64 = ctx.r1.s64 + 569;
	// stw r10,552(r1)
	PPC_STORE_U32(ctx.r1.u32 + 552, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,556(r1)
	PPC_STORE_U32(ctx.r1.u32 + 556, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,560(r1)
	PPC_STORE_U32(ctx.r1.u32 + 560, ctx.r10.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r11,12(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 12);
	// stw r10,564(r1)
	PPC_STORE_U32(ctx.r1.u32 + 564, ctx.r10.u32);
	// stb r11,568(r1)
	PPC_STORE_U8(ctx.r1.u32 + 568, r11.u8);
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// lis r11,-32229
	r11.s64 = -2112159744;
	// li r29,0
	r29.s64 = 0;
	// addi r11,r11,-28016
	r11.s64 = r11.s64 + -28016;
	// addi r31,r1,80
	r31.s64 = ctx.r1.s64 + 80;
	// stw r11,620(r1)
	PPC_STORE_U32(ctx.r1.u32 + 620, r11.u32);
loc_821A94F0:
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821A94F8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821a94f8
	if (!cr6.getEQ()) goto loc_821A94F8;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// rotlwi r30,r11,0
	r30.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x823ee630
	sub_823EE630(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a955c
	if (cr0.getEQ()) goto loc_821A955C;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,68
	r31.s64 = r31.s64 + 68;
	// cmplwi cr6,r29,8
	cr6.compare<uint32_t>(r29.u32, 8, xer);
	// blt cr6,0x821a94f0
	if (cr6.getLT()) goto loc_821A94F0;
loc_821A953C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r4,r11,-14360
	ctx.r4.s64 = r11.s64 + -14360;
loc_821A9544:
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x823f0600
	sub_823F0600(ctx, base);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r1,r1,688
	ctx.r1.s64 = ctx.r1.s64 + 688;
	// b 0x823ed17c
	return;
loc_821A955C:
	// mulli r10,r29,68
	ctx.r10.s64 = r29.s64 * 68;
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// add r11,r30,r28
	r11.u64 = r30.u64 + r28.u64;
	// addi r3,r11,1
	ctx.r3.s64 = r11.s64 + 1;
	// lwzx r11,r10,r9
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r25,730
	r25.s64 = 47841280;
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821a953c
	if (cr0.getEQ()) goto loc_821A953C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r4,r11,-6584
	ctx.r4.s64 = r11.s64 + -6584;
	// b 0x821a9544
	goto loc_821A9544;
}

__attribute__((alias("__imp__sub_821A9590"))) PPC_WEAK_FUNC(sub_821A9590);
PPC_FUNC_IMPL(__imp__sub_821A9590) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed134
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// li r8,4096
	ctx.r8.s64 = 4096;
	// li r7,-1
	ctx.r7.s64 = -1;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,1028
	ctx.r5.s64 = 1028;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// li r30,0
	r30.s64 = 0;
	// bl 0x8240fbac
	__imp__MmAllocatePhysicalMemoryEx(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x821a9600
	if (!cr0.getEQ()) goto loc_821A9600;
	// lis r4,-18048
	ctx.r4.s64 = -1182793728;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a95f0
	if (cr0.getEQ()) goto loc_821A95F0;
	// li r30,1
	r30.s64 = 1;
	// b 0x821a9600
	goto loc_821A9600;
loc_821A95F0:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821a9600
	if (cr6.getEQ()) goto loc_821A9600;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// li r30,2
	r30.s64 = 2;
loc_821A9600:
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed184
	return;
}

__attribute__((alias("__imp__sub_821A9610"))) PPC_WEAK_FUNC(sub_821A9610);
PPC_FUNC_IMPL(__imp__sub_821A9610) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r31,r3,21660
	r31.s64 = ctx.r3.s64 + 21660;
	// lwz r3,592(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 592);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a9658
	if (cr0.getEQ()) goto loc_821A9658;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 612);
	// rlwinm. r11,r11,0,4,4
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821a9650
	if (!cr0.getEQ()) goto loc_821A9650;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x8240f8fc
	__imp__MmFreePhysicalMemory(ctx, base);
	// b 0x821a9658
	goto loc_821A9658;
loc_821A9650:
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_821A9658:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lis r30,-20096
	r30.s64 = -1317011456;
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a9694
	if (cr0.getEQ()) goto loc_821A9694;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 612);
	// srawi r11,r11,30
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 30;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x821a9688
	if (cr6.getLT()) goto loc_821A9688;
	// bne cr6,0x821a9694
	if (!cr6.getEQ()) goto loc_821A9694;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// b 0x821a9694
	goto loc_821A9694;
loc_821A9688:
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x8240f8fc
	__imp__MmFreePhysicalMemory(ctx, base);
loc_821A9694:
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821a96d0
	if (cr0.getEQ()) goto loc_821A96D0;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 612);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r11,r11,30
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 30;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x821a96c4
	if (cr6.getLT()) goto loc_821A96C4;
	// bne cr6,0x821a96d0
	if (!cr6.getEQ()) goto loc_821A96D0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// b 0x821a96d0
	goto loc_821A96D0;
loc_821A96C4:
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x8240f8fc
	__imp__MmFreePhysicalMemory(ctx, base);
loc_821A96D0:
	// li r5,620
	ctx.r5.s64 = 620;
	// lwz r30,616(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 616);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// addi r11,r31,20
	r11.s64 = r31.s64 + 20;
	// li r9,-1
	ctx.r9.s64 = -1;
	// stw r30,616(r31)
	PPC_STORE_U32(r31.u32 + 616, r30.u32);
	// li r10,41
	ctx.r10.s64 = 41;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_821A96F8:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x821a96f8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_821A96F8;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821A9720"))) PPC_WEAK_FUNC(sub_821A9720);
PPC_FUNC_IMPL(__imp__sub_821A9720) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// lwz r7,-1380(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + -1380);
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne cr6,0x821a974c
	if (!cr6.getEQ()) goto loc_821A974C;
	// ld r7,-1376(r8)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r8.u32 + -1376);
	// cmpdi cr6,r7,0
	cr6.compare<int64_t>(ctx.r7.s64, 0, xer);
	// bne cr6,0x821a974c
	if (!cr6.getEQ()) goto loc_821A974C;
	// stfs f1,496(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r11.u32 + 496, temp.u32);
	// lwz r8,484(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 484);
	// stw r8,492(r11)
	PPC_STORE_U32(r11.u32 + 492, ctx.r8.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r8,512(r11)
	PPC_STORE_U32(r11.u32 + 512, ctx.r8.u32);
loc_821A974C:
	// blr 
	return;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpwi cr6,r30,3
	cr6.compare<int32_t>(r30.s32, 3, xer);
	// beq cr6,0x821a9764
	if (cr6.getEQ()) goto loc_821A9764;
	// cmpwi cr6,r30,6
	cr6.compare<int32_t>(r30.s32, 6, xer);
	// bne cr6,0x821a9768
	if (!cr6.getEQ()) goto loc_821A9768;
loc_821A9764:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
loc_821A9768:
	// b 0x820f7750
	// ERROR 820F7750
	return;
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// cmpwi cr6,r26,3
	cr6.compare<int32_t>(r26.s32, 3, xer);
	// beq cr6,0x821a9780
	if (cr6.getEQ()) goto loc_821A9780;
	// cmpwi cr6,r26,6
	cr6.compare<int32_t>(r26.s32, 6, xer);
	// bne cr6,0x821a9784
	if (!cr6.getEQ()) goto loc_821A9784;
loc_821A9780:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
loc_821A9784:
	// b 0x820f7d04
	// ERROR 820F7D04
	return;
	// xori r11,r3,43
	r11.u64 = ctx.r3.u64 ^ 43;
	// lis r28,-32190
	r28.s64 = -2109603840;
	// stbu r11,569(r28)
	ea = 569 + r28.u32;
	PPC_STORE_U8(ea, r11.u8);
	r28.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// b 0x8209f5f4
	// ERROR 8209F5F4
	return;
	// cmpwi cr6,r31,3
	cr6.compare<int32_t>(r31.s32, 3, xer);
	// bne cr6,0x821a97ac
	if (!cr6.getEQ()) goto loc_821A97AC;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
loc_821A97AC:
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// b 0x820c4918
	// ERROR 820C4918
	return;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x821a97c0
	if (!cr6.getEQ()) goto loc_821A97C0;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
loc_821A97C0:
	// stw r10,-1604(r9)
	PPC_STORE_U32(ctx.r9.u32 + -1604, ctx.r10.u32);
	// b 0x820c7454
	// ERROR 820C7454
	return;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x821a97d4
	if (!cr6.getEQ()) goto loc_821A97D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_821A97D4:
	// stw r10,-1604(r9)
	PPC_STORE_U32(ctx.r9.u32 + -1604, ctx.r10.u32);
	// b 0x820c7430
	// ERROR 820C7430
	return;
	// extsh r4,r11
	ctx.r4.s64 = r11.s16;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r6,19944(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + 19944);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r3,19936(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 19936);
	// bl 0x82144920
	sub_82144920(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x820a3e9c
	// ERROR 820A3E9C
	return;
	// addi r27,r6,16
	r27.s64 = ctx.r6.s64 + 16;
	// lvlx v12,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r5,r31,424
	ctx.r5.s64 = r31.s64 + 424;
	// addi r26,r5,16
	r26.s64 = ctx.r5.s64 + 16;
	// lfs f11,16024(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16024);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f11,84(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,12
	ctx.r8.s64 = 12;
	// lfs f0,3060(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3060);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// addi r10,r11,-6632
	ctx.r10.s64 = r11.s64 + -6632;
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// lvlx v11,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v0,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// li r11,16
	r11.s64 = 16;
	// addi r28,r10,16
	r28.s64 = ctx.r10.s64 + 16;
	// li r7,48
	ctx.r7.s64 = 48;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fdivs f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 / f0.f64));
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v10,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fmuls f0,f0,f12
	f0.f64 = double(float(f0.f64 * ctx.f12.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vrlimi128 v10,v12,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// lvlx v0,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stfs f10,88(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r29,r10,32
	r29.s64 = ctx.r10.s64 + 32;
	// lvlx v12,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r30,r11
	r30.u64 = r11.u64;
	// vrlimi128 v0,v12,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v12,v10,v10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v0,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vrlimi128 v12,v13,3,2
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// vrlimi128 v10,v11,3,2
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// vor v0,v12,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// vor v13,v10,v10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r6,r31,440
	ctx.r6.s64 = r31.s64 + 440;
	// stvrx v0,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r27,r1,80
	r27.s64 = ctx.r1.s64 + 80;
	// lvrx v0,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvlx v0,0,r5
	ea = ctx.r5.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,0,r26
	ea = r26.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r26,r1,84
	r26.s64 = ctx.r1.s64 + 84;
	// lvrx v0,r11,r28
	temp.u32 = r11.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r28,32
	r28.s64 = 32;
	// lvlx v12,r10,r11
	temp.u32 = ctx.r10.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r31,456
	r11.s64 = r31.s64 + 456;
	// vor v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r19,r6,16
	r19.s64 = ctx.r6.s64 + 16;
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// addi r24,r1,84
	r24.s64 = ctx.r1.s64 + 84;
	// addi r10,r31,472
	ctx.r10.s64 = r31.s64 + 472;
	// addi r23,r1,80
	r23.s64 = ctx.r1.s64 + 80;
	// stvrx v0,0,r19
	ea = r19.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r22,r1,84
	r22.s64 = ctx.r1.s64 + 84;
	// lvlx v12,r3,r28
	temp.u32 = ctx.r3.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// lvrx v0,r30,r29
	temp.u32 = r30.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r21,r1,80
	r21.s64 = ctx.r1.s64 + 80;
	// vor v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r20,r1,84
	r20.s64 = ctx.r1.s64 + 84;
	// addi r5,r31,488
	ctx.r5.s64 = r31.s64 + 488;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stvrx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lfs f0,2688(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r27
	temp.u32 = r27.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v12,0,r26
	temp.u32 = r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f10,-6420(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -6420);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v11,0,r25
	temp.u32 = r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f10,-6424(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -6424);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v10,0,r24
	temp.u32 = r24.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vor v0,v10,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stvrx v0,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// lfs f10,12468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12468);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r23
	temp.u32 = r23.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r21
	temp.u32 = r21.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f10,12272(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12272);
	ctx.f10.f64 = double(temp.f32);
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvlx v12,0,r22
	temp.u32 = r22.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v10,0,r20
	temp.u32 = r20.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vor v0,v10,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// stvlx v0,0,r30
	ea = r30.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r30,r1,80
	r30.s64 = ctx.r1.s64 + 80;
	// stvrx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f10,14492(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14492);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f10,12900(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12900);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v12,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v10,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// lfs f10,24852(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 24852);
	ctx.f10.f64 = double(temp.f32);
	// addi r11,r31,504
	r11.s64 = r31.s64 + 504;
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vor v0,v10,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// addi r24,r11,16
	r24.s64 = r11.s64 + 16;
	// addi r29,r1,84
	r29.s64 = ctx.r1.s64 + 84;
	// addi r10,r31,520
	ctx.r10.s64 = r31.s64 + 520;
	// addi r6,r31,536
	ctx.r6.s64 = r31.s64 + 536;
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// addi r23,r10,16
	r23.s64 = ctx.r10.s64 + 16;
	// addi r22,r6,16
	r22.s64 = ctx.r6.s64 + 16;
	// addi r28,r1,80
	r28.s64 = ctx.r1.s64 + 80;
	// addi r27,r1,84
	r27.s64 = ctx.r1.s64 + 84;
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stvrx v0,0,r24
	ea = r24.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r26,r1,80
	r26.s64 = ctx.r1.s64 + 80;
	// stfs f11,80(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r25,r1,84
	r25.s64 = ctx.r1.s64 + 84;
	// lvlx v0,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f11,14220(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14220);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f11,84(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v12,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lfs f13,17872(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 17872);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v10,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vor v0,v10,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,0,r23
	ea = r23.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// stvlx v13,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,0,r22
	ea = r22.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r28
	temp.u32 = r28.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f0,-18432(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -18432);
	f0.f64 = double(temp.f32);
	// addi r11,r31,552
	r11.s64 = r31.s64 + 552;
	// lvlx v13,0,r27
	temp.u32 = r27.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,0,r26
	temp.u32 = r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// lvlx v11,0,r25
	temp.u32 = r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lhz r11,2(r9)
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,27,5,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFE0;
	// b 0x821a9e7c
	goto loc_821A9E7C;
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// bne cr6,0x821a9e84
	if (!cr6.getEQ()) goto loc_821A9E84;
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f0,2688(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	f0.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f0,96(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// std r10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r10.u64);
	// lfd f13,104(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lvlx v11,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r5,r31,424
	ctx.r5.s64 = r31.s64 + 424;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f11,2692(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2692);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// stfs f11,84(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r25,r5,16
	r25.s64 = ctx.r5.s64 + 16;
	// lvlx v13,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r5,-32254
	ctx.r5.s64 = -2113798144;
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,92
	ctx.r7.s64 = ctx.r1.s64 + 92;
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// lfs f10,16024(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16024);
	ctx.f10.f64 = double(temp.f32);
	// addi r30,r1,96
	r30.s64 = ctx.r1.s64 + 96;
	// stfs f10,92(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r29,r1,80
	r29.s64 = ctx.r1.s64 + 80;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f13,-18868(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -18868);
	ctx.f13.f64 = double(temp.f32);
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// lvlx v12,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r31,408
	ctx.r6.s64 = r31.s64 + 408;
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lis r11,-32018
	r11.s64 = -2098331648;
	// addi r27,r6,16
	r27.s64 = ctx.r6.s64 + 16;
	// addi r10,r11,-31864
	ctx.r10.s64 = r11.s64 + -31864;
	// li r11,16
	r11.s64 = 16;
	// addi r28,r10,16
	r28.s64 = ctx.r10.s64 + 16;
	// li r8,3
	ctx.r8.s64 = 3;
	// li r7,12
	ctx.r7.s64 = 12;
	// fmuls f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f13,2776(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 2776);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f9,f13,f12
	ctx.f9.f64 = double(float(ctx.f13.f64 / ctx.f12.f64));
	// stfs f9,92(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fdivs f9,f13,f12
	ctx.f9.f64 = double(float(ctx.f13.f64 / ctx.f12.f64));
	// stfs f9,96(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v0,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fmuls f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lvlx v10,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r30,r11
	r30.u64 = r11.u64;
	// vrlimi128 v10,v0,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// vrlimi128 v0,v11,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// addi r29,r10,32
	r29.s64 = ctx.r10.s64 + 32;
	// vor v11,v10,v10
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vor v10,v0,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// vrlimi128 v11,v13,3,2
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// vrlimi128 v10,v12,3,2
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vor v13,v10,v10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r6,r31,440
	ctx.r6.s64 = r31.s64 + 440;
	// stvrx v0,0,r27
	ea = r27.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lvrx v0,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvlx v0,0,r26
	ea = r26.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,0,r25
	ea = r25.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lvrx v0,r11,r28
	temp.u32 = r11.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// li r28,32
	r28.s64 = 32;
	// lvlx v12,r10,r11
	temp.u32 = ctx.r10.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r31,456
	r11.s64 = r31.s64 + 456;
	// vor v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r19,r6,16
	r19.s64 = ctx.r6.s64 + 16;
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// addi r27,r1,80
	r27.s64 = ctx.r1.s64 + 80;
	// addi r26,r1,84
	r26.s64 = ctx.r1.s64 + 84;
	// addi r25,r1,80
	r25.s64 = ctx.r1.s64 + 80;
	// stvrx v0,0,r19
	ea = r19.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r24,r1,84
	r24.s64 = ctx.r1.s64 + 84;
	// lvlx v12,r3,r28
	temp.u32 = ctx.r3.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r31,472
	ctx.r10.s64 = r31.s64 + 472;
	// lvrx v0,r30,r29
	temp.u32 = r30.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r23,r1,80
	r23.s64 = ctx.r1.s64 + 80;
	// vor v0,v12,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// addi r22,r1,84
	r22.s64 = ctx.r1.s64 + 84;
	// addi r21,r1,80
	r21.s64 = ctx.r1.s64 + 80;
	// addi r20,r1,84
	r20.s64 = ctx.r1.s64 + 84;
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stvrx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r5,r31,488
	ctx.r5.s64 = r31.s64 + 488;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lvlx v0,0,r27
	temp.u32 = r27.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lfs f12,-6420(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -6420);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r26
	temp.u32 = r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r25
	temp.u32 = r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lfs f12,-6424(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -6424);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v10,0,r24
	temp.u32 = r24.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vor v0,v10,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stvrx v0,0,r3
	ea = ctx.r3.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// lfs f12,12468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12468);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r23
	temp.u32 = r23.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r21
	temp.u32 = r21.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f12,12272(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12272);
	ctx.f12.f64 = double(temp.f32);
	// addi r11,r5,16
	r11.s64 = ctx.r5.s64 + 16;
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvlx v12,0,r22
	temp.u32 = r22.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v10,0,r20
	temp.u32 = r20.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vor v0,v10,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// stvlx v0,0,r30
	ea = r30.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f12,14492(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14492);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f12,12900(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12900);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v12,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v10,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// lfs f12,24852(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 24852);
	ctx.f12.f64 = double(temp.f32);
	// addi r11,r31,504
	r11.s64 = r31.s64 + 504;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vor v0,v10,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// addi r24,r11,16
	r24.s64 = r11.s64 + 16;
	// addi r30,r1,80
	r30.s64 = ctx.r1.s64 + 80;
	// addi r29,r1,84
	r29.s64 = ctx.r1.s64 + 84;
	// addi r10,r31,520
	ctx.r10.s64 = r31.s64 + 520;
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// addi r6,r31,536
	ctx.r6.s64 = r31.s64 + 536;
	// addi r23,r10,16
	r23.s64 = ctx.r10.s64 + 16;
	// addi r22,r6,16
	r22.s64 = ctx.r6.s64 + 16;
	// addi r28,r1,80
	r28.s64 = ctx.r1.s64 + 80;
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stvrx v0,0,r24
	ea = r24.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r27,r1,84
	r27.s64 = ctx.r1.s64 + 84;
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r26,r1,80
	r26.s64 = ctx.r1.s64 + 80;
	// lvlx v0,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r25,r1,84
	r25.s64 = ctx.r1.s64 + 84;
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f12,14220(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 14220);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v12,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v0,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lfs f13,17872(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 17872);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v10,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vor v0,v10,v10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,0,r23
	ea = r23.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// stvlx v13,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v13.u8[15 - i]);
	// stvrx v13,0,r22
	ea = r22.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v13.u8[i]);
	// lfs f13,28824(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 28824);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r28
	temp.u32 = r28.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f0,-18432(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -18432);
	f0.f64 = double(temp.f32);
	// addi r11,r31,552
	r11.s64 = r31.s64 + 552;
	// lvlx v13,0,r27
	temp.u32 = r27.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,0,r26
	temp.u32 = r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r11,16
	ctx.r10.s64 = r11.s64 + 16;
	// lvlx v11,0,r25
	temp.u32 = r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// stvrx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lhz r11,2(r9)
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lhz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,23,9,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 23) & 0x7FFFFE;
loc_821A9E7C:
	// stw r11,376(r31)
	PPC_STORE_U32(r31.u32 + 376, r11.u32);
	// b 0x821aa21c
	goto loc_821AA21C;
loc_821A9E84:
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// bne cr6,0x821aa214
	if (!cr6.getEQ()) goto loc_821AA214;
	// lis r11,-32018
	r11.s64 = -2098331648;
	// addi r9,r31,424
	ctx.r9.s64 = r31.s64 + 424;
	// addi r10,r11,-31768
	ctx.r10.s64 = r11.s64 + -31768;
	// li r11,16
	r11.s64 = 16;
	// addi r19,r9,16
	r19.s64 = ctx.r9.s64 + 16;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// mr r30,r11
	r30.u64 = r11.u64;
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r29,r10,16
	r29.s64 = ctx.r10.s64 + 16;
	// lvrx v0,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// mr r28,r11
	r28.u64 = r11.u64;
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r6,r31,440
	ctx.r6.s64 = r31.s64 + 440;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
	// addi r18,r6,16
	r18.s64 = ctx.r6.s64 + 16;
	// mr r26,r11
	r26.u64 = r11.u64;
	// stvlx v0,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r25,r10,32
	r25.s64 = ctx.r10.s64 + 32;
	// stvrx v0,0,r19
	ea = r19.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// li r24,32
	r24.s64 = 32;
	// lvlx v13,r3,r28
	temp.u32 = ctx.r3.u32 + r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r31,456
	r11.s64 = r31.s64 + 456;
	// lvrx v0,r30,r29
	temp.u32 = r30.u32 + r29.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r23,r1,80
	r23.s64 = ctx.r1.s64 + 80;
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r11,16
	ctx.r9.s64 = r11.s64 + 16;
	// addi r22,r1,84
	r22.s64 = ctx.r1.s64 + 84;
	// addi r21,r1,80
	r21.s64 = ctx.r1.s64 + 80;
	// addi r20,r1,84
	r20.s64 = ctx.r1.s64 + 84;
	// stvlx v0,0,r6
	ea = ctx.r6.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r10,r31,472
	ctx.r10.s64 = r31.s64 + 472;
	// stvrx v0,0,r18
	ea = r18.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lvlx v13,r27,r24
	temp.u32 = r27.u32 + r24.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvrx v0,r26,r25
	temp.u32 = r26.u32 + r25.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// addi r30,r1,84
	r30.s64 = ctx.r1.s64 + 84;
	// vor v0,v13,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// li r8,3
	ctx.r8.s64 = 3;
	// li r7,6
	ctx.r7.s64 = 6;
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stvrx v0,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfs f0,2688(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2688);
	f0.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r23
	temp.u32 = r23.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v13,0,r22
	temp.u32 = r22.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f13,-6420(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -6420);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,0,r21
	temp.u32 = r21.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f13,-6424(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -6424);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r31,488
	r11.s64 = r31.s64 + 488;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v11,0,r20
	temp.u32 = r20.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stvrx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f13,14492(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 14492);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f13,28808(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28808);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v11,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f12,2776(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2776);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r24,r11,16
	r24.s64 = r11.s64 + 16;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// addi r30,r1,80
	r30.s64 = ctx.r1.s64 + 80;
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// addi r29,r1,84
	r29.s64 = ctx.r1.s64 + 84;
	// addi r10,r31,504
	ctx.r10.s64 = r31.s64 + 504;
	// addi r28,r1,80
	r28.s64 = ctx.r1.s64 + 80;
	// addi r23,r10,16
	r23.s64 = ctx.r10.s64 + 16;
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stvrx v0,0,r24
	ea = r24.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r27,r1,84
	r27.s64 = ctx.r1.s64 + 84;
	// addi r26,r1,80
	r26.s64 = ctx.r1.s64 + 80;
	// addi r25,r1,84
	r25.s64 = ctx.r1.s64 + 84;
	// addi r9,r31,520
	ctx.r9.s64 = r31.s64 + 520;
	// lfs f12,3908(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3908);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lfs f12,12900(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12900);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v13,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lfs f11,3060(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3060);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f11,80(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,84
	r30.s64 = ctx.r1.s64 + 84;
	// lfs f10,12468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12468);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v11,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stvrx v0,0,r23
	ea = r23.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lfs f10,-18868(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -18868);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r28
	temp.u32 = r28.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f10,17872(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 17872);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v13,0,r27
	temp.u32 = r27.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lfs f10,-18432(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -18432);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r26
	temp.u32 = r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f10,28824(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 28824);
	ctx.f10.f64 = double(temp.f32);
	// addi r11,r31,552
	r11.s64 = r31.s64 + 552;
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v11,0,r25
	temp.u32 = r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// stvlx v0,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// stvrx v0,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lfs f12,2692(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2692);
	ctx.f12.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f12,-6428(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -6428);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v11,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// addi r24,r11,16
	r24.s64 = r11.s64 + 16;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// addi r30,r1,80
	r30.s64 = ctx.r1.s64 + 80;
	// addi r29,r1,84
	r29.s64 = ctx.r1.s64 + 84;
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// addi r10,r31,408
	ctx.r10.s64 = r31.s64 + 408;
	// addi r28,r1,80
	r28.s64 = ctx.r1.s64 + 80;
	// addi r23,r10,16
	r23.s64 = ctx.r10.s64 + 16;
	// addi r27,r1,84
	r27.s64 = ctx.r1.s64 + 84;
	// stvlx v0,0,r11
	ea = r11.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stvrx v0,0,r24
	ea = r24.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// addi r26,r1,80
	r26.s64 = ctx.r1.s64 + 80;
	// addi r25,r1,84
	r25.s64 = ctx.r1.s64 + 84;
	// addi r9,r31,536
	ctx.r9.s64 = r31.s64 + 536;
	// lfs f12,2772(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2772);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f11,80(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f12,16004(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16004);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v13,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lfs f12,25792(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 25792);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v11,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// stvlx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// addi r10,r9,16
	ctx.r10.s64 = ctx.r9.s64 + 16;
	// stvrx v0,0,r23
	ea = r23.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f13,2956(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2956);
	ctx.f13.f64 = double(temp.f32);
	// li r11,48
	r11.s64 = 48;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r28
	temp.u32 = r28.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// lvlx v13,0,r27
	temp.u32 = r27.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v12,0,r26
	temp.u32 = r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r25
	temp.u32 = r25.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v0,v11,v11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// vrlimi128 v0,v13,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// stvlx v0,0,r9
	ea = ctx.r9.u32;
	for (size_t i = 0; i < (16 - (ea & 0xF)); i++)
		PPC_STORE_U8(ea + i, ctx.v0.u8[15 - i]);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// stvrx v0,0,r10
	ea = ctx.r10.u32;
	for (size_t i = 0; i < (ea & 0xF); i++)
		PPC_STORE_U8(ea - i - 1, ctx.v0.u8[i]);
	// lhz r10,2(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 2);
	// lhz r11,0(r5)
	r11.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r10,r10,-48
	ctx.r10.s64 = ctx.r10.s64 + -48;
	// addi r11,r11,-80
	r11.s64 = r11.s64 + -80;
	// divwu r10,r10,r6
	ctx.r10.u32 = ctx.r10.u32 / ctx.r6.u32;
	// divwu r11,r11,r9
	r11.u32 = r11.u32 / ctx.r9.u32;
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r11,r11,6,0,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// b 0x821a9e7c
	goto loc_821A9E7C;
loc_821AA214:
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_821AA21C:
	// lis r10,-32018
	ctx.r10.s64 = -2098331648;
	// lwz r11,376(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 376);
	// li r5,260
	ctx.r5.s64 = 260;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lwz r10,-31940(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -31940);
	// rldicr r30,r10,20,63
	r30.u64 = __builtin_rotateleft64(ctx.r10.u64, 20) & 0xFFFFFFFFFFFFFFFF;
	// mullw r10,r11,r7
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// mullw r11,r11,r8
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r8.s32);
	// stw r11,388(r31)
	PPC_STORE_U32(r31.u32 + 388, r11.u32);
	// addi r10,r10,511
	ctx.r10.s64 = ctx.r10.s64 + 511;
	// rlwinm r11,r10,23,9,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 23) & 0x7FFFFF;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// addi r10,r10,172
	ctx.r10.s64 = ctx.r10.s64 + 172;
	// sth r11,14(r31)
	PPC_STORE_U16(r31.u32 + 14, r11.u16);
	// rlwinm r28,r10,9,0,22
	r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 9) & 0xFFFFFE00;
	// bl 0x823f0600
	sub_823F0600(ctx, base);
	// li r27,0
	r27.s64 = 0;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// stb r27,371(r1)
	PPC_STORE_U8(ctx.r1.u32 + 371, r27.u8);
loc_821AA270:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821aa270
	if (!cr6.getEQ()) goto loc_821AA270;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x821aa2c0
	if (cr6.getLT()) goto loc_821AA2C0;
loc_821AA2A4:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r10,92
	cr6.compare<uint32_t>(ctx.r10.u32, 92, xer);
	// beq cr6,0x821aa2c0
	if (cr6.getEQ()) goto loc_821AA2C0;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x821aa2a4
	if (!cr6.getLT()) goto loc_821AA2A4;
loc_821AA2C0:
	// li r6,0
	ctx.r6.s64 = 0;
	// stb r27,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r27.u8);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8235ebb0
	sub_8235EBB0(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821aa30c
	if (cr0.getEQ()) goto loc_821AA30C;
	// lis r10,640
	ctx.r10.s64 = 41943040;
	// ld r11,104(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// add r9,r30,r10
	ctx.r9.u64 = r30.u64 + ctx.r10.u64;
	// cmpld cr6,r11,r9
	cr6.compare<uint64_t>(r11.u64, ctx.r9.u64, xer);
	// bge cr6,0x821aa30c
	if (!cr6.getLT()) goto loc_821AA30C;
	// cmpld cr6,r11,r10
	cr6.compare<uint64_t>(r11.u64, ctx.r10.u64, xer);
	// ble cr6,0x821aa308
	if (!cr6.getGT()) goto loc_821AA308;
	// lis r10,-640
	ctx.r10.s64 = -41943040;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// b 0x821aa30c
	goto loc_821AA30C;
loc_821AA308:
	// mr r30,r27
	r30.u64 = r27.u64;
loc_821AA30C:
	// addi r11,r28,2048
	r11.s64 = r28.s64 + 2048;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// cmpld cr6,r11,r30
	cr6.compare<uint64_t>(r11.u64, r30.u64, xer);
	// ble cr6,0x821aa328
	if (!cr6.getGT()) goto loc_821AA328;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// b 0x821aa454
	goto loc_821AA454;
loc_821AA328:
	// lis r11,-17
	r11.s64 = -1114112;
	// li r9,0
	ctx.r9.s64 = 0;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// oris r8,r9,65520
	ctx.r8.u64 = ctx.r9.u64 | 4293918720;
	// clrldi r10,r11,32
	ctx.r10.u64 = r11.u64 & 0xFFFFFFFF;
	// lwz r11,596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 596);
	// rotlwi r9,r30,0
	ctx.r9.u64 = __builtin_rotateleft32(r30.u32, 0);
	// add r10,r30,r10
	ctx.r10.u64 = r30.u64 + ctx.r10.u64;
	// mr r29,r27
	r29.u64 = r27.u64;
	// divdu r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 / ctx.r8.u64;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// rlwimi r11,r10,14,12,17
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 14) & 0xFC000) | (r11.u64 & 0xFFFFFFFFFFF03FFF);
	// rlwinm r10,r11,18,26,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x3F;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r11,596(r31)
	PPC_STORE_U32(r31.u32 + 596, r11.u32);
	// rlwinm r10,r10,20,0,11
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFFF00000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,384(r31)
	PPC_STORE_U32(r31.u32 + 384, ctx.r10.u32);
loc_821AA370:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821aa38c
	if (cr6.getEQ()) goto loc_821AA38C;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 612);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// srawi r11,r11,30
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 30;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x821aa390
	if (cr6.getEQ()) goto loc_821AA390;
loc_821AA38C:
	// lwz r4,616(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 616);
loc_821AA390:
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// add r30,r11,r31
	r30.u64 = r11.u64 + r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x821a9590
	sub_821A9590(ctx, base);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821aa44c
	if (cr6.getEQ()) goto loc_821AA44C;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 612);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// bne cr6,0x821aa3cc
	if (!cr6.getEQ()) goto loc_821AA3CC;
	// rlwimi r11,r10,30,0,1
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 30) & 0xC0000000) | (r11.u64 & 0xFFFFFFFF3FFFFFFF);
	// b 0x821aa3d0
	goto loc_821AA3D0;
loc_821AA3CC:
	// rlwimi r11,r10,28,2,3
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 28) & 0x30000000) | (r11.u64 & 0xFFFFFFFFCFFFFFFF);
loc_821AA3D0:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// stw r11,612(r31)
	PPC_STORE_U32(r31.u32 + 612, r11.u32);
	// cmplwi cr6,r29,2
	cr6.compare<uint32_t>(r29.u32, 2, xer);
	// blt cr6,0x821aa370
	if (cr6.getLT()) goto loc_821AA370;
	// li r8,4096
	ctx.r8.s64 = 4096;
	// li r7,-1
	ctx.r7.s64 = -1;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,1536
	ctx.r4.s64 = 1536;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x8240fbac
	__imp__MmAllocatePhysicalMemoryEx(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,592(r31)
	PPC_STORE_U32(r31.u32 + 592, ctx.r3.u32);
	// beq 0x821aa414
	if (cr0.getEQ()) goto loc_821AA414;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 612);
	// rlwinm r11,r11,0,5,3
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
	// b 0x821aa434
	goto loc_821AA434;
loc_821AA414:
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// li r3,1536
	ctx.r3.s64 = 1536;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,592(r31)
	PPC_STORE_U32(r31.u32 + 592, ctx.r3.u32);
	// beq 0x821aa44c
	if (cr0.getEQ()) goto loc_821AA44C;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 612);
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
loc_821AA434:
	// stw r11,612(r31)
	PPC_STORE_U32(r31.u32 + 612, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r11,608(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 608);
	// ori r11,r11,16
	r11.u64 = r11.u64 | 16;
	// stb r11,608(r31)
	PPC_STORE_U8(r31.u32 + 608, r11.u8);
	// b 0x821aa454
	goto loc_821AA454;
loc_821AA44C:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
loc_821AA454:
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// b 0x823ed160
	return;
}

__attribute__((alias("__imp__sub_821AA460"))) PPC_WEAK_FUNC(sub_821AA460);
PPC_FUNC_IMPL(__imp__sub_821AA460) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,596(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 596);
	// lwz r11,380(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 380);
	// lhz r9,14(r3)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// rlwinm r8,r10,12,26,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x3F;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// rlwinm r10,r10,18,26,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3F;
	// addi r9,r9,172
	ctx.r9.s64 = ctx.r9.s64 + 172;
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// rlwinm r9,r9,9,0,22
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 9) & 0xFFFFFE00;
	// stw r11,380(r3)
	PPC_STORE_U32(ctx.r3.u32 + 380, r11.u32);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + r11.u64;
	// bne cr6,0x821aa49c
	if (!cr6.getEQ()) goto loc_821AA49C;
	// lwz r9,384(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 384);
	// b 0x821aa4a0
	goto loc_821AA4A0;
loc_821AA49C:
	// lis r9,-16
	ctx.r9.s64 = -1048576;
loc_821AA4A0:
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x821aa4b8
	if (cr6.getLT()) goto loc_821AA4B8;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blelr cr6
	if (!cr6.getGT()) return;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beqlr cr6
	if (cr6.getEQ()) return;
loc_821AA4B8:
	// addi r10,r8,46
	ctx.r10.s64 = ctx.r8.s64 + 46;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r10,r3
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, r11.u32);
	// lwz r11,596(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 596);
	// rlwinm r10,r11,12,26,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3F;
	// rlwinm r9,r11,18,26,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x3F;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// twllei r9,0
	// divwu r8,r10,r9
	ctx.r8.u32 = ctx.r10.u32 / ctx.r9.u32;
	// mullw r9,r8,r9
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// rlwimi r11,r10,20,6,11
	r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 20) & 0x3F00000) | (r11.u64 & 0xFFFFFFFFFC0FFFFF);
	// stw r11,596(r3)
	PPC_STORE_U32(ctx.r3.u32 + 596, r11.u32);
	// rlwinm. r10,r11,0,6,11
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x3F00000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821aa50c
	if (!cr0.getEQ()) goto loc_821AA50C;
	// lbz r11,608(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 608);
	// li r10,2048
	ctx.r10.s64 = 2048;
	// ori r11,r11,32
	r11.u64 = r11.u64 | 32;
	// stw r10,380(r3)
	PPC_STORE_U32(ctx.r3.u32 + 380, ctx.r10.u32);
	// stb r11,608(r3)
	PPC_STORE_U8(ctx.r3.u32 + 608, r11.u8);
	// blr 
	return;
loc_821AA50C:
	// li r11,0
	r11.s64 = 0;
	// stw r11,380(r3)
	PPC_STORE_U32(ctx.r3.u32 + 380, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AA518"))) PPC_WEAK_FUNC(sub_821AA518);
PPC_FUNC_IMPL(__imp__sub_821AA518) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed128
	// stwu r1,-2320(r1)
	ea = -2320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// addi r31,r25,21660
	r31.s64 = r25.s64 + 21660;
	// lbz r11,608(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 608);
	// rlwinm. r10,r11,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821aa8a4
	if (cr0.getEQ()) goto loc_821AA8A4;
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 364);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821aa87c
	if (cr0.getEQ()) goto loc_821AA87C;
	// rlwinm. r10,r11,0,0,24
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r27,-1
	r27.s64 = -1;
	// beq 0x821aa668
	if (cr0.getEQ()) goto loc_821AA668;
	// rlwinm. r11,r11,0,25,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821aa564
	if (cr0.getEQ()) goto loc_821AA564;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x8235eba8
	sub_8235EBA8(ctx, base);
loc_821AA564:
	// lwz r28,16(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r3,588(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 588);
	// bl 0x8219d298
	sub_8219D298(ctx, base);
	// lis r11,-32018
	r11.s64 = -2098331648;
	// addi r29,r11,-31944
	r29.s64 = r11.s64 + -31944;
	// lis r11,-31987
	r11.s64 = -2096300032;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// addi r11,r11,-15360
	r11.s64 = r11.s64 + -15360;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
loc_821AA588:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// stwcx. r11,0,r8
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r9,1
	// bne 0x821aa588
	if (!cr0.getEQ()) goto loc_821AA588;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r9,6144
	ctx.r9.s64 = 6144;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// divwu r10,r11,r9
	ctx.r10.u32 = r11.u32 / ctx.r9.u32;
	// cmplwi cr6,r10,14
	cr6.compare<uint32_t>(ctx.r10.u32, 14, xer);
	// blt cr6,0x821aa5c0
	if (cr6.getLT()) goto loc_821AA5C0;
	// li r10,14
	ctx.r10.s64 = 14;
loc_821AA5C0:
	// lwz r9,584(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 584);
	// mulli r8,r10,12
	ctx.r8.s64 = ctx.r10.s64 * 12;
	// addi r7,r9,-1
	ctx.r7.s64 = ctx.r9.s64 + -1;
	// lwz r6,380(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// lhz r9,12(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 12);
	// lis r11,-25768
	r11.s64 = -1688731648;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// ori r24,r11,59162
	r24.u64 = r11.u64 | 59162;
	// lwz r11,596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 596);
	// rlwinm r7,r7,2,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0x4;
	// rlwinm r30,r9,9,0,22
	r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 9) & 0xFFFFFE00;
	// rlwinm r9,r11,4,30,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0x3;
	// rlwinm r11,r11,12,26,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3F;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// lwzx r4,r7,r31
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + r31.u32);
	// stw r6,356(r31)
	PPC_STORE_U32(r31.u32 + 356, ctx.r6.u32);
	// addi r7,r31,348
	ctx.r7.s64 = r31.s64 + 348;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r26,360(r31)
	PPC_STORE_U32(r31.u32 + 360, r26.u32);
	// addi r11,r4,-4
	r11.s64 = ctx.r4.s64 + -4;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lwzx r3,r8,r31
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// stwu r24,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r24.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r28,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r28.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// bl 0x8235e018
	sub_8235E018(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821aa460
	sub_821AA460(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 364);
	// bl 0x8235eba8
	sub_8235EBA8(ctx, base);
	// lis r30,-31987
	r30.s64 = -2096300032;
	// b 0x821aa658
	goto loc_821AA658;
loc_821AA650:
	// li r3,6
	ctx.r3.s64 = 6;
	// bl 0x8235eb58
	sub_8235EB58(ctx, base);
loc_821AA658:
	// lwz r11,-15352(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + -15352);
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x821aa650
	if (!cr6.getEQ()) goto loc_821AA650;
loc_821AA668:
	// lwz r11,596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 596);
	// mr r30,r26
	r30.u64 = r26.u64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// rlwinm. r11,r11,0,12,17
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFC000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821aa6e4
	if (cr0.getEQ()) goto loc_821AA6E4;
	// addi r29,r31,20
	r29.s64 = r31.s64 + 20;
loc_821AA680:
	// lbz r11,608(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 608);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821aa6b0
	if (!cr0.getEQ()) goto loc_821AA6B0;
	// lwz r11,596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 596);
	// rlwinm r11,r11,12,26,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3F;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x821aa6b0
	if (cr6.getLT()) goto loc_821AA6B0;
	// ble cr6,0x821aa6a8
	if (!cr6.getGT()) goto loc_821AA6A8;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// b 0x821aa6b4
	goto loc_821AA6B4;
loc_821AA6A8:
	// lwz r4,380(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// b 0x821aa6b4
	goto loc_821AA6B4;
loc_821AA6B0:
	// lwz r4,164(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 164);
loc_821AA6B4:
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// bl 0x8235ed68
	sub_8235ED68(ctx, base);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// bl 0x8235ecc0
	sub_8235ECC0(ctx, base);
	// lwz r11,596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 596);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r11,r11,18,26,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x3F;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x821aa680
	if (cr6.getLT()) goto loc_821AA680;
loc_821AA6E4:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8240fa5c
	__imp__VdGetCurrentDisplayInformation(ctx, base);
	// lwz r11,604(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 604);
	// srawi. r11,r11,29
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 29;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821aa704
	if (!cr0.getEQ()) goto loc_821AA704;
	// lis r10,21415
	ctx.r10.s64 = 1403453440;
	// ori r10,r10,8884
	ctx.r10.u64 = ctx.r10.u64 | 8884;
	// b 0x821aa728
	goto loc_821AA728;
loc_821AA704:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821aa718
	if (!cr6.getEQ()) goto loc_821AA718;
	// lis r10,21415
	ctx.r10.s64 = 1403453440;
	// ori r10,r10,8885
	ctx.r10.u64 = ctx.r10.u64 | 8885;
	// b 0x821aa728
	goto loc_821AA728;
loc_821AA718:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x821aa72c
	if (!cr6.getEQ()) goto loc_821AA72C;
	// lis r10,21415
	ctx.r10.s64 = 1403453440;
	// ori r10,r10,8886
	ctx.r10.u64 = ctx.r10.u64 | 8886;
loc_821AA728:
	// stw r10,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r10.u32);
loc_821AA72C:
	// lis r10,1
	ctx.r10.s64 = 65536;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 | 1;
	// stw r10,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r10.u32);
	// beq cr6,0x821aa768
	if (cr6.getEQ()) goto loc_821AA768;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x821aa768
	if (cr6.getEQ()) goto loc_821AA768;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x821aa788
	if (!cr6.getEQ()) goto loc_821AA788;
	// li r10,400
	ctx.r10.s64 = 400;
	// li r11,224
	r11.s64 = 224;
	// stw r10,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r10.u32);
	// stw r11,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r11.u32);
	// stw r10,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r10.u32);
	// b 0x821aa784
	goto loc_821AA784;
loc_821AA768:
	// lhz r11,368(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 368);
	// stw r11,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r11.u32);
	// lhz r11,370(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 370);
	// stw r11,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r11.u32);
	// lhz r11,168(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 168);
	// stw r11,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r11.u32);
	// lhz r11,170(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 170);
loc_821AA784:
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r11.u32);
loc_821AA788:
	// bl 0x8240f8ec
	__imp__KeQueryPerformanceFrequency(ctx, base);
	// lwz r10,596(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 596);
	// mr r9,r26
	ctx.r9.u64 = r26.u64;
	// stw r3,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r3.u32);
	// rlwinm r11,r10,6,26,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0x3F;
	// stw r9,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r9.u32);
	// clrlwi. r8,r11,31
	ctx.r8.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821aa7b0
	if (cr0.getEQ()) goto loc_821AA7B0;
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r9,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r9.u32);
loc_821AA7B0:
	// rlwinm. r11,r11,0,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821aa7c0
	if (cr0.getEQ()) goto loc_821AA7C0;
	// ori r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 2;
	// stw r9,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r9.u32);
loc_821AA7C0:
	// rlwinm. r11,r10,0,0,0
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821aa7d0
	if (cr0.getEQ()) goto loc_821AA7D0;
	// lwz r4,592(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 592);
	// b 0x821aa7d4
	goto loc_821AA7D4;
loc_821AA7D0:
	// addi r4,r25,14988
	ctx.r4.s64 = r25.s64 + 14988;
loc_821AA7D4:
	// lbz r11,101(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 101);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x821aa7f8
	if (!cr6.getEQ()) goto loc_821AA7F8;
	// ori r11,r9,4
	r11.u64 = ctx.r9.u64 | 4;
	// stw r11,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r11.u32);
	// bl 0x8219e228
	sub_8219E228(ctx, base);
	// b 0x821aa7fc
	goto loc_821AA7FC;
loc_821AA7F8:
	// bl 0x8219e190
	sub_8219E190(ctx, base);
loc_821AA7FC:
	// lwz r30,596(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 596);
	// addi r3,r1,232
	ctx.r3.s64 = ctx.r1.s64 + 232;
	// li r5,56
	ctx.r5.s64 = 56;
	// addi r4,r25,13708
	ctx.r4.s64 = r25.s64 + 13708;
	// rlwinm. r11,r30,0,5,5
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 0) & 0x4000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821aa818
	if (!cr0.getEQ()) goto loc_821AA818;
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
loc_821AA818:
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// lbz r11,608(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 608);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821aa83c
	if (cr0.getEQ()) goto loc_821AA83C;
	// lwz r11,380(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// stw r11,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, r11.u32);
	// rlwinm r11,r30,12,26,31
	r11.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 12) & 0x3F;
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r11.u32);
	// b 0x821aa848
	goto loc_821AA848;
loc_821AA83C:
	// li r11,2048
	r11.s64 = 2048;
	// stw r26,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r26.u32);
	// stw r11,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, r11.u32);
loc_821AA848:
	// addi r7,r31,348
	ctx.r7.s64 = r31.s64 + 348;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// stw r26,356(r31)
	PPC_STORE_U32(r31.u32 + 356, r26.u32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// stw r26,360(r31)
	PPC_STORE_U32(r31.u32 + 360, r26.u32);
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// bl 0x8235e018
	sub_8235E018(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 364);
	// bl 0x8235eba8
	sub_8235EBA8(ctx, base);
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 364);
	// bl 0x8235d388
	sub_8235D388(ctx, base);
loc_821AA87C:
	// mr r30,r26
	r30.u64 = r26.u64;
	// addi r31,r31,20
	r31.s64 = r31.s64 + 20;
loc_821AA884:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x821aa8a4
	if (cr6.getEQ()) goto loc_821AA8A4;
	// bl 0x8235d388
	sub_8235D388(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplwi cr6,r30,41
	cr6.compare<uint32_t>(r30.u32, 41, xer);
	// blt cr6,0x821aa884
	if (cr6.getLT()) goto loc_821AA884;
loc_821AA8A4:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1772(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1772);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x821aa8d4
	if (cr6.getEQ()) goto loc_821AA8D4;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821aa900
	if (cr0.getEQ()) goto loc_821AA900;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821aa900
	if (cr0.getEQ()) goto loc_821AA900;
	// b 0x821aa8ec
	goto loc_821AA8EC;
loc_821AA8D4:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1756(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1756);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821aa900
	if (cr0.getEQ()) goto loc_821AA900;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
loc_821AA8EC:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// li r3,27
	ctx.r3.s64 = 27;
	// addi r4,r10,-6416
	ctx.r4.s64 = ctx.r10.s64 + -6416;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AA900:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x821a9610
	sub_821A9610(ctx, base);
	// lis r11,-32018
	r11.s64 = -2098331648;
	// addi r3,r11,-31920
	ctx.r3.s64 = r11.s64 + -31920;
	// bl 0x8240fbbc
	__imp__ObDeleteSymbolicLink(ctx, base);
	// lis r11,-31991
	r11.s64 = -2096562176;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// addi r11,r11,-9522
	r11.s64 = r11.s64 + -9522;
	// stb r10,-2(r11)
	PPC_STORE_U8(r11.u32 + -2, ctx.r10.u8);
	// stb r10,-1(r11)
	PPC_STORE_U8(r11.u32 + -1, ctx.r10.u8);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r1,r1,2320
	ctx.r1.s64 = ctx.r1.s64 + 2320;
	// b 0x823ed178
	return;
}

__attribute__((alias("__imp__sub_821AA938"))) PPC_WEAK_FUNC(sub_821AA938);
PPC_FUNC_IMPL(__imp__sub_821AA938) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed11c
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r6,16
	ctx.r6.s64 = 16;
	// li r5,112
	ctx.r5.s64 = 112;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r30,r31,21660
	r30.s64 = r31.s64 + 21660;
	// bl 0x8218e450
	sub_8218E450(ctx, base);
	// ld r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// cmpldi cr6,r4,0
	cr6.compare<uint64_t>(ctx.r4.u64, 0, xer);
	// beq cr6,0x821aaa80
	if (cr6.getEQ()) goto loc_821AAA80;
	// ld r11,40(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 40);
	// and r11,r11,r4
	r11.u64 = r11.u64 & ctx.r4.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x821aa98c
	if (cr6.getEQ()) goto loc_821AA98C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,10560(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 10560);
	// bl 0x821a09a8
	sub_821A09A8(ctx, base);
	// std r3,16(r31)
	PPC_STORE_U64(r31.u32 + 16, ctx.r3.u64);
loc_821AA98C:
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// clrldi r10,r11,52
	ctx.r10.u64 = r11.u64 & 0xFFF;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821aa9bc
	if (cr6.getEQ()) goto loc_821AA9BC;
	// addi r6,r31,10548
	ctx.r6.s64 = r31.s64 + 10548;
	// li r5,8704
	ctx.r5.s64 = 8704;
	// rldicr r4,r11,52,11
	ctx.r4.u64 = __builtin_rotateleft64(r11.u64, 52) & 0xFFF0000000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a0d40
	sub_821A0D40(ctx, base);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// rldicr r11,r11,0,51
	r11.u64 = __builtin_rotateleft64(r11.u64, 0) & 0xFFFFFFFFFFFFF000;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_821AA9BC:
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// rlwinm r10,r11,0,15,19
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1F000;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821aa9f4
	if (cr6.getEQ()) goto loc_821AA9F4;
	// addi r6,r31,10528
	ctx.r6.s64 = r31.s64 + 10528;
	// li r5,8576
	ctx.r5.s64 = 8576;
	// rldicr r4,r11,47,4
	ctx.r4.u64 = __builtin_rotateleft64(r11.u64, 47) & 0xF800000000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a0d40
	sub_821A0D40(ctx, base);
	// lis r12,-2
	r12.s64 = -131072;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,4095
	r12.u64 = r12.u64 | 4095;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_821AA9F4:
	// lis r12,0
	r12.s64 = 0;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,65535
	r12.u64 = r12.u64 | 65535;
	// rldicr r12,r12,42,21
	r12.u64 = __builtin_rotateleft64(r12.u64, 42) & 0xFFFFFC0000000000;
	// and r10,r11,r12
	ctx.r10.u64 = r11.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821aaa3c
	if (cr6.getEQ()) goto loc_821AAA3C;
	// addi r6,r31,10368
	ctx.r6.s64 = r31.s64 + 10368;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// rldicr r4,r11,6,15
	ctx.r4.u64 = __builtin_rotateleft64(r11.u64, 6) & 0xFFFF000000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a0d40
	sub_821A0D40(ctx, base);
	// lis r12,-1
	r12.s64 = -65536;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// rldicr r12,r12,42,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 42) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_821AAA3C:
	// lis r12,-32
	r12.s64 = -2097152;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// clrldi r12,r12,22
	r12.u64 = r12.u64 & 0x3FFFFFFFFFF;
	// and r10,r11,r12
	ctx.r10.u64 = r11.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821aaa80
	if (cr6.getEQ()) goto loc_821AAA80;
	// addi r6,r31,10444
	ctx.r6.s64 = r31.s64 + 10444;
	// li r5,8448
	ctx.r5.s64 = 8448;
	// rldicr r4,r11,22,20
	ctx.r4.u64 = __builtin_rotateleft64(r11.u64, 22) & 0xFFFFF80000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a0d40
	sub_821A0D40(ctx, base);
	// lis r12,-32
	r12.s64 = -2097152;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// rldicr r12,r12,21,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 21) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_821AAA80:
	// ld r11,24(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x821aaad0
	if (cr6.getEQ()) goto loc_821AAAD0;
	// lis r12,31
	r12.s64 = 2031616;
	// ori r12,r12,65535
	r12.u64 = r12.u64 | 65535;
	// rldicr r12,r12,34,29
	r12.u64 = __builtin_rotateleft64(r12.u64, 34) & 0xFFFFFFFC00000000;
	// and r10,r11,r12
	ctx.r10.u64 = r11.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821aaad0
	if (cr6.getEQ()) goto loc_821AAAD0;
	// addi r6,r31,10596
	ctx.r6.s64 = r31.s64 + 10596;
	// li r5,8832
	ctx.r5.s64 = 8832;
	// rldicr r4,r11,9,20
	ctx.r4.u64 = __builtin_rotateleft64(r11.u64, 9) & 0xFFFFF80000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a0d40
	sub_821A0D40(ctx, base);
	// lis r12,-32
	r12.s64 = -2097152;
	// ld r11,24(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// rldicr r12,r12,34,63
	r12.u64 = __builtin_rotateleft64(r12.u64, 34) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
loc_821AAAD0:
	// ld r11,32(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 32);
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x821aab08
	if (cr6.getEQ()) goto loc_821AAB08;
	// clrldi r10,r11,26
	ctx.r10.u64 = r11.u64 & 0x3FFFFFFFFF;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x821aab08
	if (cr6.getEQ()) goto loc_821AAB08;
	// addi r6,r31,10680
	ctx.r6.s64 = r31.s64 + 10680;
	// li r5,8960
	ctx.r5.s64 = 8960;
	// rldicr r4,r11,26,37
	ctx.r4.u64 = __builtin_rotateleft64(r11.u64, 26) & 0xFFFFFFFFFC000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821a0d40
	sub_821A0D40(ctx, base);
	// ld r11,32(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 32);
	// rldicr r11,r11,0,25
	r11.u64 = __builtin_rotateleft64(r11.u64, 0) & 0xFFFFFFC000000000;
	// std r11,32(r31)
	PPC_STORE_U64(r31.u32 + 32, r11.u64);
loc_821AAB08:
	// lwz r11,604(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 604);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// srawi. r11,r11,29
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 29;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r25,r10,-13416
	r25.s64 = ctx.r10.s64 + -13416;
	// bne 0x821aab30
	if (!cr0.getEQ()) goto loc_821AAB30;
	// lis r26,1792
	r26.s64 = 117440512;
	// li r28,525
	r28.s64 = 525;
	// mr r27,r25
	r27.u64 = r25.u64;
	// ori r26,r26,21
	r26.u64 = r26.u64 | 21;
	// b 0x821aab74
	goto loc_821AAB74;
loc_821AAB30:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821aab4c
	if (!cr6.getEQ()) goto loc_821AAB4C;
	// lis r26,1792
	r26.s64 = 117440512;
	// addi r27,r25,2160
	r27.s64 = r25.s64 + 2160;
	// li r28,933
	r28.s64 = 933;
	// ori r26,r26,19
	r26.u64 = r26.u64 | 19;
	// b 0x821aab74
	goto loc_821AAB74;
loc_821AAB4C:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x821aab68
	if (!cr6.getEQ()) goto loc_821AAB68;
	// lis r26,1792
	r26.s64 = 117440512;
	// addi r27,r25,5896
	r27.s64 = r25.s64 + 5896;
	// li r28,210
	r28.s64 = 210;
	// ori r26,r26,15
	r26.u64 = r26.u64 | 15;
	// b 0x821aab74
	goto loc_821AAB74;
loc_821AAB68:
	// lwz r28,80(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r27,80(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r26,80(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_821AAB74:
	// addi r4,r28,5
	ctx.r4.s64 = r28.s64 + 5;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// lis r11,-16384
	r11.s64 = -1073741824;
	// li r10,768
	ctx.r10.s64 = 768;
	// ori r11,r11,15104
	r11.u64 = r11.u64 | 15104;
	// lis r8,-16384
	ctx.r8.s64 = -1073741824;
	// addi r9,r28,1
	ctx.r9.s64 = r28.s64 + 1;
	// ori r8,r8,11008
	ctx.r8.u64 = ctx.r8.u64 | 11008;
	// li r22,0
	r22.s64 = 0;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// rlwimi r8,r9,16,2,15
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0x3FFF0000) | (ctx.r8.u64 & 0xFFFFFFFFC000FFFF);
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// clrlwi r9,r28,18
	ctx.r9.u64 = r28.u32 & 0x3FFF;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r28,r28,2,0,29
	r28.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// addi r3,r27,4
	ctx.r3.s64 = r27.s64 + 4;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// add r3,r28,r27
	ctx.r3.u64 = r28.u64 + r27.u64;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// ble cr6,0x821aabf0
	if (!cr6.getGT()) goto loc_821AABF0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821AABF0:
	// lis r11,-16368
	r11.s64 = -1072693248;
	// li r10,1
	ctx.r10.s64 = 1;
	// ori r11,r11,11008
	r11.u64 = r11.u64 | 11008;
	// li r9,15
	ctx.r9.s64 = 15;
	// addi r4,r25,2100
	ctx.r4.s64 = r25.s64 + 2100;
	// li r5,60
	ctx.r5.s64 = 60;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// lis r10,1
	ctx.r10.s64 = 65536;
	// addi r11,r28,60
	r11.s64 = r28.s64 + 60;
	// ori r10,r10,8576
	ctx.r10.u64 = ctx.r10.u64 | 8576;
	// oris r9,r26,4096
	ctx.r9.u64 = r26.u64 | 268435456;
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// cmplw cr6,r3,r10
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r10.u32, xer);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// ble cr6,0x821aac5c
	if (!cr6.getGT()) goto loc_821AAC5C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821AAC5C:
	// lis r11,2
	r11.s64 = 131072;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// ori r10,r11,8448
	ctx.r10.u64 = r11.u64 | 8448;
	// lis r11,0
	r11.s64 = 0;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// li r6,8851
	ctx.r6.s64 = 8851;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// li r27,768
	r27.s64 = 768;
	// ori r4,r10,8708
	ctx.r4.u64 = ctx.r10.u64 | 8708;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// li r26,8978
	r26.s64 = 8978;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// li r25,8205
	r25.s64 = 8205;
	// mr r24,r22
	r24.u64 = r22.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// li r23,8704
	r23.s64 = 8704;
	// mr r21,r22
	r21.u64 = r22.u64;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// stwu r23,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r23.u32);
	ctx.r3.u32 = ea;
	// stwu r21,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r21.u32);
	ctx.r3.u32 = ea;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// ble cr6,0x821aad00
	if (!cr6.getGT()) goto loc_821AAD00;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821AAD00:
	// lis r11,5
	r11.s64 = 327680;
	// addi r28,r30,392
	r28.s64 = r30.s64 + 392;
	// ori r11,r11,18432
	r11.u64 = r11.u64 | 18432;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r10,28(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// lhz r11,372(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 372);
	// rlwinm r10,r10,0,22,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3FC;
	// rlwinm r11,r11,17,0,9
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 17) & 0xFFC00000;
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// ori r11,r11,18434
	r11.u64 = r11.u64 | 18434;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,32(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwinm r11,r11,0,19,19
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,36(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 36);
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// rlwinm r11,r11,13,0,18
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 13) & 0xFFFFE000;
	// rlwinm r10,r10,0,19,7
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFF001FFF;
	// srawi r11,r11,13
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1FFF) != 0);
	r11.s64 = r11.s32 >> 13;
	// rlwinm r10,r10,0,7,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFF81FFFFFF;
	// rlwimi r11,r9,24,19,12
	r11.u64 = (__builtin_rotateleft32(ctx.r9.u32, 24) & 0xFFFFFFFFFFF81FFF) | (r11.u64 & 0x7E000);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,44(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 44);
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,48(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 48);
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// lwz r11,596(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 596);
	// lwz r10,584(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 584);
	// rlwinm r27,r11,2,30,31
	r27.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0x3;
	// lwz r11,604(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 604);
	// rlwinm r10,r10,2,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x4;
	// clrlwi r24,r27,31
	r24.u64 = r27.u32 & 0x1;
	// srawi. r11,r11,29
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 29;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mulli r9,r24,56
	ctx.r9.s64 = r24.s64 * 56;
	// lwzx r25,r10,r30
	r25.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// rlwinm r10,r27,31,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 31) & 0x1;
	// addi r9,r9,527
	ctx.r9.s64 = ctx.r9.s64 + 527;
	// mulli r10,r10,1536
	ctx.r10.s64 = ctx.r10.s64 * 1536;
	// rlwinm r9,r9,0,0,22
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFE00;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r29,r10,r25
	r29.u64 = ctx.r10.u64 + r25.u64;
	// bne 0x821aadf8
	if (!cr0.getEQ()) goto loc_821AADF8;
loc_821AADD0:
	// lwz r8,8(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r9,r29,0,3,29
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x1FFFFFFC;
	// rlwinm r10,r29,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 12) & 0xFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// andi. r9,r8,49400
	ctx.r9.u64 = ctx.r8.u64 & 49400;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// oris r9,r9,19200
	ctx.r9.u64 = ctx.r9.u64 | 1258291200;
	// ori r9,r9,1536
	ctx.r9.u64 = ctx.r9.u64 | 1536;
	// b 0x821aae2c
	goto loc_821AAE2C;
loc_821AADF8:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x821aadd0
	if (cr6.getEQ()) goto loc_821AADD0;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x821aae54
	if (!cr6.getEQ()) goto loc_821AAE54;
	// lwz r8,8(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// rlwinm r9,r29,0,3,29
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x1FFFFFFC;
	// rlwinm r10,r29,12,20,31
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 12) & 0xFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// andi. r9,r8,49400
	ctx.r9.u64 = ctx.r8.u64 & 49400;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// oris r9,r9,19200
	ctx.r9.u64 = ctx.r9.u64 | 1258291200;
	// ori r9,r9,2560
	ctx.r9.u64 = ctx.r9.u64 | 2560;
loc_821AAE2C:
	// lis r7,16384
	ctx.r7.s64 = 1073741824;
	// lwz r11,388(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 388);
	// li r6,75
	ctx.r6.s64 = 75;
	// stw r9,8(r28)
	PPC_STORE_U32(r28.u32 + 8, ctx.r9.u32);
	// rlwimi r7,r10,30,2,31
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r10.u32, 30) & 0x3FFFFFFF) | (ctx.r7.u64 & 0xFFFFFFFFC0000000);
	// lis r10,19200
	ctx.r10.s64 = 1258291200;
	// rlwimi r11,r6,24,0,8
	r11.u64 = (__builtin_rotateleft32(ctx.r6.u32, 24) & 0xFF800000) | (r11.u64 & 0xFFFFFFFF007FFFFF);
	// stw r7,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r7.u32);
	// stw r10,4(r28)
	PPC_STORE_U32(r28.u32 + 4, ctx.r10.u32);
	// stw r11,12(r28)
	PPC_STORE_U32(r28.u32 + 12, r11.u32);
loc_821AAE54:
	// li r4,49
	ctx.r4.s64 = 49;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d8f0
	sub_8219D8F0(ctx, base);
	// lis r11,47
	r11.s64 = 3080192;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// ori r11,r11,16384
	r11.u64 = r11.u64 | 16384;
	// li r5,192
	ctx.r5.s64 = 192;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// addi r11,r28,192
	r11.s64 = r28.s64 + 192;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// lbz r11,608(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 608);
	// rlwinm. r11,r11,0,25,25
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821aaea0
	if (cr0.getEQ()) goto loc_821AAEA0;
	// li r4,-1
	ctx.r4.s64 = -1;
	// lwz r3,364(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 364);
	// bl 0x8235eba8
	sub_8235EBA8(ctx, base);
loc_821AAEA0:
	// lhz r10,14(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// lis r11,-32018
	r11.s64 = -2098331648;
	// rotlwi r10,r10,9
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 9);
	// addi r11,r11,-31944
	r11.s64 = r11.s64 + -31944;
	// add r9,r10,r29
	ctx.r9.u64 = ctx.r10.u64 + r29.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AAEB8:
	// mfmsr r7
	// mtmsrd r13,1
	// lwarx r8,0,r10
	reserved.u32 = *(uint32_t*)(base + ctx.r10.u32);
	ctx.r8.u64 = __builtin_bswap32(reserved.u32);
	// stwcx. r9,0,r10
	cr0.getLT() = 0;
	cr0.getGT() = 0;
	cr0.getEQ() = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r10.u32), reserved.s32, __builtin_bswap32(ctx.r9.s32));
	cr0.getSO() = xer.so;
	// mtmsrd r7,1
	// bne 0x821aaeb8
	if (!cr0.getEQ()) goto loc_821AAEB8;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
	// lwz r8,8(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// li r7,6144
	ctx.r7.s64 = 6144;
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// divwu r11,r11,r7
	r11.u32 = r11.u32 / ctx.r7.u32;
	// cmplwi cr6,r11,14
	cr6.compare<uint32_t>(r11.u32, 14, xer);
	// mr r23,r11
	r23.u64 = r11.u64;
	// blt cr6,0x821aaef4
	if (cr6.getLT()) goto loc_821AAEF4;
	// li r23,14
	r23.s64 = 14;
loc_821AAEF4:
	// lwz r11,584(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 584);
	// lis r10,-25768
	ctx.r10.s64 = -1688731648;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// clrlwi r8,r29,3
	ctx.r8.u64 = r29.u32 & 0x1FFFFFFF;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r7,596(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 596);
	// ori r6,r10,59162
	ctx.r6.u64 = ctx.r10.u64 | 59162;
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// rlwinm r5,r11,2,29,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0x4;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// rlwinm r11,r29,12,20,31
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 12) & 0xFFF;
	// lwz r4,56(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// rlwinm r7,r7,4,30,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0x3;
	// addi r9,r11,512
	ctx.r9.s64 = r11.s64 + 512;
	// cmplw cr6,r3,r4
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r4.u32, xer);
	// lwzx r11,r5,r30
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + r30.u32);
	// rlwinm r9,r9,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1000;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stwu r6,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	r11.u32 = ea;
	// stw r9,10392(r31)
	PPC_STORE_U32(r31.u32 + 10392, ctx.r9.u32);
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r23,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r23.u32);
	r11.u32 = ea;
	// ble cr6,0x821aaf60
	if (!cr6.getGT()) goto loc_821AAF60;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821AAF60:
	// li r11,8198
	r11.s64 = 8198;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// mr r28,r22
	r28.u64 = r22.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// lwz r11,13488(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 13488);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// stw r11,13488(r31)
	PPC_STORE_U32(r31.u32 + 13488, r11.u32);
	// lwz r29,376(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 376);
loc_821AAF88:
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x821aafa4
	if (!cr6.getGT()) goto loc_821AAFA4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821AAFA4:
	// li r9,8450
	ctx.r9.s64 = 8450;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// cmplwi cr6,r29,65535
	cr6.compare<uint32_t>(r29.u32, 65535, xer);
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r28,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r28.u32);
	r11.u32 = ea;
	// ble cr6,0x821aafc4
	if (!cr6.getGT()) goto loc_821AAFC4;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r10,r10,65534
	ctx.r10.u64 = ctx.r10.u64 | 65534;
loc_821AAFC4:
	// lis r9,-16384
	ctx.r9.s64 = -1073741824;
	// li r8,129
	ctx.r8.s64 = 129;
	// ori r9,r9,13825
	ctx.r9.u64 = ctx.r9.u64 | 13825;
	// rlwimi r8,r10,16,0,15
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r8.u64 & 0xFFFFFFFF0000FFFF);
	// subf. r29,r10,r29
	r29.s64 = r29.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// beq 0x821aaff0
	if (cr0.getEQ()) goto loc_821AAFF0;
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// b 0x821aaf88
	goto loc_821AAF88;
loc_821AAFF0:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// stw r22,10392(r31)
	PPC_STORE_U32(r31.u32 + 10392, r22.u32);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x821ab00c
	if (!cr6.getGT()) goto loc_821AB00C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
loc_821AB00C:
	// li r11,8198
	r11.s64 = 8198;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// lwz r10,13488(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 13488);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r9,56(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// stw r10,13488(r31)
	PPC_STORE_U32(r31.u32 + 13488, ctx.r10.u32);
	// ble cr6,0x821ab048
	if (!cr6.getGT()) goto loc_821AB048;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821AB048:
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// li r9,6
	ctx.r9.s64 = 6;
	// ori r10,r10,17920
	ctx.r10.u64 = ctx.r10.u64 | 17920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r11,r11,8
	r11.u64 = r11.u64 | 8;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// oris r11,r11,8
	r11.u64 = r11.u64 | 524288;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// oris r11,r11,16
	r11.u64 = r11.u64 | 1048576;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// ld r11,24(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// oris r11,r11,65024
	r11.u64 = r11.u64 | 4261412864;
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r11,r11,2048
	r11.u64 = r11.u64 | 2048;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// lwz r26,588(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + 588);
	// bl 0x8219d7d8
	sub_8219D7D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,588(r30)
	PPC_STORE_U32(r30.u32 + 588, r11.u32);
	// bl 0x8218e450
	sub_8218E450(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8218e450
	sub_8218E450(ctx, base);
	// lwz r11,596(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 596);
	// clrlwi r28,r27,30
	r28.u64 = r27.u32 & 0x3;
	// rlwinm r11,r11,0,4,1
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFCFFFFFFF;
	// rlwinm r10,r28,28,0,3
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 28) & 0xF0000000;
	// addi r29,r25,16
	r29.s64 = r25.s64 + 16;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// stw r11,596(r30)
	PPC_STORE_U32(r30.u32 + 596, r11.u32);
	// beq cr6,0x821ab110
	if (cr6.getEQ()) goto loc_821AB110;
	// li r5,56
	ctx.r5.s64 = 56;
	// addi r4,r31,13708
	ctx.r4.s64 = r31.s64 + 13708;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// addi r29,r29,56
	r29.s64 = r29.s64 + 56;
loc_821AB110:
	// subf r11,r25,r29
	r11.s64 = r29.s64 - r25.s64;
	// rlwinm. r10,r27,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,511
	r11.s64 = r11.s64 + 511;
	// rlwinm r11,r11,0,0,22
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFE00;
	// add r29,r11,r25
	r29.u64 = r11.u64 + r25.u64;
	// beq 0x821ab160
	if (cr0.getEQ()) goto loc_821AB160;
	// lbz r11,600(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 600);
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,592(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 592);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// rlwinm. r11,r11,0,0,24
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821ab148
	if (cr0.getEQ()) goto loc_821AB148;
	// bl 0x8219e228
	sub_8219E228(ctx, base);
	// b 0x821ab14c
	goto loc_821AB14C;
loc_821AB148:
	// bl 0x8219e190
	sub_8219E190(ctx, base);
loc_821AB14C:
	// li r5,1536
	ctx.r5.s64 = 1536;
	// lwz r4,592(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 592);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// addi r29,r29,1536
	r29.s64 = r29.s64 + 1536;
loc_821AB160:
	// lwz r8,584(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 584);
	// rlwinm r7,r28,26,0,5
	ctx.r7.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 26) & 0xFC000000;
	// lhz r9,12(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// mulli r11,r23,12
	r11.s64 = r23.s64 * 12;
	// lwz r6,596(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 596);
	// lbz r5,608(r30)
	ctx.r5.u64 = PPC_LOAD_U8(r30.u32 + 608);
	// lhz r10,14(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// rlwinm r8,r8,2,29,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0x4;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// rlwinm r31,r11,9,0,22
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 9) & 0xFFFFFE00;
	// clrlwi r9,r7,2
	ctx.r9.u64 = ctx.r7.u32 & 0x3FFFFFFF;
	// lwzx r8,r8,r30
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r30.u32);
	// subf r11,r8,r29
	r11.s64 = r29.s64 - ctx.r8.s64;
	// rlwinm r11,r11,23,9,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 23) & 0x7FFFFF;
	// stw r9,596(r30)
	PPC_STORE_U32(r30.u32 + 596, ctx.r9.u32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// sth r11,12(r30)
	PPC_STORE_U16(r30.u32 + 12, r11.u16);
	// rlwinm. r7,r5,0,0,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x821ab210
	if (cr0.getEQ()) goto loc_821AB210;
	// lwz r11,380(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 380);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r22,360(r30)
	PPC_STORE_U32(r30.u32 + 360, r22.u32);
	// stw r11,356(r30)
	PPC_STORE_U32(r30.u32 + 356, r11.u32);
	// bl 0x8219d298
	sub_8219D298(ctx, base);
	// lwz r11,596(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 596);
	// addi r7,r30,348
	ctx.r7.s64 = r30.s64 + 348;
	// lwz r10,584(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 584);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// rlwinm r11,r11,12,26,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 12) & 0x3F;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// rlwinm r10,r10,2,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x4;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwzx r4,r10,r30
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// lwzx r3,r11,r30
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// bl 0x8235e018
	sub_8235E018(ctx, base);
	// lbz r11,608(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 608);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ori r11,r11,64
	r11.u64 = r11.u64 | 64;
	// stb r11,608(r30)
	PPC_STORE_U8(r30.u32 + 608, r11.u8);
	// bl 0x821aa460
	sub_821AA460(ctx, base);
loc_821AB210:
	// lbz r10,608(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 608);
	// lwz r11,584(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 584);
	// ori r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 128;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r10,608(r30)
	PPC_STORE_U8(r30.u32 + 608, ctx.r10.u8);
	// stw r11,584(r30)
	PPC_STORE_U32(r30.u32 + 584, r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x823ed16c
	return;
}

__attribute__((alias("__imp__sub_821AB230"))) PPC_WEAK_FUNC(sub_821AB230);
PPC_FUNC_IMPL(__imp__sub_821AB230) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x821aa518
	sub_821AA518(ctx, base);
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// li r11,0
	r11.s64 = 0;
	// stb r11,-9521(r10)
	PPC_STORE_U8(ctx.r10.u32 + -9521, r11.u8);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AB260"))) PPC_WEAK_FUNC(sub_821AB260);
PPC_FUNC_IMPL(__imp__sub_821AB260) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed128
	// stwu r1,-816(r1)
	ea = -816 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r29,r24,21660
	r29.s64 = r24.s64 + 21660;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// lbz r11,608(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 608);
	// rlwinm. r11,r11,0,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821ab298
	if (cr0.getEQ()) goto loc_821AB298;
loc_821AB288:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x821aa518
	sub_821AA518(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x821ab5ec
	goto loc_821AB5EC;
loc_821AB298:
	// li r5,260
	ctx.r5.s64 = 260;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x823f0600
	sub_823F0600(ctx, base);
	// li r25,0
	r25.s64 = 0;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// stb r25,371(r1)
	PPC_STORE_U8(ctx.r1.u32 + 371, r25.u8);
loc_821AB2B4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821ab2b4
	if (!cr6.getEQ()) goto loc_821AB2B4;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r31,r11,-1
	r31.s64 = r11.s64 + -1;
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// blt cr6,0x821ab304
	if (cr6.getLT()) goto loc_821AB304;
loc_821AB2E8:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,92
	cr6.compare<uint32_t>(r11.u32, 92, xer);
	// beq cr6,0x821ab304
	if (cr6.getEQ()) goto loc_821AB304;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bge cr6,0x821ab2e8
	if (!cr6.getLT()) goto loc_821AB2E8;
loc_821AB304:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,92
	cr6.compare<uint32_t>(r11.u32, 92, xer);
	// beq cr6,0x821ab320
	if (cr6.getEQ()) goto loc_821AB320;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,-6288
	ctx.r3.s64 = r11.s64 + -6288;
	// bl 0x821a2690
	sub_821A2690(ctx, base);
	// b 0x821ab288
	goto loc_821AB288;
loc_821AB320:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// stb r25,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r25.u8);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x8240fbdc
	__imp__RtlInitAnsiString(ctx, base);
	// lis r11,-32018
	r11.s64 = -2098331648;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r3,r11,-31920
	ctx.r3.s64 = r11.s64 + -31920;
	// bl 0x8240fbcc
	__imp__ObCreateSymbolicLink(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x821ab35c
	if (!cr0.getLT()) goto loc_821AB35C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,-6324
	ctx.r3.s64 = r11.s64 + -6324;
loc_821AB350:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// bl 0x821a2690
	sub_821A2690(ctx, base);
	// b 0x821ab288
	goto loc_821AB288;
loc_821AB35C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r6,r31,1
	ctx.r6.s64 = r31.s64 + 1;
	// addi r5,r11,-6332
	ctx.r5.s64 = r11.s64 + -6332;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r1,480
	ctx.r3.s64 = ctx.r1.s64 + 480;
	// addi r4,r11,-20196
	ctx.r4.s64 = r11.s64 + -20196;
	// bl 0x8240f93c
	__imp__sprintf(ctx, base);
	// addi r5,r1,480
	ctx.r5.s64 = ctx.r1.s64 + 480;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x821a9720
	sub_821A9720(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821ab288
	if (cr0.getLT()) goto loc_821AB288;
	// li r5,260
	ctx.r5.s64 = 260;
	// addi r4,r1,480
	ctx.r4.s64 = ctx.r1.s64 + 480;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x823f0600
	sub_823F0600(ctx, base);
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// stb r25,371(r1)
	PPC_STORE_U8(ctx.r1.u32 + 371, r25.u8);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AB3AC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821ab3ac
	if (!cr6.getEQ()) goto loc_821AB3AC;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x821ab40c
	if (cr6.getLT()) goto loc_821AB40C;
loc_821AB3E4:
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// cmpwi cr6,r10,46
	cr6.compare<int32_t>(ctx.r10.s32, 46, xer);
	// beq cr6,0x821ab40c
	if (cr6.getEQ()) goto loc_821AB40C;
	// cmpwi cr6,r10,92
	cr6.compare<int32_t>(ctx.r10.s32, 92, xer);
	// beq cr6,0x821ab40c
	if (cr6.getEQ()) goto loc_821AB40C;
	// addi r30,r30,-1
	r30.s64 = r30.s64 + -1;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bge cr6,0x821ab3e4
	if (!cr6.getLT()) goto loc_821AB3E4;
loc_821AB40C:
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// cmplwi cr6,r10,46
	cr6.compare<uint32_t>(ctx.r10.u32, 46, xer);
	// beq cr6,0x821ab41c
	if (cr6.getEQ()) goto loc_821AB41C;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_821AB41C:
	// lwz r11,596(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 596);
	// mr r31,r25
	r31.u64 = r25.u64;
	// rlwinm. r11,r11,0,12,17
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFC000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r27,r11,-6340
	r27.s64 = r11.s64 + -6340;
	// beq 0x821ab504
	if (cr0.getEQ()) goto loc_821AB504;
	// addi r28,r29,20
	r28.s64 = r29.s64 + 20;
	// lis r26,-31991
	r26.s64 = -2096562176;
loc_821AB43C:
	// lbz r11,-9524(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + -9524);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821ab288
	if (!cr0.getEQ()) goto loc_821AB288;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,480
	ctx.r10.s64 = ctx.r1.s64 + 480;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// add r6,r11,r10
	ctx.r6.u64 = r11.u64 + ctx.r10.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8240f93c
	__imp__sprintf(ctx, base);
	// li r9,0
	ctx.r9.s64 = 0;
	// lis r8,26624
	ctx.r8.s64 = 1744830464;
	// li r7,2
	ctx.r7.s64 = 2;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// lis r4,16384
	ctx.r4.s64 = 1073741824;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8235d008
	sub_8235D008(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// beq cr6,0x821ab5f4
	if (cr6.getEQ()) goto loc_821AB5F4;
	// lwz r11,596(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 596);
	// rlwinm r11,r11,18,26,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x3F;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x821ab4c8
	if (!cr6.getEQ()) goto loc_821AB4C8;
	// lwz r11,384(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 384);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821ab4bc
	if (cr0.getEQ()) goto loc_821AB4BC;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// b 0x821ab4d0
	goto loc_821AB4D0;
loc_821AB4BC:
	// li r11,1
	r11.s64 = 1;
	// rldicr r11,r11,32,63
	r11.u64 = __builtin_rotateleft64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// b 0x821ab4d0
	goto loc_821AB4D0;
loc_821AB4C8:
	// li r11,0
	r11.s64 = 0;
	// oris r11,r11,65520
	r11.u64 = r11.u64 | 4293918720;
loc_821AB4D0:
	// li r6,0
	ctx.r6.s64 = 0;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x8235ed68
	sub_8235ED68(ctx, base);
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// bl 0x8235ecc0
	sub_8235ECC0(ctx, base);
	// lwz r11,596(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 596);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// rlwinm r11,r11,18,26,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 18) & 0x3F;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x821ab43c
	if (cr6.getLT()) goto loc_821AB43C;
loc_821AB504:
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,480
	ctx.r10.s64 = ctx.r1.s64 + 480;
	// subf r11,r11,r30
	r11.s64 = r30.s64 - r11.s64;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
loc_821AB514:
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8240f93c
	__imp__sprintf(ctx, base);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// bl 0x8235eed8
	sub_8235EED8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821ab514
	if (!cr0.getEQ()) goto loc_821AB514;
	// addi r31,r29,348
	r31.s64 = r29.s64 + 348;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r25,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r25.u32);
	// stw r25,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r25.u32);
	// stw r25,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r25.u32);
	// stw r25,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r25.u32);
	// stw r25,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r25.u32);
	// bl 0x8235dec8
	sub_8235DEC8(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,364(r29)
	PPC_STORE_U32(r29.u32 + 364, ctx.r3.u32);
	// beq 0x821ab288
	if (cr0.getEQ()) goto loc_821AB288;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r25,356(r29)
	PPC_STORE_U32(r29.u32 + 356, r25.u32);
	// stw r25,360(r29)
	PPC_STORE_U32(r29.u32 + 360, r25.u32);
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// lwz r3,20(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// bl 0x8235e018
	sub_8235E018(ctx, base);
	// li r4,-1
	ctx.r4.s64 = -1;
	// lwz r3,364(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 364);
	// bl 0x8235eba8
	sub_8235EBA8(ctx, base);
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// bl 0x8240fa5c
	__imp__VdGetCurrentDisplayInformation(ctx, base);
	// lbz r11,389(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 389);
	// lbz r10,600(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 600);
	// li r9,2048
	ctx.r9.s64 = 2048;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r8,596(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 596);
	// stw r25,584(r29)
	PPC_STORE_U32(r29.u32 + 584, r25.u32);
	// li r3,1
	ctx.r3.s64 = 1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r8,r8,0,12,5
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFC0FFFFF;
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// stw r9,380(r29)
	PPC_STORE_U32(r29.u32 + 380, ctx.r9.u32);
	// rlwimi r10,r11,7,24,24
	ctx.r10.u64 = (__builtin_rotateleft32(r11.u32, 7) & 0x80) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF7F);
	// stw r8,596(r29)
	PPC_STORE_U32(r29.u32 + 596, ctx.r8.u32);
	// stb r10,600(r29)
	PPC_STORE_U8(r29.u32 + 600, ctx.r10.u8);
loc_821AB5EC:
	// addi r1,r1,816
	ctx.r1.s64 = ctx.r1.s64 + 816;
	// b 0x823ed178
	return;
loc_821AB5F4:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,-6396
	ctx.r3.s64 = r11.s64 + -6396;
	// b 0x821ab350
	goto loc_821AB350;
}

__attribute__((alias("__imp__sub_821AB600"))) PPC_WEAK_FUNC(sub_821AB600);
PPC_FUNC_IMPL(__imp__sub_821AB600) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31991
	r11.s64 = -2096562176;
	// addi r5,r11,-9784
	ctx.r5.s64 = r11.s64 + -9784;
	// lbz r11,262(r5)
	r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + 262);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821ab6e0
	if (cr0.getEQ()) goto loc_821AB6E0;
	// lbz r11,22268(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 22268);
	// rlwinm. r11,r11,0,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ab6bc
	if (!cr0.getEQ()) goto loc_821AB6BC;
	// bl 0x821ab260
	sub_821AB260(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821ab640
	if (cr0.getEQ()) goto loc_821AB640;
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x821ab648
	goto loc_821AB648;
loc_821AB640:
	// lis r6,-32768
	ctx.r6.s64 = -2147483648;
	// ori r6,r6,16389
	ctx.r6.u64 = ctx.r6.u64 | 16389;
loc_821AB648:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r5,r11,-6224
	ctx.r5.s64 = r11.s64 + -6224;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r4,r11,-6236
	ctx.r4.s64 = r11.s64 + -6236;
	// bl 0x8240f93c
	__imp__sprintf(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1772(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1772);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x821ab690
	if (cr6.getEQ()) goto loc_821AB690;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821ab6e0
	if (cr0.getEQ()) goto loc_821AB6E0;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821ab6e0
	if (cr0.getEQ()) goto loc_821AB6E0;
	// b 0x821ab6a8
	goto loc_821AB6A8;
loc_821AB690:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,1756(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 1756);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821ab6e0
	if (cr0.getEQ()) goto loc_821AB6E0;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
loc_821AB6A8:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,27
	ctx.r3.s64 = 27;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x821ab6e0
	goto loc_821AB6E0;
loc_821AB6BC:
	// lbz r11,260(r5)
	r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + 260);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821ab6d0
	if (cr0.getEQ()) goto loc_821AB6D0;
	// bl 0x821aa518
	sub_821AA518(ctx, base);
	// b 0x821ab6e0
	goto loc_821AB6E0;
loc_821AB6D0:
	// lbz r11,261(r5)
	r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + 261);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821ab6e0
	if (cr0.getEQ()) goto loc_821AB6E0;
	// bl 0x821aa938
	sub_821AA938(ctx, base);
loc_821AB6E0:
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AB6F0"))) PPC_WEAK_FUNC(sub_821AB6F0);
PPC_FUNC_IMPL(__imp__sub_821AB6F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,-1
	r11.s64 = -1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// clrldi r30,r11,32
	r30.u64 = r11.u64 & 0xFFFFFFFF;
	// addi r11,r31,1152
	r11.s64 = r31.s64 + 1152;
	// li r10,26
	ctx.r10.s64 = 26;
	// std r30,11816(r31)
	PPC_STORE_U64(r31.u32 + 11816, r30.u64);
loc_821AB714:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwinm r9,r9,0,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFC;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,24
	r11.s64 = r11.s64 + 24;
	// bne 0x821ab714
	if (!cr0.getEQ()) goto loc_821AB714;
	// addi r11,r31,1776
	r11.s64 = r31.s64 + 1776;
	// li r10,18
	ctx.r10.s64 = 18;
loc_821AB734:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// li r8,1
	ctx.r8.s64 = 1;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwimi r9,r8,0,30,31
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r8.u32, 0) & 0x3) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFFFC);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x821ab734
	if (!cr0.getEQ()) goto loc_821AB734;
	// lis r7,8192
	ctx.r7.s64 = 536870912;
	// lwz r11,10564(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 10564);
	// lis r6,15
	ctx.r6.s64 = 983040;
	// lwz r8,10568(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 10568);
	// ori r7,r7,8192
	ctx.r7.u64 = ctx.r7.u64 | 8192;
	// lis r5,15
	ctx.r5.s64 = 983040;
	// li r9,8
	ctx.r9.s64 = 8;
	// ori r6,r6,61440
	ctx.r6.u64 = ctx.r6.u64 | 61440;
	// ori r5,r5,61696
	ctx.r5.u64 = ctx.r5.u64 | 61696;
	// stw r7,10428(r31)
	PPC_STORE_U32(r31.u32 + 10428, ctx.r7.u32);
	// oris r7,r11,8
	ctx.r7.u64 = r11.u64 | 524288;
	// lis r11,255
	r11.s64 = 16711680;
	// stw r9,10604(r31)
	PPC_STORE_U32(r31.u32 + 10604, ctx.r9.u32);
	// li r10,14
	ctx.r10.s64 = 14;
	// stw r6,10708(r31)
	PPC_STORE_U32(r31.u32 + 10708, ctx.r6.u32);
	// li r9,4
	ctx.r9.s64 = 4;
	// stw r5,10712(r31)
	PPC_STORE_U32(r31.u32 + 10712, ctx.r5.u32);
	// ori r6,r11,65535
	ctx.r6.u64 = r11.u64 | 65535;
	// li r4,16
	ctx.r4.s64 = 16;
	// stw r7,10564(r31)
	PPC_STORE_U32(r31.u32 + 10564, ctx.r7.u32);
	// oris r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 | 65536;
	// li r5,2
	ctx.r5.s64 = 2;
	// stw r10,10628(r31)
	PPC_STORE_U32(r31.u32 + 10628, ctx.r10.u32);
	// li r11,-1
	r11.s64 = -1;
	// stw r9,10580(r31)
	PPC_STORE_U32(r31.u32 + 10580, ctx.r9.u32);
	// stw r9,10688(r31)
	PPC_STORE_U32(r31.u32 + 10688, ctx.r9.u32);
	// stw r10,10768(r31)
	PPC_STORE_U32(r31.u32 + 10768, ctx.r10.u32);
	// stw r4,10772(r31)
	PPC_STORE_U32(r31.u32 + 10772, ctx.r4.u32);
	// stw r8,10568(r31)
	PPC_STORE_U32(r31.u32 + 10568, ctx.r8.u32);
	// stw r6,10444(r31)
	PPC_STORE_U32(r31.u32 + 10444, ctx.r6.u32);
	// stw r5,10824(r31)
	PPC_STORE_U32(r31.u32 + 10824, ctx.r5.u32);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// std r11,8(r31)
	PPC_STORE_U64(r31.u32 + 8, r11.u64);
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
	// std r11,32(r31)
	PPC_STORE_U64(r31.u32 + 32, r11.u64);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// lwz r9,56(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// stw r10,10916(r31)
	PPC_STORE_U32(r31.u32 + 10916, ctx.r10.u32);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// ble cr6,0x821ab800
	if (!cr6.getGT()) goto loc_821AB800;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219d6d8
	sub_8219D6D8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_821AB800:
	// li r9,3329
	ctx.r9.s64 = 3329;
	// lis r8,1024
	ctx.r8.s64 = 67108864;
	// lis r10,-16382
	ctx.r10.s64 = -1073610752;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// ori r10,r10,8448
	ctx.r10.u64 = ctx.r10.u64 | 8448;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// li r9,129
	ctx.r9.s64 = 129;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// li r5,130
	ctx.r5.s64 = 130;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// li r3,3650
	ctx.r3.s64 = 3650;
	// li r29,8032
	r29.s64 = 8032;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// li r9,15
	ctx.r9.s64 = 15;
	// stwu r6,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	r11.u32 = ea;
	// lwz r10,-9512(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -9512);
	// oris r10,r10,32769
	ctx.r10.u64 = ctx.r10.u64 | 2147549184;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// lis r10,-31991
	ctx.r10.s64 = -2096562176;
	// stwu r4,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	r11.u32 = ea;
	// stwu r5,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	r11.u32 = ea;
	// stwu r30,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r30.u32);
	r11.u32 = ea;
	// lwz r10,-9508(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -9508);
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r3,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r3.u32);
	r11.u32 = ea;
	// stwu r29,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r29.u32);
	r11.u32 = ea;
	// lwz r10,22280(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 22280);
	// rlwinm. r10,r10,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r10,3205
	ctx.r10.s64 = 3205;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// bne 0x821ab88c
	if (!cr0.getEQ()) goto loc_821AB88C;
	// li r9,3
	ctx.r9.s64 = 3;
loc_821AB88C:
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// li r10,1404
	ctx.r10.s64 = 1404;
	// lis r9,2989
	ctx.r9.s64 = 195887104;
	// li r8,1403
	ctx.r8.s64 = 1403;
	// ori r9,r9,61453
	ctx.r9.u64 = ctx.r9.u64 | 61453;
	// li r7,0
	ctx.r7.s64 = 0;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821AB8C0"))) PPC_WEAK_FUNC(sub_821AB8C0);
PPC_FUNC_IMPL(__imp__sub_821AB8C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lbz r11,0(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,118
	cr6.compare<uint32_t>(r11.u32, 118, xer);
	// bne cr6,0x821ab954
	if (!cr6.getEQ()) goto loc_821AB954;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AB8E4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821ab8e4
	if (!cr6.getEQ()) goto loc_821AB8E4;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// subf r10,r3,r11
	ctx.r10.s64 = r11.s64 - ctx.r3.s64;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// ble cr6,0x821ab954
	if (!cr6.getGT()) goto loc_821AB954;
	// addi r31,r11,-3
	r31.s64 = r11.s64 + -3;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,3
	ctx.r5.s64 = 3;
	// addi r4,r11,5192
	ctx.r4.s64 = r11.s64 + 5192;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823ee630
	sub_823EE630(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821ab94c
	if (cr0.getEQ()) goto loc_821AB94C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,3
	ctx.r5.s64 = 3;
	// addi r4,r11,5188
	ctx.r4.s64 = r11.s64 + 5188;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823ee630
	sub_823EE630(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821ab954
	if (!cr0.getEQ()) goto loc_821AB954;
loc_821AB94C:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x821ab958
	goto loc_821AB958;
loc_821AB954:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821AB958:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AB970"))) PPC_WEAK_FUNC(sub_821AB970);
PPC_FUNC_IMPL(__imp__sub_821AB970) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,118
	cr6.compare<uint32_t>(r11.u32, 118, xer);
	// bne cr6,0x821aba18
	if (!cr6.getEQ()) goto loc_821ABA18;
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AB998:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821ab998
	if (!cr6.getEQ()) goto loc_821AB998;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// subf r10,r31,r11
	ctx.r10.s64 = r11.s64 - r31.s64;
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// ble cr6,0x821aba18
	if (!cr6.getGT()) goto loc_821ABA18;
	// addi r30,r11,-3
	r30.s64 = r11.s64 + -3;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,3
	ctx.r5.s64 = 3;
	// addi r4,r11,5192
	ctx.r4.s64 = r11.s64 + 5192;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823ee630
	sub_823EE630(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821ab9f0
	if (!cr0.getEQ()) goto loc_821AB9F0;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,4188
	ctx.r3.s64 = r11.s64 + 4188;
	// b 0x821aba50
	goto loc_821ABA50;
loc_821AB9F0:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,3
	ctx.r5.s64 = 3;
	// addi r4,r11,5188
	ctx.r4.s64 = r11.s64 + 5188;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823ee630
	sub_823EE630(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821aba18
	if (!cr0.getEQ()) goto loc_821ABA18;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,4176
	ctx.r3.s64 = r11.s64 + 4176;
	// b 0x821aba50
	goto loc_821ABA50;
loc_821ABA18:
	// rlwinm. r11,r29,0,11,11
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x100000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821aba4c
	if (!cr0.getEQ()) goto loc_821ABA4C;
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,118
	cr6.compare<int32_t>(r11.s32, 118, xer);
	// bne cr6,0x821aba3c
	if (!cr6.getEQ()) goto loc_821ABA3C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r31,r11,4208
	r31.s64 = r11.s64 + 4208;
	// b 0x821aba4c
	goto loc_821ABA4C;
loc_821ABA3C:
	// cmpwi cr6,r11,112
	cr6.compare<int32_t>(r11.s32, 112, xer);
	// bne cr6,0x821aba4c
	if (!cr6.getEQ()) goto loc_821ABA4C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r31,r11,11992
	r31.s64 = r11.s64 + 11992;
loc_821ABA4C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_821ABA50:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821ABA58"))) PPC_WEAK_FUNC(sub_821ABA58);
PPC_FUNC_IMPL(__imp__sub_821ABA58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x821aba74
	if (!cr6.getEQ()) goto loc_821ABA74;
	// lis r3,-30602
	ctx.r3.s64 = -2005532672;
	// ori r3,r3,2156
	ctx.r3.u64 = ctx.r3.u64 | 2156;
	// b 0x821abae8
	goto loc_821ABAE8;
loc_821ABA74:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r31,0
	r31.s64 = 0;
	// li r6,49
	ctx.r6.s64 = 49;
	// addi r30,r11,4560
	r30.s64 = r11.s64 + 4560;
loc_821ABA84:
	// add r11,r6,r31
	r11.u64 = ctx.r6.u64 + r31.u64;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// rlwinm r9,r11,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mulli r11,r9,12
	r11.s64 = ctx.r9.s64 * 12;
	// lwzx r11,r11,r30
	r11.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
loc_821ABA98:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821ababc
	if (cr0.getEQ()) goto loc_821ABABC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821aba98
	if (cr6.getEQ()) goto loc_821ABA98;
loc_821ABABC:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x821abaf4
	if (cr0.getEQ()) goto loc_821ABAF4;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bge cr6,0x821abad4
	if (!cr6.getLT()) goto loc_821ABAD4;
	// addi r31,r9,1
	r31.s64 = ctx.r9.s64 + 1;
	// b 0x821abad8
	goto loc_821ABAD8;
loc_821ABAD4:
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
loc_821ABAD8:
	// cmplw cr6,r31,r6
	cr6.compare<uint32_t>(r31.u32, ctx.r6.u32, xer);
	// blt cr6,0x821aba84
	if (cr6.getLT()) goto loc_821ABA84;
loc_821ABAE0:
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
loc_821ABAE8:
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_821ABAF4:
	// mulli r11,r9,12
	r11.s64 = ctx.r9.s64 * 12;
	// addi r10,r30,8
	ctx.r10.s64 = r30.s64 + 8;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// and r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 & ctx.r4.u64;
	// cmplw cr6,r10,r4
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r4.u32, xer);
	// bne cr6,0x821abae0
	if (!cr6.getEQ()) goto loc_821ABAE0;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x821abb30
	if (cr6.getEQ()) goto loc_821ABB30;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r10.u32);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, r11.u32);
loc_821ABB30:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x821abae8
	goto loc_821ABAE8;
}

__attribute__((alias("__imp__sub_821ABB38"))) PPC_WEAK_FUNC(sub_821ABB38);
PPC_FUNC_IMPL(__imp__sub_821ABB38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r9,49
	ctx.r9.s64 = 49;
	// addi r10,r11,4560
	ctx.r10.s64 = r11.s64 + 4560;
	// addi r11,r10,596
	r11.s64 = ctx.r10.s64 + 596;
loc_821ABB48:
	// addi r11,r11,-12
	r11.s64 = r11.s64 + -12;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r8,-4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// cmplw cr6,r8,r3
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r3.u32, xer);
	// bne cr6,0x821abb6c
	if (!cr6.getEQ()) goto loc_821ABB6C;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// and r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 & ctx.r4.u64;
	// cmplw cr6,r8,r4
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r4.u32, xer);
	// beq cr6,0x821abb84
	if (cr6.getEQ()) goto loc_821ABB84;
loc_821ABB6C:
	// addi r8,r10,8
	ctx.r8.s64 = ctx.r10.s64 + 8;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bgt cr6,0x821abb48
	if (cr6.getGT()) goto loc_821ABB48;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// blr 
	return;
loc_821ABB84:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x821abbac
	if (cr6.getEQ()) goto loc_821ABBAC;
	// mulli r11,r9,12
	r11.s64 = ctx.r9.s64 * 12;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r10.u32);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r11,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, r11.u32);
loc_821ABBAC:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821ABBB8"))) PPC_WEAK_FUNC(sub_821ABBB8);
PPC_FUNC_IMPL(__imp__sub_821ABBB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r28,0
	r28.s64 = 0;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r28
	r29.u64 = r28.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x821abbe4
	if (!cr6.getEQ()) goto loc_821ABBE4;
	// lis r29,-32761
	r29.s64 = -2147024896;
	// ori r29,r29,87
	r29.u64 = r29.u64 | 87;
	// b 0x821abc18
	goto loc_821ABC18;
loc_821ABBE4:
	// addi r31,r4,-1
	r31.s64 = ctx.r4.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x823eef48
	sub_823EEF48(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821abc0c
	if (cr0.getLT()) goto loc_821ABC0C;
	// cmplw cr6,r3,r31
	cr6.compare<uint32_t>(ctx.r3.u32, r31.u32, xer);
	// bgt cr6,0x821abc0c
	if (cr6.getGT()) goto loc_821ABC0C;
	// bne cr6,0x821abc18
	if (!cr6.getEQ()) goto loc_821ABC18;
	// b 0x821abc14
	goto loc_821ABC14;
loc_821ABC0C:
	// lis r29,-32761
	r29.s64 = -2147024896;
	// ori r29,r29,122
	r29.u64 = r29.u64 | 122;
loc_821ABC14:
	// stbx r28,r31,r30
	PPC_STORE_U8(r31.u32 + r30.u32, r28.u8);
loc_821ABC18:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821ABC28"))) PPC_WEAK_FUNC(sub_821ABC28);
PPC_FUNC_IMPL(__imp__sub_821ABC28) {
	PPC_FUNC_PROLOGUE();
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// b 0x8209d000
	sub_8209D000(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_821ABC30"))) PPC_WEAK_FUNC(sub_821ABC30);
PPC_FUNC_IMPL(__imp__sub_821ABC30) {
	PPC_FUNC_PROLOGUE();
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// b 0x8209d060
	sub_8209D060(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_821ABC38"))) PPC_WEAK_FUNC(sub_821ABC38);
PPC_FUNC_IMPL(__imp__sub_821ABC38) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// addi r4,r10,2334
	ctx.r4.s64 = ctx.r10.s64 + 2334;
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// lwz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// b 0x821b0420
	sub_821B0420(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_821ABC60"))) PPC_WEAK_FUNC(sub_821ABC60);
PPC_FUNC_IMPL(__imp__sub_821ABC60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r11,5148
	r11.s64 = r11.s64 + 5148;
	// clrlwi. r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// beq 0x821abc8c
	if (cr0.getEQ()) goto loc_821ABC8C;
	// bl 0x8209d150
	sub_8209D150(ctx, base);
loc_821ABC8C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821ABCA8"))) PPC_WEAK_FUNC(sub_821ABCA8);
PPC_FUNC_IMPL(__imp__sub_821ABCA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r31,0
	r31.s64 = 0;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821abccc
	if (cr6.getEQ()) goto loc_821ABCCC;
	// stw r31,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r31.u32);
loc_821ABCCC:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x821abce0
	if (!cr6.getEQ()) goto loc_821ABCE0;
loc_821ABCD4:
	// lis r3,-30602
	ctx.r3.s64 = -2005532672;
	// ori r3,r3,2156
	ctx.r3.u64 = ctx.r3.u64 | 2156;
	// b 0x821abd88
	goto loc_821ABD88;
loc_821ABCE0:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821abcd4
	if (cr6.getEQ()) goto loc_821ABCD4;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lis r10,4138
	ctx.r10.s64 = 271187968;
	// ori r10,r10,4352
	ctx.r10.u64 = ctx.r10.u64 | 4352;
	// rlwinm r9,r11,0,0,23
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF00;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x821abd1c
	if (cr6.getEQ()) goto loc_821ABD1C;
	// rlwinm r11,r11,0,0,15
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF0000;
	// lis r10,-2
	ctx.r10.s64 = -131072;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x821abd1c
	if (cr6.getEQ()) goto loc_821ABD1C;
	// lis r10,-1
	ctx.r10.s64 = -65536;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x821abd80
	if (!cr6.getEQ()) goto loc_821ABD80;
loc_821ABD1C:
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,32
	ctx.r3.s64 = 32;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821abd38
	if (cr0.getEQ()) goto loc_821ABD38;
	// bl 0x821b6b08
	sub_821B6B08(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
loc_821ABD38:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x821abd4c
	if (!cr6.getEQ()) goto loc_821ABD4C;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x821abd88
	goto loc_821ABD88;
loc_821ABD4C:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821b7030
	sub_821B7030(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bge 0x821abd80
	if (!cr0.getLT()) goto loc_821ABD80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821b5cf0
	sub_821B5CF0(ctx, base);
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// b 0x821abd88
	goto loc_821ABD88;
loc_821ABD80:
	// stw r31,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r31.u32);
	// li r3,0
	ctx.r3.s64 = 0;
loc_821ABD88:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821ABD90"))) PPC_WEAK_FUNC(sub_821ABD90);
PPC_FUNC_IMPL(__imp__sub_821ABD90) {
	PPC_FUNC_PROLOGUE();
	// b 0x821abca8
	sub_821ABCA8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_821ABD98"))) PPC_WEAK_FUNC(sub_821ABD98);
PPC_FUNC_IMPL(__imp__sub_821ABD98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,32767
	r11.s64 = 2147418112;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// ble cr6,0x821abdd4
	if (!cr6.getGT()) goto loc_821ABDD4;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// b 0x821abde8
	goto loc_821ABDE8;
loc_821ABDD4:
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,136
	ctx.r10.s64 = ctx.r1.s64 + 136;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x821abbb8
	sub_821ABBB8(ctx, base);
loc_821ABDE8:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821ABDF8"))) PPC_WEAK_FUNC(sub_821ABDF8);
PPC_FUNC_IMPL(__imp__sub_821ABDF8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed114
	// stwu r1,-1152(r1)
	ea = -1152 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r20,0
	r20.s64 = 0;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// stw r20,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r20.u32);
	// mr r22,r7
	r22.u64 = ctx.r7.u64;
	// stw r20,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r20.u32);
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// mr r21,r9
	r21.u64 = ctx.r9.u64;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
	// bl 0x821ae2e8
	sub_821AE2E8(ctx, base);
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x821abe44
	if (cr6.getEQ()) goto loc_821ABE44;
	// stw r20,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r20.u32);
loc_821ABE44:
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x821abe50
	if (cr6.getEQ()) goto loc_821ABE50;
	// stw r20,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r20.u32);
loc_821ABE50:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x821abe70
	if (cr6.getEQ()) goto loc_821ABE70;
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821abe70
	if (cr6.getEQ()) goto loc_821ABE70;
	// lis r30,-30602
	r30.s64 = -2005532672;
	// ori r30,r30,2156
	r30.u64 = r30.u64 | 2156;
	// b 0x821ac440
	goto loc_821AC440;
loc_821ABE70:
	// lis r12,-863
	r12.s64 = -56557568;
	// ori r12,r12,57792
	r12.u64 = r12.u64 | 57792;
	// and. r11,r29,r12
	r11.u64 = r29.u64 & r12.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821abe8c
	if (cr0.getEQ()) goto loc_821ABE8C;
loc_821ABE80:
	// lis r30,-30602
	r30.s64 = -2005532672;
	// ori r30,r30,2156
	r30.u64 = r30.u64 | 2156;
	// b 0x821ac424
	goto loc_821AC424;
loc_821ABE8C:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821abe80
	if (cr6.getEQ()) goto loc_821ABE80;
	// mr r31,r20
	r31.u64 = r20.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x821ac2d4
	if (cr6.getEQ()) goto loc_821AC2D4;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// rlwinm. r11,r11,0,30,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821ac2d4
	if (cr0.getEQ()) goto loc_821AC2D4;
	// bl 0x822567e0
	sub_822567E0(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// stw r31,20(r27)
	PPC_STORE_U32(r27.u32 + 20, r31.u32);
	// bne 0x821abecc
	if (!cr0.getEQ()) goto loc_821ABECC;
	// lis r30,-32761
	r30.s64 = -2147024896;
	// ori r30,r30,14
	r30.u64 = r30.u64 | 14;
	// b 0x821ac424
	goto loc_821AC424;
loc_821ABECC:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r11,5496
	ctx.r4.s64 = r11.s64 + 5496;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822549a0
	sub_822549A0(ctx, base);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821abf34
	if (cr0.getEQ()) goto loc_821ABF34;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// b 0x821abf28
	goto loc_821ABF28;
loc_821ABF18:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// bl 0x82257140
	sub_82257140(ctx, base);
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
loc_821ABF28:
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne 0x821abf18
	if (!cr0.getEQ()) goto loc_821ABF18;
loc_821ABF34:
	// clrlwi. r11,r29,31
	r11.u64 = r29.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r28,r11,5368
	r28.s64 = r11.s64 + 5368;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// addi r26,r11,5360
	r26.s64 = r11.s64 + 5360;
	// bne 0x821abf54
	if (!cr0.getEQ()) goto loc_821ABF54;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821ABF54:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5356
	ctx.r4.s64 = r11.s64 + 5356;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,30,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821abf74
	if (!cr0.getEQ()) goto loc_821ABF74;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821ABF74:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5352
	ctx.r4.s64 = r11.s64 + 5352;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,11,11
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x100000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821abf94
	if (!cr0.getEQ()) goto loc_821ABF94;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821ABF94:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5336
	ctx.r4.s64 = r11.s64 + 5336;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,14,14
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x20000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821abfb4
	if (!cr0.getEQ()) goto loc_821ABFB4;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821ABFB4:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5320
	ctx.r4.s64 = r11.s64 + 5320;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r11,0,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821abfd8
	if (!cr0.getEQ()) goto loc_821ABFD8;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821ABFD8:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5312
	ctx.r4.s64 = r11.s64 + 5312;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r11,0,29,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821abffc
	if (!cr0.getEQ()) goto loc_821ABFFC;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821ABFFC:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5304
	ctx.r4.s64 = r11.s64 + 5304;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r4,16
	ctx.r4.s64 = 16;
	// lwz r6,16(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// addi r30,r11,5300
	r30.s64 = r11.s64 + 5300;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x821abd98
	sub_821ABD98(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,5284
	ctx.r4.s64 = r11.s64 + 5284;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// li r4,16
	ctx.r4.s64 = 16;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r6,32(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821abd98
	sub_821ABD98(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,5272
	ctx.r4.s64 = r11.s64 + 5272;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// li r4,16
	ctx.r4.s64 = 16;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r6,36(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821abd98
	sub_821ABD98(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,5260
	ctx.r4.s64 = r11.s64 + 5260;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// li r4,16
	ctx.r4.s64 = 16;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r6,40(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 40);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821abd98
	sub_821ABD98(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,5248
	ctx.r4.s64 = r11.s64 + 5248;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// li r4,16
	ctx.r4.s64 = 16;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r6,44(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 44);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821abd98
	sub_821ABD98(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,5236
	ctx.r4.s64 = r11.s64 + 5236;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// li r4,16
	ctx.r4.s64 = 16;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r6,52(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 52);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821abd98
	sub_821ABD98(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,5224
	ctx.r4.s64 = r11.s64 + 5224;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// li r4,16
	ctx.r4.s64 = 16;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r6,48(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 48);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821abd98
	sub_821ABD98(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,5212
	ctx.r4.s64 = r11.s64 + 5212;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,29,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac13c
	if (!cr0.getEQ()) goto loc_821AC13C;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC13C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5492
	ctx.r4.s64 = r11.s64 + 5492;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,28,28
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac15c
	if (!cr0.getEQ()) goto loc_821AC15C;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC15C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5484
	ctx.r4.s64 = r11.s64 + 5484;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,27,27
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac17c
	if (!cr0.getEQ()) goto loc_821AC17C;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC17C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5476
	ctx.r4.s64 = r11.s64 + 5476;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,26,26
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac19c
	if (!cr0.getEQ()) goto loc_821AC19C;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC19C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5468
	ctx.r4.s64 = r11.s64 + 5468;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,23,23
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac1bc
	if (!cr0.getEQ()) goto loc_821AC1BC;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC1BC:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5464
	ctx.r4.s64 = r11.s64 + 5464;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,22,22
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x200;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac1dc
	if (!cr0.getEQ()) goto loc_821AC1DC;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC1DC:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5456
	ctx.r4.s64 = r11.s64 + 5456;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,21,21
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x400;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac1fc
	if (!cr0.getEQ()) goto loc_821AC1FC;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC1FC:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5448
	ctx.r4.s64 = r11.s64 + 5448;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,13,13
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x40000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac21c
	if (!cr0.getEQ()) goto loc_821AC21C;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC21C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5432
	ctx.r4.s64 = r11.s64 + 5432;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,9,9
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x400000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac23c
	if (!cr0.getEQ()) goto loc_821AC23C;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC23C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5424
	ctx.r4.s64 = r11.s64 + 5424;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,6,6
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac25c
	if (!cr0.getEQ()) goto loc_821AC25C;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC25C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5416
	ctx.r4.s64 = r11.s64 + 5416;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r29,0,7,7
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 0) & 0x1000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac27c
	if (!cr0.getEQ()) goto loc_821AC27C;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC27C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5408
	ctx.r4.s64 = r11.s64 + 5408;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r11,0,28,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac2a0
	if (!cr0.getEQ()) goto loc_821AC2A0;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC2A0:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5388
	ctx.r4.s64 = r11.s64 + 5388;
	// bl 0x82256828
	sub_82256828(ctx, base);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// rlwinm. r11,r11,0,26,26
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac2c4
	if (!cr0.getEQ()) goto loc_821AC2C4;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
loc_821AC2C4:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,5376
	ctx.r4.s64 = r11.s64 + 5376;
	// bl 0x82256828
	sub_82256828(ctx, base);
loc_821AC2D4:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r3,0(r24)
	ctx.r3.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// bl 0x821ab970
	sub_821AB970(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stw r27,1028(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1028, r27.u32);
	// addi r4,r1,336
	ctx.r4.s64 = ctx.r1.s64 + 336;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821ac424
	if (cr0.getLT()) goto loc_821AC424;
	// lwz r3,0(r24)
	ctx.r3.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// bl 0x821ab8c0
	sub_821AB8C0(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// beq 0x821ac32c
	if (cr0.getEQ()) goto loc_821AC32C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r4,r11,5176
	ctx.r4.s64 = r11.s64 + 5176;
	// b 0x821ac334
	goto loc_821AC334;
loc_821AC32C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r4,r11,5160
	ctx.r4.s64 = r11.s64 + 5160;
loc_821AC334:
	// bl 0x821af4e0
	sub_821AF4E0(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x821bf678
	sub_821BF678(ctx, base);
	// addi r11,r1,100
	r11.s64 = ctx.r1.s64 + 100;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lwz r8,0(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// ori r9,r29,256
	ctx.r9.u64 = r29.u64 | 256;
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r23
	ctx.r6.u64 = r23.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r4,r1,336
	ctx.r4.s64 = ctx.r1.s64 + 336;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x821d5668
	sub_821D5668(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bge 0x821ac388
	if (!cr0.getLT()) goto loc_821AC388;
	// bl 0x821c15e8
	sub_821C15E8(ctx, base);
	// b 0x821ac424
	goto loc_821AC424;
loc_821AC388:
	// bl 0x821c15e8
	sub_821C15E8(ctx, base);
	// addi r3,r1,360
	ctx.r3.s64 = ctx.r1.s64 + 360;
	// bl 0x823dc1f0
	sub_823DC1F0(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ac3a8
	if (cr0.getEQ()) goto loc_821AC3A8;
	// lis r30,-30602
	r30.s64 = -2005532672;
	// ori r30,r30,2905
	r30.u64 = r30.u64 | 2905;
	// b 0x821ac424
	goto loc_821AC424;
loc_821AC3A8:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821ac3fc
	if (cr6.getEQ()) goto loc_821AC3FC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822572b8
	sub_822572B8(ctx, base);
	// addi r28,r27,12
	r28.s64 = r27.s64 + 12;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x821b21d8
	sub_821B21D8(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821ac3fc
	if (cr0.getLT()) goto loc_821AC3FC;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822572c0
	sub_822572C0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_821AC3FC:
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x821ac410
	if (cr6.getEQ()) goto loc_821AC410;
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r20,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r20.u32);
	// stw r11,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r11.u32);
loc_821AC410:
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x821ac424
	if (cr6.getEQ()) goto loc_821AC424;
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stw r20,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r20.u32);
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
loc_821AC424:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x821ac440
	if (cr6.getEQ()) goto loc_821AC440;
	// lwz r3,20(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 20);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ac440
	if (cr0.getEQ()) goto loc_821AC440;
	// bl 0x82253110
	sub_82253110(ctx, base);
	// stw r20,20(r27)
	PPC_STORE_U32(r27.u32 + 20, r20.u32);
loc_821AC440:
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x821ac454
	if (cr6.getEQ()) goto loc_821AC454;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// addi r3,r1,360
	ctx.r3.s64 = ctx.r1.s64 + 360;
	// bl 0x821b7480
	sub_821B7480(ctx, base);
loc_821AC454:
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821ac474
	if (cr6.getEQ()) goto loc_821AC474;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r20,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r20.u32);
loc_821AC474:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821ac494
	if (cr6.getEQ()) goto loc_821AC494;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r20,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r20.u32);
loc_821AC494:
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// bl 0x821ae3f8
	sub_821AE3F8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,1152
	ctx.r1.s64 = ctx.r1.s64 + 1152;
	// b 0x823ed164
	return;
}

__attribute__((alias("__imp__sub_821AC4A8"))) PPC_WEAK_FUNC(sub_821AC4A8);
PPC_FUNC_IMPL(__imp__sub_821AC4A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed108
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r30,-31991
	r30.s64 = -2096562176;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// lbz r11,-9499(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + -9499);
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// mr r21,r9
	r21.u64 = ctx.r9.u64;
	// mr r23,r10
	r23.u64 = ctx.r10.u64;
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// lis r29,-31991
	r29.s64 = -2096562176;
	// bne 0x821ac518
	if (!cr0.getEQ()) goto loc_821AC518;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,5736
	ctx.r3.s64 = r11.s64 + 5736;
	// bl 0x8235f050
	sub_8235F050(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x821ac510
	if (cr6.getEQ()) goto loc_821AC510;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,5648
	ctx.r3.s64 = r11.s64 + 5648;
	// li r11,1
	r11.s64 = 1;
	// stb r11,-9500(r29)
	PPC_STORE_U8(r29.u32 + -9500, r11.u8);
	// bl 0x8235e140
	sub_8235E140(ctx, base);
loc_821AC510:
	// li r11,1
	r11.s64 = 1;
	// stb r11,-9499(r30)
	PPC_STORE_U8(r30.u32 + -9499, r11.u8);
loc_821AC518:
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// mr r7,r24
	ctx.r7.u64 = r24.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x821abdf8
	sub_821ABDF8(ctx, base);
	// mr. r20,r3
	r20.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r20.s32, 0, xer);
	// blt 0x821ac968
	if (cr0.getLT()) goto loc_821AC968;
	// lbz r11,-9500(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + -9500);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821ac968
	if (cr0.getEQ()) goto loc_821AC968;
	// rlwinm. r11,r31,0,6,6
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x2000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac968
	if (!cr0.getEQ()) goto loc_821AC968;
	// rlwinm. r11,r31,0,11,11
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x100000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac968
	if (!cr0.getEQ()) goto loc_821AC968;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x821ac968
	if (cr6.getEQ()) goto loc_821AC968;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821ac968
	if (cr6.getEQ()) goto loc_821AC968;
	// li r29,0
	r29.s64 = 0;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// oris r6,r31,16
	ctx.r6.u64 = r31.u64 | 1048576;
	// oris r26,r31,512
	r26.u64 = r31.u64 | 33554432;
	// stw r29,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r29.u32);
	// stw r29,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r29.u32);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// stw r29,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r29.u32);
	// stw r29,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r29.u32);
	// cmplwi cr6,r11,118
	cr6.compare<uint32_t>(r11.u32, 118, xer);
	// bne cr6,0x821ac5ac
	if (!cr6.getEQ()) goto loc_821AC5AC;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r11,r11,4208
	r11.s64 = r11.s64 + 4208;
	// b 0x821ac5b4
	goto loc_821AC5B4;
loc_821AC5AC:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r11,r11,11992
	r11.s64 = r11.s64 + 11992;
loc_821AC5B4:
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r11.u32);
	// addi r9,r1,120
	ctx.r9.s64 = ctx.r1.s64 + 120;
	// addi r8,r1,116
	ctx.r8.s64 = ctx.r1.s64 + 116;
	// addi r7,r1,124
	ctx.r7.s64 = ctx.r1.s64 + 124;
	// addi r5,r1,132
	ctx.r5.s64 = ctx.r1.s64 + 132;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x821abdf8
	sub_821ABDF8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x821ac5f4
	if (!cr0.getLT()) goto loc_821AC5F4;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
loc_821AC5E4:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821ac968
	if (cr6.getEQ()) goto loc_821AC968;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// b 0x821ac95c
	goto loc_821AC95C;
loc_821AC5F4:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lis r3,1
	ctx.r3.s64 = 65536;
	// bl 0x821b21d8
	sub_821B21D8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x821ac64c
	if (!cr0.getLT()) goto loc_821AC64C;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821ac628
	if (cr6.getEQ()) goto loc_821AC628;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r29,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r29.u32);
loc_821AC628:
	// lwz r3,124(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821ac644
	if (cr6.getEQ()) goto loc_821AC644;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AC644:
	// lwz r3,120(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// b 0x821ac5e4
	goto loc_821AC5E4;
loc_821AC64C:
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r29,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r29.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x821ac69c
	if (cr6.getEQ()) goto loc_821AC69C;
	// lwz r30,16(r23)
	r30.u64 = PPC_LOAD_U32(r23.u32 + 16);
	// b 0x821ac6a0
	goto loc_821AC6A0;
loc_821AC69C:
	// mr r30,r29
	r30.u64 = r29.u64;
loc_821AC6A0:
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lis r11,-32195
	r11.s64 = -2109931520;
	// lwz r28,120(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r19,r1,128
	r19.s64 = ctx.r1.s64 + 128;
	// addi r27,r11,8928
	r27.s64 = r11.s64 + 8928;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,16(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r18,r3
	r18.u64 = ctx.r3.u64;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r31,124(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r17,r3
	r17.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r16,r3
	r16.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// bl 0x8225b4f8
	sub_8225B4F8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x821ac7a0
	if (!cr0.getLT()) goto loc_821AC7A0;
loc_821AC748:
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821ac768
	if (cr6.getEQ()) goto loc_821AC768;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r29,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r29.u32);
loc_821AC768:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x821ac798
	if (cr6.getEQ()) goto loc_821AC798;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AC798:
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// b 0x821ac5e4
	goto loc_821AC5E4;
loc_821AC7A0:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r3,0(r24)
	ctx.r3.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r11,118
	cr6.compare<uint32_t>(r11.u32, 118, xer);
	// bne cr6,0x821ac7cc
	if (!cr6.getEQ()) goto loc_821AC7CC;
	// bl 0x8217aa40
	sub_8217AA40(ctx, base);
	// b 0x821ac7d0
	goto loc_821AC7D0;
loc_821AC7CC:
	// bl 0x8217aa40
	sub_8217AA40(ctx, base);
loc_821AC7D0:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x821ac900
	if (!cr6.getEQ()) goto loc_821AC900;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r3,r11,5512
	ctx.r3.s64 = r11.s64 + 5512;
	// bl 0x8235e140
	sub_8235E140(ctx, base);
	// addi r4,r1,124
	ctx.r4.s64 = ctx.r1.s64 + 124;
	// lwz r3,128(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// stw r29,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r29.u32);
	// bl 0x821b21d8
	sub_821B21D8(ctx, base);
	// mr. r20,r3
	r20.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r20.s32, 0, xer);
	// blt 0x821ac748
	if (cr0.getLT()) goto loc_821AC748;
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r30,128(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821ac858
	if (cr6.getEQ()) goto loc_821AC858;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AC858:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r11.u32);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,0(r24)
	ctx.r3.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// stw r11,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r11.u32);
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ac8ac
	if (cr0.getEQ()) goto loc_821AC8AC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AC8AC:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stw r11,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r11.u32);
	// lwz r3,0(r21)
	ctx.r3.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ac8d0
	if (cr0.getEQ()) goto loc_821AC8D0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AC8D0:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// stw r28,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r28.u32);
	// beq cr6,0x821ac968
	if (cr6.getEQ()) goto loc_821AC968;
	// lwz r3,12(r23)
	ctx.r3.u64 = PPC_LOAD_U32(r23.u32 + 12);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ac968
	if (cr0.getEQ()) goto loc_821AC968;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r29,12(r23)
	PPC_STORE_U32(r23.u32 + 12, r29.u32);
	// b 0x821ac968
	goto loc_821AC968;
loc_821AC900:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,112(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821ac930
	if (cr6.getEQ()) goto loc_821AC930;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AC930:
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821ac94c
	if (cr6.getEQ()) goto loc_821AC94C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AC94C:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x821ac968
	if (cr6.getEQ()) goto loc_821AC968;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
loc_821AC95C:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AC968:
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x823ed158
	return;
}

__attribute__((alias("__imp__sub_821AC978"))) PPC_WEAK_FUNC(sub_821AC978);
PPC_FUNC_IMPL(__imp__sub_821AC978) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed128
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// mr r24,r10
	r24.u64 = ctx.r10.u64;
	// bl 0x821ac4a8
	sub_821AC4A8(ctx, base);
	// lis r11,-30602
	r11.s64 = -2005532672;
	// ori r11,r11,2924
	r11.u64 = r11.u64 | 2924;
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// bne cr6,0x821ac9e4
	if (!cr6.getEQ()) goto loc_821AC9E4;
	// rlwinm. r11,r31,0,29,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821ac9e4
	if (!cr0.getEQ()) goto loc_821AC9E4;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// ori r6,r31,4
	ctx.r6.u64 = r31.u64 | 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821ac4a8
	sub_821AC4A8(ctx, base);
loc_821AC9E4:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed178
	return;
}

__attribute__((alias("__imp__sub_821AC9F0"))) PPC_WEAK_FUNC(sub_821AC9F0);
PPC_FUNC_IMPL(__imp__sub_821AC9F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r31,0
	r31.s64 = 0;
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r30,r9
	r30.u64 = ctx.r9.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// bne cr6,0x821aca44
	if (!cr6.getEQ()) goto loc_821ACA44;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// li r10,7
	ctx.r10.s64 = 7;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_821ACA34:
	// std r9,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bdnz 0x821aca34
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_821ACA34;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
loc_821ACA44:
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x821aca70
	if (cr6.getEQ()) goto loc_821ACA70;
	// lbz r11,0(r5)
	r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,118
	cr6.compare<int32_t>(r11.s32, 118, xer);
	// beq cr6,0x821aca70
	if (cr6.getEQ()) goto loc_821ACA70;
	// cmpwi cr6,r11,112
	cr6.compare<int32_t>(r11.s32, 112, xer);
	// beq cr6,0x821aca70
	if (cr6.getEQ()) goto loc_821ACA70;
	// li r9,1
	ctx.r9.s64 = 1;
	// oris r6,r6,16
	ctx.r6.u64 = ctx.r6.u64 | 1048576;
loc_821ACA70:
	// clrlwi. r11,r9,24
	r11.u64 = ctx.r9.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821acb44
	if (!cr0.getEQ()) goto loc_821ACB44;
	// rlwinm. r11,r6,0,11,11
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x100000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821acb44
	if (!cr0.getEQ()) goto loc_821ACB44;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// addi r5,r1,92
	ctx.r5.s64 = ctx.r1.s64 + 92;
	// bl 0x821ac978
	sub_821AC978(ctx, base);
	// mr. r27,r3
	r27.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r27.s32, 0, xer);
	// blt 0x821acad0
	if (cr0.getLT()) goto loc_821ACAD0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x821acab4
	if (cr6.getEQ()) goto loc_821ACAB4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r29,r31
	r29.u64 = r31.u64;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// b 0x821acab8
	goto loc_821ACAB8;
loc_821ACAB4:
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_821ACAB8:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x821acad4
	if (cr6.getEQ()) goto loc_821ACAD4;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// b 0x821acad8
	goto loc_821ACAD8;
loc_821ACAD0:
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_821ACAD4:
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_821ACAD8:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x821acaec
	if (cr6.getEQ()) goto loc_821ACAEC;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_821ACAEC:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821acb04
	if (cr6.getEQ()) goto loc_821ACB04;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821ACB04:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821acb20
	if (cr6.getEQ()) goto loc_821ACB20;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821ACB20:
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x821acb3c
	if (cr6.getEQ()) goto loc_821ACB3C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821ACB3C:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// b 0x821acb58
	goto loc_821ACB58;
loc_821ACB44:
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// addi r5,r1,92
	ctx.r5.s64 = ctx.r1.s64 + 92;
	// bl 0x821ac978
	sub_821AC978(ctx, base);
loc_821ACB58:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821ACB60"))) PPC_WEAK_FUNC(sub_821ACB60);
PPC_FUNC_IMPL(__imp__sub_821ACB60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// addi r28,r10,5196
	r28.s64 = ctx.r10.s64 + 5196;
	// mr r5,r8
	ctx.r5.u64 = ctx.r8.u64;
	// lwz r8,228(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// lwz r9,236(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// stw r29,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r29.u32);
	// bl 0x821ac9f0
	sub_821AC9F0(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821ACBC8"))) PPC_WEAK_FUNC(sub_821ACBC8);
PPC_FUNC_IMPL(__imp__sub_821ACBC8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x821b7558
	sub_821B7558(ctx, base);
	// addi r3,r31,60
	ctx.r3.s64 = r31.s64 + 60;
	// bl 0x821d5700
	sub_821D5700(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r11.u32);
	// stw r11,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r11.u32);
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// stw r11,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r11.u32);
	// stw r11,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r11.u32);
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// stw r11,92(r31)
	PPC_STORE_U32(r31.u32 + 92, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821ACC20"))) PPC_WEAK_FUNC(sub_821ACC20);
PPC_FUNC_IMPL(__imp__sub_821ACC20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r26,0
	r26.s64 = 0;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821ACC44:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821acc44
	if (!cr6.getEQ()) goto loc_821ACC44;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rotlwi r28,r11,0
	r28.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r30,r28,1
	r30.s64 = r28.s64 + 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x821d5928
	sub_821D5928(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,80(r31)
	PPC_STORE_U32(r31.u32 + 80, ctx.r3.u32);
	// beq 0x821acc9c
	if (cr0.getEQ()) goto loc_821ACC9C;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x821d5928
	sub_821D5928(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r3.u32);
	// bne 0x821acca8
	if (!cr0.getEQ()) goto loc_821ACCA8;
loc_821ACC9C:
	// lis r26,-32761
	r26.s64 = -2147024896;
	// ori r26,r26,14
	r26.u64 = r26.u64 | 14;
	// b 0x821accf4
	goto loc_821ACCF4;
loc_821ACCA8:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x823f0600
	sub_823F0600(ctx, base);
	// li r4,92
	ctx.r4.s64 = 92;
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// bl 0x823f0670
	sub_823F0670(ctx, base);
	// mr. r29,r3
	r29.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x821accd0
	if (cr0.getEQ()) goto loc_821ACCD0;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
loc_821ACCD0:
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stbx r26,r11,r28
	PPC_STORE_U8(r11.u32 + r28.u32, r26.u8);
	// lwz r4,80(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// lwz r3,76(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821accf4
	if (cr6.getEQ()) goto loc_821ACCF4;
	// stb r26,0(r29)
	PPC_STORE_U8(r29.u32 + 0, r26.u8);
loc_821ACCF4:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_821ACD00"))) PPC_WEAK_FUNC(sub_821ACD00);
PPC_FUNC_IMPL(__imp__sub_821ACD00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed120
	// stwu r1,-704(r1)
	ea = -704 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// mr r23,r7
	r23.u64 = ctx.r7.u64;
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// mr r24,r10
	r24.u64 = ctx.r10.u64;
	// stw r9,72(r31)
	PPC_STORE_U32(r31.u32 + 72, ctx.r9.u32);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x821acd5c
	if (cr6.getEQ()) goto loc_821ACD5C;
	// lis r3,0
	ctx.r3.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,260
	ctx.r8.s64 = 260;
	// addi r7,r1,352
	ctx.r7.s64 = ctx.r1.s64 + 352;
	// li r6,-1
	ctx.r6.s64 = -1;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r3,r3,65001
	ctx.r3.u64 = ctx.r3.u64 | 65001;
	// bl 0x8235ef90
	sub_8235EF90(ctx, base);
	// addi r29,r1,352
	r29.s64 = ctx.r1.s64 + 352;
loc_821ACD5C:
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821ace94
	if (cr6.getEQ()) goto loc_821ACE94;
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821ACD70:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821acd70
	if (!cr6.getEQ()) goto loc_821ACD70;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x821d5928
	sub_821D5928(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r3.u32);
	// bne 0x821acdb8
	if (!cr0.getEQ()) goto loc_821ACDB8;
loc_821ACDAC:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x821acf04
	goto loc_821ACF04;
loc_821ACDB8:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// li r26,0
	r26.s64 = 0;
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// addi r27,r31,88
	r27.s64 = r31.s64 + 88;
	// addi r28,r31,84
	r28.s64 = r31.s64 + 84;
	// lwz r5,76(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r6,788(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	// li r10,260
	ctx.r10.s64 = 260;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stb r26,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r26.u8);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bge 0x821ace2c
	if (!cr0.getLT()) goto loc_821ACE2C;
loc_821ACE0C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// addi r6,r11,8152
	ctx.r6.s64 = r11.s64 + 8152;
	// li r5,1507
	ctx.r5.s64 = 1507;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
	// b 0x821acf00
	goto loc_821ACF00;
loc_821ACE2C:
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x821acedc
	if (cr0.getEQ()) goto loc_821ACEDC;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// stb r26,339(r1)
	PPC_STORE_U8(ctx.r1.u32 + 339, r26.u8);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821ACE44:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821ace44
	if (!cr6.getEQ()) goto loc_821ACE44;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x821d5928
	sub_821D5928(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x821acdac
	if (cr0.getEQ()) goto loc_821ACDAC;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// stw r30,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r30.u32);
	// b 0x821acedc
	goto loc_821ACEDC;
loc_821ACE94:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821acc20
	sub_821ACC20(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821acf00
	if (cr0.getLT()) goto loc_821ACF00;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// addi r3,r31,60
	ctx.r3.s64 = r31.s64 + 60;
	// bl 0x821d5718
	sub_821D5718(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821ace0c
	if (cr0.getLT()) goto loc_821ACE0C;
	// lwz r11,64(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 64);
	// addi r28,r31,84
	r28.s64 = r31.s64 + 84;
	// lwz r10,68(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// addi r27,r31,88
	r27.s64 = r31.s64 + 88;
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// stw r10,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r10.u32);
loc_821ACEDC:
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// lwz r6,76(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// lwz r5,0(r27)
	ctx.r5.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821b7580
	sub_821B7580(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
loc_821ACF00:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_821ACF04:
	// addi r1,r1,704
	ctx.r1.s64 = ctx.r1.s64 + 704;
	// b 0x823ed170
	return;
}

__attribute__((alias("__imp__sub_821ACF10"))) PPC_WEAK_FUNC(sub_821ACF10);
PPC_FUNC_IMPL(__imp__sub_821ACF10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x821acf4c
	if (cr6.getEQ()) goto loc_821ACF4C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821acf4c
	if (!cr6.getEQ()) goto loc_821ACF4C;
	// lis r3,-30602
	ctx.r3.s64 = -2005532672;
	// ori r3,r3,2156
	ctx.r3.u64 = ctx.r3.u64 | 2156;
	// b 0x821acf8c
	goto loc_821ACF8C;
loc_821ACF4C:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stw r11,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r11.u32);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// stw r6,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r6.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821acc20
	sub_821ACC20(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821acf8c
	if (cr0.getLT()) goto loc_821ACF8C;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// lwz r5,88(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r4,84(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821b7580
	sub_821B7580(ctx, base);
loc_821ACF8C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821ACF98"))) PPC_WEAK_FUNC(sub_821ACF98);
PPC_FUNC_IMPL(__imp__sub_821ACF98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x821d9810
	sub_821D9810(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821acfd0
	if (cr0.getLT()) goto loc_821ACFD0;
	// li r11,1
	r11.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,684(r31)
	PPC_STORE_U32(r31.u32 + 684, r11.u32);
	// bl 0x821d59f0
	sub_821D59F0(ctx, base);
	// stw r3,688(r31)
	PPC_STORE_U32(r31.u32 + 688, ctx.r3.u32);
loc_821ACFD0:
	// stw r30,680(r31)
	PPC_STORE_U32(r31.u32 + 680, r30.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821ACFF0"))) PPC_WEAK_FUNC(sub_821ACFF0);
PPC_FUNC_IMPL(__imp__sub_821ACFF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,684(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 684);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821ad024
	if (cr6.getEQ()) goto loc_821AD024;
	// lwz r3,688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 688);
	// bl 0x821d59f0
	sub_821D59F0(ctx, base);
	// bl 0x821d9958
	sub_821D9958(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,684(r31)
	PPC_STORE_U32(r31.u32 + 684, r11.u32);
loc_821AD024:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AD038"))) PPC_WEAK_FUNC(sub_821AD038);
PPC_FUNC_IMPL(__imp__sub_821AD038) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x821ad04c
	if (cr6.getEQ()) goto loc_821AD04C;
	// lwz r11,632(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 632);
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
loc_821AD04C:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x821ad060
	if (cr6.getEQ()) goto loc_821AD060;
	// lwz r11,632(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 632);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
loc_821AD060:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AD068"))) PPC_WEAK_FUNC(sub_821AD068);
PPC_FUNC_IMPL(__imp__sub_821AD068) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,628(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 628);
	// b 0x821ad074
	goto loc_821AD074;
loc_821AD070:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_821AD074:
	// lwz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 92);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x821ad070
	if (!cr0.getEQ()) goto loc_821AD070;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x821ad090
	if (cr6.getEQ()) goto loc_821AD090;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
loc_821AD090:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x821ad0b4
	if (cr6.getEQ()) goto loc_821AD0B4;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// bge cr6,0x821ad0b0
	if (!cr6.getLT()) goto loc_821AD0B0;
	// li r11,0
	r11.s64 = 0;
loc_821AD0B0:
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
loc_821AD0B4:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AD0C0"))) PPC_WEAK_FUNC(sub_821AD0C0);
PPC_FUNC_IMPL(__imp__sub_821AD0C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,632(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 632);
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// bl 0x821b8768
	sub_821B8768(ctx, base);
	// lwz r11,92(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821ad20c
	if (cr6.getEQ()) goto loc_821AD20C;
	// lwz r5,632(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 632);
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// b 0x821ad124
	goto loc_821AD124;
loc_821AD100:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,32
	cr6.compare<int32_t>(r11.s32, 32, xer);
	// beq cr6,0x821ad118
	if (cr6.getEQ()) goto loc_821AD118;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x821ad12c
	if (!cr6.getEQ()) goto loc_821AD12C;
loc_821AD118:
	// lwz r11,632(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 632);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_821AD124:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x821ad100
	if (cr6.getLT()) goto loc_821AD100;
loc_821AD12C:
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r10,r31,2
	ctx.r10.s64 = r31.s64 + 2;
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
loc_821AD138:
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// bge cr6,0x821ad1d8
	if (!cr6.getLT()) goto loc_821AD1D8;
	// lbz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// extsb r6,r7
	ctx.r6.s64 = ctx.r7.s8;
	// cmpwi cr6,r6,92
	cr6.compare<int32_t>(ctx.r6.s32, 92, xer);
	// bne cr6,0x821ad1b0
	if (!cr6.getEQ()) goto loc_821AD1B0;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x821ad178
	if (!cr6.getLT()) goto loc_821AD178;
	// lbz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r4,10
	cr6.compare<uint32_t>(ctx.r4.u32, 10, xer);
	// bne cr6,0x821ad178
	if (!cr6.getEQ()) goto loc_821AD178;
	// addi r31,r31,2
	r31.s64 = r31.s64 + 2;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// b 0x821ad1d0
	goto loc_821AD1D0;
loc_821AD178:
	// cmpwi cr6,r6,92
	cr6.compare<int32_t>(ctx.r6.s32, 92, xer);
	// bne cr6,0x821ad1b0
	if (!cr6.getEQ()) goto loc_821AD1B0;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bge cr6,0x821ad1b0
	if (!cr6.getLT()) goto loc_821AD1B0;
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r9,13
	cr6.compare<uint32_t>(ctx.r9.u32, 13, xer);
	// bne cr6,0x821ad1b0
	if (!cr6.getEQ()) goto loc_821AD1B0;
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplwi cr6,r9,10
	cr6.compare<uint32_t>(ctx.r9.u32, 10, xer);
	// bne cr6,0x821ad1b0
	if (!cr6.getEQ()) goto loc_821AD1B0;
	// addi r31,r31,3
	r31.s64 = r31.s64 + 3;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// b 0x821ad1d0
	goto loc_821AD1D0;
loc_821AD1B0:
	// cmpwi cr6,r6,13
	cr6.compare<int32_t>(ctx.r6.s32, 13, xer);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// beq cr6,0x821ad1d0
	if (cr6.getEQ()) goto loc_821AD1D0;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stbx r7,r8,r9
	PPC_STORE_U8(ctx.r8.u32 + ctx.r9.u32, ctx.r7.u8);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
loc_821AD1D0:
	// cmplwi cr6,r8,255
	cr6.compare<uint32_t>(ctx.r8.u32, 255, xer);
	// blt cr6,0x821ad138
	if (cr6.getLT()) goto loc_821AD138;
loc_821AD1D8:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r6,r11,8188
	ctx.r6.s64 = r11.s64 + 8188;
	// li r11,0
	r11.s64 = 0;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r30,640
	ctx.r4.s64 = r30.s64 + 640;
	// addi r3,r30,24
	ctx.r3.s64 = r30.s64 + 24;
	// stbx r11,r8,r10
	PPC_STORE_U8(ctx.r8.u32 + ctx.r10.u32, r11.u8);
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// stw r11,84(r30)
	PPC_STORE_U32(r30.u32 + 84, r11.u32);
	// stw r11,80(r30)
	PPC_STORE_U32(r30.u32 + 80, r11.u32);
loc_821AD20C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AD228"))) PPC_WEAK_FUNC(sub_821AD228);
PPC_FUNC_IMPL(__imp__sub_821AD228) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,628(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 628);
	// lwz r10,56(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x821ad278
	if (!cr0.getEQ()) goto loc_821AD278;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,1508
	ctx.r5.s64 = 1508;
	// addi r6,r11,8236
	ctx.r6.s64 = r11.s64 + 8236;
loc_821AD258:
	// addi r4,r31,640
	ctx.r4.s64 = r31.s64 + 640;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// b 0x821ad2d4
	goto loc_821AD2D4;
loc_821AD278:
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821ad294
	if (cr6.getEQ()) goto loc_821AD294;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,1513
	ctx.r5.s64 = 1513;
	// addi r6,r11,8200
	ctx.r6.s64 = r11.s64 + 8200;
	// b 0x821ad258
	goto loc_821AD258;
loc_821AD294:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x821ad2b8
	if (cr6.getEQ()) goto loc_821AD2B8;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821ad2b8
	if (!cr6.getEQ()) goto loc_821AD2B8;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x821ad2bc
	if (!cr6.getEQ()) goto loc_821AD2BC;
loc_821AD2B8:
	// li r11,0
	r11.s64 = 0;
loc_821AD2BC:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
	// beq cr6,0x821ad2d0
	if (cr6.getEQ()) goto loc_821AD2D0;
	// li r11,1
	r11.s64 = 1;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
loc_821AD2D0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821AD2D4:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AD2E8"))) PPC_WEAK_FUNC(sub_821AD2E8);
PPC_FUNC_IMPL(__imp__sub_821AD2E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,628(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 628);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821ad338
	if (!cr0.getEQ()) goto loc_821AD338;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,1509
	ctx.r5.s64 = 1509;
	// addi r6,r11,8292
	ctx.r6.s64 = r11.s64 + 8292;
loc_821AD318:
	// addi r4,r31,640
	ctx.r4.s64 = r31.s64 + 640;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// b 0x821ad388
	goto loc_821AD388;
loc_821AD338:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x821ad354
	if (cr6.getEQ()) goto loc_821AD354;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,1514
	ctx.r5.s64 = 1514;
	// addi r6,r11,8256
	ctx.r6.s64 = r11.s64 + 8256;
	// b 0x821ad318
	goto loc_821AD318;
loc_821AD354:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// li r9,1
	ctx.r9.s64 = 1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x821ad374
	if (!cr6.getEQ()) goto loc_821AD374;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// bne cr6,0x821ad378
	if (!cr6.getEQ()) goto loc_821AD378;
loc_821AD374:
	// li r10,0
	ctx.r10.s64 = 0;
loc_821AD378:
	// stw r10,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r10.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
loc_821AD388:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AD3A0"))) PPC_WEAK_FUNC(sub_821AD3A0);
PPC_FUNC_IMPL(__imp__sub_821AD3A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed134
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r30,r28,640
	r30.s64 = r28.s64 + 640;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,672(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 672);
	// lwz r3,632(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821ad588
	if (cr0.getLT()) goto loc_821AD588;
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// bne cr6,0x821ad564
	if (!cr6.getEQ()) goto loc_821AD564;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r31,r28,648
	r31.s64 = r28.s64 + 648;
	// addi r10,r11,25744
	ctx.r10.s64 = r11.s64 + 25744;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_821AD3EC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad410
	if (cr0.getEQ()) goto loc_821AD410;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad3ec
	if (cr6.getEQ()) goto loc_821AD3EC;
loc_821AD410:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad564
	if (!cr0.getEQ()) goto loc_821AD564;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,672(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 672);
	// lwz r3,632(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821ad588
	if (cr0.getLT()) goto loc_821AD588;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x821ad45c
	if (!cr6.getEQ()) goto loc_821AD45C;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r29,0(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r4,672(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 672);
	// lwz r3,632(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821ad588
	if (cr0.getLT()) goto loc_821AD588;
	// b 0x821ad460
	goto loc_821AD460;
loc_821AD45C:
	// mr r29,r27
	r29.u64 = r27.u64;
loc_821AD460:
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// bne cr6,0x821ad564
	if (!cr6.getEQ()) goto loc_821AD564;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r11,8340
	ctx.r10.s64 = r11.s64 + 8340;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_821AD478:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad49c
	if (cr0.getEQ()) goto loc_821AD49C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad478
	if (cr6.getEQ()) goto loc_821AD478;
loc_821AD49C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad564
	if (!cr0.getEQ()) goto loc_821AD564;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,672(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 672);
	// lwz r3,632(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821ad588
	if (cr0.getLT()) goto loc_821AD588;
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpwi cr6,r7,12
	cr6.compare<int32_t>(ctx.r7.s32, 12, xer);
	// beq cr6,0x821ad4d0
	if (cr6.getEQ()) goto loc_821AD4D0;
	// cmpwi cr6,r7,13
	cr6.compare<int32_t>(ctx.r7.s32, 13, xer);
	// bne cr6,0x821ad564
	if (!cr6.getEQ()) goto loc_821AD564;
loc_821AD4D0:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x821ad4e0
	if (!cr6.getEQ()) goto loc_821AD4E0;
	// stw r27,60(r28)
	PPC_STORE_U32(r28.u32 + 60, r27.u32);
	// b 0x821ad584
	goto loc_821AD584;
loc_821AD4E0:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r11,8328
	ctx.r10.s64 = r11.s64 + 8328;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_821AD4EC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad510
	if (cr0.getEQ()) goto loc_821AD510;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad4ec
	if (cr6.getEQ()) goto loc_821AD4EC;
loc_821AD510:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad520
	if (!cr0.getEQ()) goto loc_821AD520;
	// li r11,1024
	r11.s64 = 1024;
	// b 0x821ad55c
	goto loc_821AD55C;
loc_821AD520:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r11,8312
	ctx.r10.s64 = r11.s64 + 8312;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_821AD52C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad550
	if (cr0.getEQ()) goto loc_821AD550;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad52c
	if (cr6.getEQ()) goto loc_821AD52C;
loc_821AD550:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad564
	if (!cr0.getEQ()) goto loc_821AD564;
	// li r11,2048
	r11.s64 = 2048;
loc_821AD55C:
	// stw r11,60(r28)
	PPC_STORE_U32(r28.u32 + 60, r11.u32);
	// b 0x821ad584
	goto loc_821AD584;
loc_821AD564:
	// cmpwi cr6,r7,12
	cr6.compare<int32_t>(ctx.r7.s32, 12, xer);
	// beq cr6,0x821ad580
	if (cr6.getEQ()) goto loc_821AD580;
	// cmpwi cr6,r7,13
	cr6.compare<int32_t>(ctx.r7.s32, 13, xer);
	// beq cr6,0x821ad580
	if (cr6.getEQ()) goto loc_821AD580;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,632(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// bl 0x821b8768
	sub_821B8768(ctx, base);
loc_821AD580:
	// stw r27,668(r28)
	PPC_STORE_U32(r28.u32 + 668, r27.u32);
loc_821AD584:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
loc_821AD588:
	// li r11,1
	r11.s64 = 1;
	// stw r11,76(r28)
	PPC_STORE_U32(r28.u32 + 76, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed184
	return;
}

__attribute__((alias("__imp__sub_821AD598"))) PPC_WEAK_FUNC(sub_821AD598);
PPC_FUNC_IMPL(__imp__sub_821AD598) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed100
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// addi r24,r26,640
	r24.s64 = r26.s64 + 640;
	// mr r14,r25
	r14.u64 = r25.u64;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// lwz r4,672(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 672);
	// lwz r3,632(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 632);
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x821ada6c
	if (cr0.getLT()) goto loc_821ADA6C;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821ada44
	if (!cr6.getEQ()) goto loc_821ADA44;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r27,r26,648
	r27.s64 = r26.s64 + 648;
	// addi r10,r11,25744
	ctx.r10.s64 = r11.s64 + 25744;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_821AD5EC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad610
	if (cr0.getEQ()) goto loc_821AD610;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad5ec
	if (cr6.getEQ()) goto loc_821AD5EC;
loc_821AD610:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ada44
	if (!cr0.getEQ()) goto loc_821ADA44;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// lwz r4,672(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 672);
	// lwz r3,632(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x821ada6c
	if (cr0.getLT()) goto loc_821ADA6C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r15,-1
	r15.s64 = -1;
	// addi r22,r11,8380
	r22.s64 = r11.s64 + 8380;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r20,r11,8376
	r20.s64 = r11.s64 + 8376;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r19,r11,8368
	r19.s64 = r11.s64 + 8368;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r18,r11,8360
	r18.s64 = r11.s64 + 8360;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r17,r11,8352
	r17.s64 = r11.s64 + 8352;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r21,r11,8344
	r21.s64 = r11.s64 + 8344;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r23,r11,8340
	r23.s64 = r11.s64 + 8340;
	// lis r11,16383
	r11.s64 = 1073676288;
	// ori r16,r11,65535
	r16.u64 = r11.u64 | 65535;
loc_821AD674:
	// lwz r7,0(r24)
	ctx.r7.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// bne cr6,0x821ad6b4
	if (!cr6.getEQ()) goto loc_821AD6B4;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
loc_821AD688:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad6ac
	if (cr0.getEQ()) goto loc_821AD6AC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad688
	if (cr6.getEQ()) goto loc_821AD688;
loc_821AD6AC:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821ada98
	if (cr0.getEQ()) goto loc_821ADA98;
loc_821AD6B4:
	// cmpwi cr6,r7,9
	cr6.compare<int32_t>(ctx.r7.s32, 9, xer);
	// bne cr6,0x821ad7b0
	if (!cr6.getEQ()) goto loc_821AD7B0;
	// lwz r7,0(r27)
	ctx.r7.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_821AD6C8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad6ec
	if (cr0.getEQ()) goto loc_821AD6EC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad6c8
	if (cr6.getEQ()) goto loc_821AD6C8;
loc_821AD6EC:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad6fc
	if (!cr0.getEQ()) goto loc_821AD6FC;
	// li r28,16
	r28.s64 = 16;
	// b 0x821ad7e0
	goto loc_821AD7E0;
loc_821AD6FC:
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
loc_821AD704:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad728
	if (cr0.getEQ()) goto loc_821AD728;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad704
	if (cr6.getEQ()) goto loc_821AD704;
loc_821AD728:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad738
	if (!cr0.getEQ()) goto loc_821AD738;
	// li r28,15
	r28.s64 = 15;
	// b 0x821ad7e0
	goto loc_821AD7E0;
loc_821AD738:
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
loc_821AD740:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad764
	if (cr0.getEQ()) goto loc_821AD764;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad740
	if (cr6.getEQ()) goto loc_821AD740;
loc_821AD764:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad774
	if (!cr0.getEQ()) goto loc_821AD774;
	// li r28,0
	r28.s64 = 0;
	// b 0x821ad7e0
	goto loc_821AD7E0;
loc_821AD774:
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
loc_821AD77C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad7a0
	if (cr0.getEQ()) goto loc_821AD7A0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad77c
	if (cr6.getEQ()) goto loc_821AD77C;
loc_821AD7A0:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ada44
	if (!cr0.getEQ()) goto loc_821ADA44;
	// li r28,255
	r28.s64 = 255;
	// b 0x821ad7e0
	goto loc_821AD7E0;
loc_821AD7B0:
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// beq cr6,0x821ad7c8
	if (cr6.getEQ()) goto loc_821AD7C8;
	// cmpwi cr6,r7,3
	cr6.compare<int32_t>(ctx.r7.s32, 3, xer);
	// beq cr6,0x821ad7c8
	if (cr6.getEQ()) goto loc_821AD7C8;
	// cmpwi cr6,r7,4
	cr6.compare<int32_t>(ctx.r7.s32, 4, xer);
	// bne cr6,0x821ada44
	if (!cr6.getEQ()) goto loc_821ADA44;
loc_821AD7C8:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x821ada44
	if (cr6.getLT()) goto loc_821ADA44;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bgt cr6,0x821ada44
	if (cr6.getGT()) goto loc_821ADA44;
	// mr r28,r11
	r28.u64 = r11.u64;
loc_821AD7E0:
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// lwz r4,672(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 672);
	// lwz r3,632(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x821ada6c
	if (cr0.getLT()) goto loc_821ADA6C;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821ada44
	if (!cr6.getEQ()) goto loc_821ADA44;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
loc_821AD80C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad830
	if (cr0.getEQ()) goto loc_821AD830;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad80c
	if (cr6.getEQ()) goto loc_821AD80C;
loc_821AD830:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ada44
	if (!cr0.getEQ()) goto loc_821ADA44;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// lwz r4,672(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 672);
	// lwz r3,632(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x821ada6c
	if (cr0.getLT()) goto loc_821ADA6C;
loc_821AD850:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x821ad86c
	if (cr6.getEQ()) goto loc_821AD86C;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x821ad86c
	if (cr6.getEQ()) goto loc_821AD86C;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x821ada44
	if (!cr6.getEQ()) goto loc_821ADA44;
loc_821AD86C:
	// not r11,r25
	r11.u64 = ~r25.u64;
	// lwz r29,0(r27)
	r29.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// and r11,r11,r25
	r11.u64 = r11.u64 & r25.u64;
	// cmplw cr6,r25,r11
	cr6.compare<uint32_t>(r25.u32, r11.u32, xer);
	// bne cr6,0x821ad918
	if (!cr6.getEQ()) goto loc_821AD918;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// rlwinm r11,r25,1,0,30
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x821ad894
	if (!cr6.getEQ()) goto loc_821AD894;
	// li r11,1
	r11.s64 = 1;
loc_821AD894:
	// cmplw cr6,r11,r16
	cr6.compare<uint32_t>(r11.u32, r16.u32, xer);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// ble cr6,0x821ad8a4
	if (!cr6.getGT()) goto loc_821AD8A4;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
loc_821AD8A4:
	// bl 0x8209d088
	sub_8209D088(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x821adb10
	if (cr0.getEQ()) goto loc_821ADB10;
	// rlwinm r30,r25,2,0,29
	r30.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8209d150
	sub_8209D150(ctx, base);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// rlwinm r11,r25,1,0,30
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x821ad8e0
	if (!cr6.getEQ()) goto loc_821AD8E0;
	// li r11,1
	r11.s64 = 1;
loc_821AD8E0:
	// cmplw cr6,r11,r16
	cr6.compare<uint32_t>(r11.u32, r16.u32, xer);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// ble cr6,0x821ad8f0
	if (!cr6.getGT()) goto loc_821AD8F0;
	// mr r3,r15
	ctx.r3.u64 = r15.u64;
loc_821AD8F0:
	// bl 0x8209d088
	sub_8209D088(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x821adb10
	if (cr0.getEQ()) goto loc_821ADB10;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// mr r3,r14
	ctx.r3.u64 = r14.u64;
	// bl 0x8209d150
	sub_8209D150(ctx, base);
	// mr r14,r31
	r14.u64 = r31.u64;
loc_821AD918:
	// rlwinm r11,r25,2,0,29
	r11.u64 = __builtin_rotateleft64(r25.u32 | (r25.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// stwx r29,r11,r14
	PPC_STORE_U32(r11.u32 + r14.u32, r29.u32);
	// stwx r28,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, r28.u32);
	// lwz r4,672(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 672);
	// lwz r3,632(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x821ada6c
	if (cr0.getLT()) goto loc_821ADA6C;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821ad850
	if (!cr6.getEQ()) goto loc_821AD850;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
loc_821AD958:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad97c
	if (cr0.getEQ()) goto loc_821AD97C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad958
	if (cr6.getEQ()) goto loc_821AD958;
loc_821AD97C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821ad9b8
	if (cr0.getEQ()) goto loc_821AD9B8;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
loc_821AD98C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad9b0
	if (cr0.getEQ()) goto loc_821AD9B0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad98c
	if (cr6.getEQ()) goto loc_821AD98C;
loc_821AD9B0:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad850
	if (!cr0.getEQ()) goto loc_821AD850;
loc_821AD9B8:
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
loc_821AD9C0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ad9e4
	if (cr0.getEQ()) goto loc_821AD9E4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ad9c0
	if (cr6.getEQ()) goto loc_821AD9C0;
loc_821AD9E4:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad674
	if (!cr0.getEQ()) goto loc_821AD674;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// lwz r4,672(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 672);
	// lwz r3,632(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x821ada6c
	if (cr0.getLT()) goto loc_821ADA6C;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821ad674
	if (!cr6.getEQ()) goto loc_821AD674;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
loc_821ADA18:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ada3c
	if (cr0.getEQ()) goto loc_821ADA3C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ada18
	if (cr6.getEQ()) goto loc_821ADA18;
loc_821ADA3C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821ad674
	if (!cr0.getEQ()) goto loc_821AD674;
loc_821ADA44:
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// beq cr6,0x821ada64
	if (cr6.getEQ()) goto loc_821ADA64;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// beq cr6,0x821ada64
	if (cr6.getEQ()) goto loc_821ADA64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,632(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 632);
	// bl 0x821b8768
	sub_821B8768(ctx, base);
loc_821ADA64:
	// li r31,0
	r31.s64 = 0;
	// stw r31,668(r26)
	PPC_STORE_U32(r26.u32 + 668, r31.u32);
loc_821ADA6C:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8209d150
	sub_8209D150(ctx, base);
	// mr r3,r14
	ctx.r3.u64 = r14.u64;
	// bl 0x8209d150
	sub_8209D150(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8209d150
	sub_8209D150(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,76(r26)
	PPC_STORE_U32(r26.u32 + 76, r11.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x823ed150
	return;
loc_821ADA98:
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// lwz r4,672(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 672);
	// lwz r3,632(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x821ada6c
	if (cr0.getLT()) goto loc_821ADA6C;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// beq cr6,0x821adac4
	if (cr6.getEQ()) goto loc_821ADAC4;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// bne cr6,0x821ada44
	if (!cr6.getEQ()) goto loc_821ADA44;
loc_821ADAC4:
	// li r27,0
	r27.s64 = 0;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x821adb08
	if (cr6.getEQ()) goto loc_821ADB08;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r29,r26,24
	r29.s64 = r26.s64 + 24;
	// mr r30,r14
	r30.u64 = r14.u64;
	// subf r28,r14,r11
	r28.s64 = r11.s64 - r14.s64;
loc_821ADAE0:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwzx r5,r28,r30
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + r30.u32);
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x821b8018
	sub_821B8018(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// blt 0x821ada6c
	if (cr0.getLT()) goto loc_821ADA6C;
	// addi r27,r27,1
	r27.s64 = r27.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r27,r25
	cr6.compare<uint32_t>(r27.u32, r25.u32, xer);
	// blt cr6,0x821adae0
	if (cr6.getLT()) goto loc_821ADAE0;
loc_821ADB08:
	// li r31,0
	r31.s64 = 0;
	// b 0x821ada6c
	goto loc_821ADA6C;
loc_821ADB10:
	// lis r31,-32761
	r31.s64 = -2147024896;
	// ori r31,r31,14
	r31.u64 = r31.u64 | 14;
	// b 0x821ada6c
	goto loc_821ADA6C;
}

__attribute__((alias("__imp__sub_821ADB20"))) PPC_WEAK_FUNC(sub_821ADB20);
PPC_FUNC_IMPL(__imp__sub_821ADB20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r10,1586
	ctx.r10.s64 = 103940096;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// ori r10,r10,55311
	ctx.r10.u64 = ctx.r10.u64 | 55311;
	// beq cr6,0x821adb68
	if (cr6.getEQ()) goto loc_821ADB68;
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// extsb. r11,r11
	r11.s64 = r11.s8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821adb68
	if (cr0.getEQ()) goto loc_821ADB68;
loc_821ADB3C:
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// mulli r10,r10,19
	ctx.r10.s64 = ctx.r10.s64 * 19;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// extsb. r11,r11
	r11.s64 = r11.s8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x821adb3c
	if (!cr0.getEQ()) goto loc_821ADB3C;
	// li r11,127
	r11.s64 = 127;
	// divwu r11,r10,r11
	r11.u32 = ctx.r10.u32 / r11.u32;
	// mulli r11,r11,127
	r11.s64 = r11.s64 * 127;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// blr 
	return;
loc_821ADB68:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821ADB70"))) PPC_WEAK_FUNC(sub_821ADB70);
PPC_FUNC_IMPL(__imp__sub_821ADB70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	// mflr r12
	// bl 0x823ed13c
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
loc_821ADB80:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821adba4
	if (cr0.getEQ()) goto loc_821ADBA4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821adb80
	if (cr6.getEQ()) goto loc_821ADB80;
loc_821ADBA4:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821adca8
	if (!cr0.getEQ()) goto loc_821ADCA8;
	// lwz r30,4(r4)
	r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r29,4(r5)
	r29.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr. r11,r30
	r11.u64 = r30.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821adbe0
	if (cr0.getEQ()) goto loc_821ADBE0;
loc_821ADBC0:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x821adbd8
	if (cr6.getEQ()) goto loc_821ADBD8;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821adbc0
	if (!cr0.getEQ()) goto loc_821ADBC0;
loc_821ADBD8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821adca8
	if (!cr6.getEQ()) goto loc_821ADCA8;
loc_821ADBE0:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x821adca8
	if (!cr6.getEQ()) goto loc_821ADCA8;
	// lwz r3,8(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r31,8(r5)
	r31.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	// b 0x821ade04
	goto loc_821ADE04;
loc_821ADBF4:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x821adc94
	if (cr6.getEQ()) goto loc_821ADC94;
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x821adc94
	if (!cr6.getEQ()) goto loc_821ADC94;
	// cmplwi cr6,r11,11
	cr6.compare<uint32_t>(r11.u32, 11, xer);
	// bgt cr6,0x821addf0
	if (cr6.getGT()) goto loc_821ADDF0;
	// lis r12,-32254
	r12.s64 = -2113798144;
	// addi r12,r12,7992
	r12.s64 = r12.s64 + 7992;
	// lbzx r0,r12,r11
	r0.u64 = PPC_LOAD_U8(r12.u32 + r11.u32);
	// rlwinm r0,r0,2,0,29
	r0.u64 = __builtin_rotateleft64(r0.u32 | (r0.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r12,-32229
	r12.s64 = -2112159744;
	// addi r12,r12,-9156
	r12.s64 = r12.s64 + -9156;
	// add r12,r12,r0
	r12.u64 = r12.u64 + r0.u64;
	// mtctr r12
	ctr.u64 = r12.u64;
	// nop 
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_821ADC3C;
	case 1:
		goto loc_821ADC54;
	case 2:
		goto loc_821ADC3C;
	case 3:
		goto loc_821ADC3C;
	case 4:
		goto loc_821ADC3C;
	case 5:
		goto loc_821ADCB0;
	case 6:
		goto loc_821ADCB0;
	case 7:
		goto loc_821ADCB0;
	case 8:
		goto loc_821ADCB0;
	case 9:
		goto loc_821ADCC0;
	case 10:
		goto loc_821ADDC0;
	case 11:
		goto loc_821ADDC0;
	default:
		__builtin_unreachable();
	}
loc_821ADC3C:
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
loc_821ADC48:
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
loc_821ADC4C:
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// b 0x821addf4
	goto loc_821ADDF4;
loc_821ADC54:
	// lbz r11,24(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 24);
	// lbz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 24);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x821adc94
	if (!cr6.getEQ()) goto loc_821ADC94;
	// lbz r11,25(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 25);
	// lbz r10,25(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 25);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x821adc94
	if (!cr6.getEQ()) goto loc_821ADC94;
	// lbz r11,26(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 26);
	// lbz r10,26(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 26);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x821adc94
	if (!cr6.getEQ()) goto loc_821ADC94;
	// lbz r11,27(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 27);
	// lbz r10,27(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 27);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
loc_821ADC90:
	// beq cr6,0x821addfc
	if (cr6.getEQ()) goto loc_821ADDFC;
loc_821ADC94:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x821adca8
	if (!cr6.getEQ()) goto loc_821ADCA8;
loc_821ADC9C:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// li r3,1
	ctx.r3.s64 = 1;
	// beq cr6,0x821adcac
	if (cr6.getEQ()) goto loc_821ADCAC;
loc_821ADCA8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821ADCAC:
	// b 0x823ed18c
	return;
loc_821ADCB0:
	// lfd f0,24(r3)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r3.u32 + 24);
	// lfd f13,24(r31)
	ctx.f13.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// b 0x821adc90
	goto loc_821ADC90;
loc_821ADCC0:
	// mr. r9,r30
	ctx.r9.u64 = r30.u64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r6,1
	ctx.r6.s64 = 1;
	// beq 0x821add20
	if (cr0.getEQ()) goto loc_821ADD20;
	// lwz r5,24(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
loc_821ADCD4:
	// lwz r10,24(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_821ADCDC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821add00
	if (cr0.getEQ()) goto loc_821ADD00;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821adcdc
	if (cr6.getEQ()) goto loc_821ADCDC;
loc_821ADD00:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x821add1c
	if (cr0.getEQ()) goto loc_821ADD1C;
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne 0x821adcd4
	if (!cr0.getEQ()) goto loc_821ADCD4;
	// b 0x821add20
	goto loc_821ADD20;
loc_821ADD1C:
	// li r4,1
	ctx.r4.s64 = 1;
loc_821ADD20:
	// mr. r8,r29
	ctx.r8.u64 = r29.u64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r5,r6,-1
	ctx.r5.s64 = ctx.r6.s64 + -1;
	// beq 0x821add7c
	if (cr0.getEQ()) goto loc_821ADD7C;
	// lwz r6,24(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 24);
loc_821ADD30:
	// lwz r10,24(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 24);
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
loc_821ADD38:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// beq 0x821add5c
	if (cr0.getEQ()) goto loc_821ADD5C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821add38
	if (cr6.getEQ()) goto loc_821ADD38;
loc_821ADD5C:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x821add78
	if (cr0.getEQ()) goto loc_821ADD78;
	// lwz r8,12(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// cmplwi r8,0
	cr0.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne 0x821add30
	if (!cr0.getEQ()) goto loc_821ADD30;
	// b 0x821add7c
	goto loc_821ADD7C;
loc_821ADD78:
	// li r4,1
	ctx.r4.s64 = 1;
loc_821ADD7C:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x821add8c
	if (cr6.getEQ()) goto loc_821ADD8C;
	// addi r11,r5,0
	r11.s64 = ctx.r5.s64 + 0;
	// b 0x821adc48
	goto loc_821ADC48;
loc_821ADD8C:
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
loc_821ADD94:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821addb8
	if (cr0.getEQ()) goto loc_821ADDB8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821add94
	if (cr6.getEQ()) goto loc_821ADD94;
loc_821ADDB8:
	// cntlzw r11,r8
	r11.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// b 0x821adc4c
	goto loc_821ADC4C;
loc_821ADDC0:
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
loc_821ADDC8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821addb8
	if (cr0.getEQ()) goto loc_821ADDB8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821addc8
	if (cr6.getEQ()) goto loc_821ADDC8;
	// b 0x821addb8
	goto loc_821ADDB8;
loc_821ADDF0:
	// li r11,1
	r11.s64 = 1;
loc_821ADDF4:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821adc94
	if (cr6.getEQ()) goto loc_821ADC94;
loc_821ADDFC:
	// lwz r3,12(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r31,12(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 12);
loc_821ADE04:
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x821adbf4
	if (!cr0.getEQ()) goto loc_821ADBF4;
	// b 0x821adc9c
	goto loc_821ADC9C;
}

__attribute__((alias("__imp__sub_821ADE10"))) PPC_WEAK_FUNC(sub_821ADE10);
PPC_FUNC_IMPL(__imp__sub_821ADE10) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,632(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 632);
	// b 0x821b7618
	sub_821B7618(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_821ADE18"))) PPC_WEAK_FUNC(sub_821ADE18);
PPC_FUNC_IMPL(__imp__sub_821ADE18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// bl 0x821adb20
	sub_821ADB20(ctx, base);
	// addi r11,r3,30
	r11.s64 = ctx.r3.s64 + 30;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r11,r29
	r30.u64 = PPC_LOAD_U32(r11.u32 + r29.u32);
	// b 0x821ade80
	goto loc_821ADE80;
loc_821ADE40:
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_821ADE48:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ade6c
	if (cr0.getEQ()) goto loc_821ADE6C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ade48
	if (cr6.getEQ()) goto loc_821ADE48;
loc_821ADE6C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt 0x821ade88
	if (cr0.getLT()) goto loc_821ADE88;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ade90
	if (cr6.getEQ()) goto loc_821ADE90;
	// lwz r30,12(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 12);
loc_821ADE80:
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// bne 0x821ade40
	if (!cr0.getEQ()) goto loc_821ADE40;
loc_821ADE88:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x821adef8
	goto loc_821ADEF8;
loc_821ADE90:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 16);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821adea4
	if (cr6.getEQ()) goto loc_821ADEA4;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x821adef8
	goto loc_821ADEF8;
loc_821ADEA4:
	// li r11,1
	r11.s64 = 1;
	// lwz r31,8(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// b 0x821aded8
	goto loc_821ADED8;
loc_821ADEB4:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x821aded4
	if (!cr6.getEQ()) goto loc_821ADED4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// bl 0x821ade18
	sub_821ADE18(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821adee0
	if (!cr0.getEQ()) goto loc_821ADEE0;
loc_821ADED4:
	// lwz r31,12(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 12);
loc_821ADED8:
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// bne 0x821adeb4
	if (!cr0.getEQ()) goto loc_821ADEB4;
loc_821ADEE0:
	// li r11,0
	r11.s64 = 0;
	// subf r10,r11,r31
	ctx.r10.s64 = r31.s64 - r11.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r3,r10,1
	ctx.r3.u64 = ctx.r10.u64 ^ 1;
loc_821ADEF8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821ADF00"))) PPC_WEAK_FUNC(sub_821ADF00);
PPC_FUNC_IMPL(__imp__sub_821ADF00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x821ade18
	sub_821ADE18(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821adf8c
	if (!cr0.getEQ()) goto loc_821ADF8C;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821adb20
	sub_821ADB20(ctx, base);
	// addi r11,r3,30
	r11.s64 = ctx.r3.s64 + 30;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r11,r31
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// b 0x821adf84
	goto loc_821ADF84;
loc_821ADF44:
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r11,r30
	r11.u64 = r30.u64;
loc_821ADF4C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821adf70
	if (cr0.getEQ()) goto loc_821ADF70;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821adf4c
	if (cr6.getEQ()) goto loc_821ADF4C;
loc_821ADF70:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt 0x821adf8c
	if (cr0.getLT()) goto loc_821ADF8C;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821adfa8
	if (cr6.getEQ()) goto loc_821ADFA8;
	// lwz r7,12(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
loc_821ADF84:
	// cmplwi r7,0
	cr0.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne 0x821adf44
	if (!cr0.getEQ()) goto loc_821ADF44;
loc_821ADF8C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821ADF90:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_821ADFA8:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x821adfb8
	if (cr6.getEQ()) goto loc_821ADFB8;
	// lwz r11,4(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
loc_821ADFB8:
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x821adfc8
	if (cr6.getEQ()) goto loc_821ADFC8;
	// lwz r11,8(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r11.u32);
loc_821ADFC8:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x821adf90
	goto loc_821ADF90;
}

__attribute__((alias("__imp__sub_821ADFD0"))) PPC_WEAK_FUNC(sub_821ADFD0);
PPC_FUNC_IMPL(__imp__sub_821ADFD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r3,0
	ctx.r3.s64 = 0;
	// li r11,0
	r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beqlr cr6
	if (cr6.getEQ()) return;
	// li r9,92
	ctx.r9.s64 = 92;
loc_821ADFE8:
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// cmplwi cr6,r8,34
	cr6.compare<uint32_t>(ctx.r8.u32, 34, xer);
	// bne cr6,0x821ae01c
	if (!cr6.getEQ()) goto loc_821AE01C;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x821ae008
	if (cr6.getEQ()) goto loc_821AE008;
	// cmplw cr6,r3,r7
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r7.u32, xer);
	// bge cr6,0x821ae008
	if (!cr6.getLT()) goto loc_821AE008;
	// stbx r9,r3,r6
	PPC_STORE_U8(ctx.r3.u32 + ctx.r6.u32, ctx.r9.u8);
loc_821AE008:
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821ae01c
	if (!cr6.getEQ()) goto loc_821AE01C;
	// cntlzw r11,r10
	r11.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r11,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
loc_821AE01C:
	// li r11,0
	r11.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x821ae050
	if (cr6.getEQ()) goto loc_821AE050;
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// cmplwi cr6,r8,92
	cr6.compare<uint32_t>(ctx.r8.u32, 92, xer);
	// bne cr6,0x821ae050
	if (!cr6.getEQ()) goto loc_821AE050;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x821ae048
	if (cr6.getEQ()) goto loc_821AE048;
	// cmplw cr6,r3,r7
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r7.u32, xer);
	// bge cr6,0x821ae048
	if (!cr6.getLT()) goto loc_821AE048;
	// stbx r9,r3,r6
	PPC_STORE_U8(ctx.r3.u32 + ctx.r6.u32, ctx.r9.u8);
loc_821AE048:
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// li r11,1
	r11.s64 = 1;
loc_821AE050:
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x821ae068
	if (cr6.getEQ()) goto loc_821AE068;
	// cmplw cr6,r3,r7
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r7.u32, xer);
	// bge cr6,0x821ae068
	if (!cr6.getLT()) goto loc_821AE068;
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// stbx r8,r3,r6
	PPC_STORE_U8(ctx.r3.u32 + ctx.r6.u32, ctx.r8.u8);
loc_821AE068:
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// bne 0x821adfe8
	if (!cr0.getEQ()) goto loc_821ADFE8;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AE080"))) PPC_WEAK_FUNC(sub_821AE080);
PPC_FUNC_IMPL(__imp__sub_821AE080) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x821ae0a4
	if (cr6.getEQ()) goto loc_821AE0A4;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// b 0x821ae0d4
	goto loc_821AE0D4;
loc_821AE0A4:
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821ae0d0
	if (!cr6.getEQ()) goto loc_821AE0D0;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r6,r11,8384
	ctx.r6.s64 = r11.s64 + 8384;
	// addi r4,r31,640
	ctx.r4.s64 = r31.s64 + 640;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
loc_821AE0D0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821AE0D4:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AE0E8"))) PPC_WEAK_FUNC(sub_821AE0E8);
PPC_FUNC_IMPL(__imp__sub_821AE0E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae118
	if (cr0.getEQ()) goto loc_821AE118;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821ae0e8
	sub_821AE0E8(ctx, base);
loc_821AE118:
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821ae12c
	if (cr0.getEQ()) goto loc_821AE12C;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_821AE12C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AE148"))) PPC_WEAK_FUNC(sub_821AE148);
PPC_FUNC_IMPL(__imp__sub_821AE148) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae170
	if (cr0.getEQ()) goto loc_821AE170;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821ae0e8
	sub_821AE0E8(ctx, base);
loc_821AE170:
	// lwz r3,92(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae184
	if (cr0.getEQ()) goto loc_821AE184;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821ae1d8
	sub_821AE1D8(ctx, base);
loc_821AE184:
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821ae1b0
	if (cr6.getEQ()) goto loc_821AE1B0;
	// lwz r4,84(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 84);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x821ae1b0
	if (cr0.getEQ()) goto loc_821AE1B0;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_821AE1B0:
	// addi r3,r31,60
	ctx.r3.s64 = r31.s64 + 60;
	// bl 0x821d5878
	sub_821D5878(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823d22e0
	sub_823D22E0(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AE1D8"))) PPC_WEAK_FUNC(sub_821AE1D8);
PPC_FUNC_IMPL(__imp__sub_821AE1D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x821ae148
	sub_821AE148(ctx, base);
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821ae20c
	if (cr0.getEQ()) goto loc_821AE20C;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_821AE20C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AE228"))) PPC_WEAK_FUNC(sub_821AE228);
PPC_FUNC_IMPL(__imp__sub_821AE228) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae258
	if (cr0.getEQ()) goto loc_821AE258;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821ae228
	sub_821AE228(ctx, base);
loc_821AE258:
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821ae26c
	if (cr0.getEQ()) goto loc_821AE26C;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_821AE26C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AE288"))) PPC_WEAK_FUNC(sub_821AE288);
PPC_FUNC_IMPL(__imp__sub_821AE288) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae2b8
	if (cr0.getEQ()) goto loc_821AE2B8;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821ae288
	sub_821AE288(ctx, base);
loc_821AE2B8:
	// clrlwi. r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x821ae2cc
	if (cr0.getEQ()) goto loc_821AE2CC;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_821AE2CC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AE2E8"))) PPC_WEAK_FUNC(sub_821AE2E8);
PPC_FUNC_IMPL(__imp__sub_821AE2E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// lis r4,16
	ctx.r4.s64 = 1048576;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x821d5890
	sub_821D5890(ctx, base);
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b7450
	sub_821B7450(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r30,684(r31)
	PPC_STORE_U32(r31.u32 + 684, r30.u32);
	// bl 0x821acf98
	sub_821ACF98(ctx, base);
	// li r29,1
	r29.s64 = 1;
	// stw r30,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r30.u32);
	// li r5,508
	ctx.r5.s64 = 508;
	// stw r30,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r30.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r30,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r30.u32);
	// addi r3,r31,120
	ctx.r3.s64 = r31.s64 + 120;
	// stw r30,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r30.u32);
	// stw r30,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r30.u32);
	// stw r29,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r29.u32);
	// stw r29,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r29.u32);
	// stw r29,92(r31)
	PPC_STORE_U32(r31.u32 + 92, r29.u32);
	// stw r29,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r29.u32);
	// stw r30,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r30.u32);
	// stw r30,104(r31)
	PPC_STORE_U32(r31.u32 + 104, r30.u32);
	// stw r30,108(r31)
	PPC_STORE_U32(r31.u32 + 108, r30.u32);
	// stw r30,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r30.u32);
	// stw r30,116(r31)
	PPC_STORE_U32(r31.u32 + 116, r30.u32);
	// stw r30,628(r31)
	PPC_STORE_U32(r31.u32 + 628, r30.u32);
	// stw r30,632(r31)
	PPC_STORE_U32(r31.u32 + 632, r30.u32);
	// stw r30,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r30.u32);
	// bl 0x823edf70
	sub_823EDF70(ctx, base);
	// std r30,640(r31)
	PPC_STORE_U64(r31.u32 + 640, r30.u64);
	// li r4,0
	ctx.r4.s64 = 0;
	// std r30,648(r31)
	PPC_STORE_U64(r31.u32 + 648, r30.u64);
	// li r3,4
	ctx.r3.s64 = 4;
	// std r30,656(r31)
	PPC_STORE_U64(r31.u32 + 656, r30.u64);
	// std r30,664(r31)
	PPC_STORE_U64(r31.u32 + 664, r30.u64);
	// stw r29,672(r31)
	PPC_STORE_U32(r31.u32 + 672, r29.u32);
	// bl 0x823f0760
	sub_823F0760(ctx, base);
	// bl 0x823f06c8
	sub_823F06C8(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,676(r31)
	PPC_STORE_U32(r31.u32 + 676, ctx.r3.u32);
	// addi r4,r11,-20548
	ctx.r4.s64 = r11.s64 + -20548;
	// beq 0x821ae3dc
	if (cr0.getEQ()) goto loc_821AE3DC;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_821AE3B0:
	// lbz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmpwi r10,0
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// subf r9,r9,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r9.s64;
	// beq 0x821ae3d4
	if (cr0.getEQ()) goto loc_821AE3D4;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x821ae3b0
	if (cr6.getEQ()) goto loc_821AE3B0;
loc_821AE3D4:
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x821ae3e4
	if (cr0.getEQ()) goto loc_821AE3E4;
loc_821AE3DC:
	// li r3,4
	ctx.r3.s64 = 4;
	// bl 0x823f0760
	sub_823F0760(ctx, base);
loc_821AE3E4:
	// stw r30,692(r31)
	PPC_STORE_U32(r31.u32 + 692, r30.u32);
	// bl 0x821d9a78
	sub_821D9A78(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821AE3F8"))) PPC_WEAK_FUNC(sub_821AE3F8);
PPC_FUNC_IMPL(__imp__sub_821AE3F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r3,64(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 64);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae41c
	if (cr0.getEQ()) goto loc_821AE41C;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821ae288
	sub_821AE288(ctx, base);
loc_821AE41C:
	// lwz r3,116(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 116);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae430
	if (cr0.getEQ()) goto loc_821AE430;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821ae228
	sub_821AE228(ctx, base);
loc_821AE430:
	// lwz r31,628(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 628);
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x821ae450
	if (cr0.getEQ()) goto loc_821AE450;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821ae148
	sub_821AE148(ctx, base);
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8209d060
	sub_8209D060(ctx, base);
loc_821AE450:
	// lwz r3,68(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 68);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae464
	if (cr0.getEQ()) goto loc_821AE464;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821ae228
	sub_821AE228(ctx, base);
loc_821AE464:
	// addi r31,r30,120
	r31.s64 = r30.s64 + 120;
	// li r29,127
	r29.s64 = 127;
loc_821AE46C:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae480
	if (cr0.getEQ()) goto loc_821AE480;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821ae0e8
	sub_821AE0E8(ctx, base);
loc_821AE480:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// bne 0x821ae46c
	if (!cr0.getEQ()) goto loc_821AE46C;
	// lwz r11,684(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 684);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821ae4a0
	if (cr6.getEQ()) goto loc_821AE4A0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821acff0
	sub_821ACFF0(ctx, base);
loc_821AE4A0:
	// addi r3,r30,24
	ctx.r3.s64 = r30.s64 + 24;
	// bl 0x821b7cd0
	sub_821B7CD0(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821d58d8
	sub_821D58D8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821AE4B8"))) PPC_WEAK_FUNC(sub_821AE4B8);
PPC_FUNC_IMPL(__imp__sub_821AE4B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821ae504
	if (cr0.getEQ()) goto loc_821AE504;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// b 0x821ae508
	goto loc_821AE508;
loc_821AE504:
	// li r11,0
	r11.s64 = 0;
loc_821AE508:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821ae51c
	if (!cr6.getEQ()) goto loc_821AE51C;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x821ae530
	goto loc_821AE530;
loc_821AE51C:
	// lwz r10,116(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// stw r11,116(r31)
	PPC_STORE_U32(r31.u32 + 116, r11.u32);
	// stw r30,672(r31)
	PPC_STORE_U32(r31.u32 + 672, r30.u32);
loc_821AE530:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AE548"))) PPC_WEAK_FUNC(sub_821AE548);
PPC_FUNC_IMPL(__imp__sub_821AE548) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// lwz r11,76(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 76);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821aed08
	if (!cr6.getEQ()) goto loc_821AED08;
	// lwz r11,84(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 84);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821aed08
	if (!cr6.getEQ()) goto loc_821AED08;
	// lwz r10,112(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 112);
	// li r30,0
	r30.s64 = 0;
	// addi r31,r28,640
	r31.s64 = r28.s64 + 640;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x821ae5b8
	if (cr0.getEQ()) goto loc_821AE5B8;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r9,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r9.u64);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// std r9,8(r31)
	PPC_STORE_U64(r31.u32 + 8, ctx.r9.u64);
	// ld r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// std r9,16(r31)
	PPC_STORE_U64(r31.u32 + 16, ctx.r9.u64);
	// ld r11,24(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 24);
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// stw r11,112(r28)
	PPC_STORE_U32(r28.u32 + 112, r11.u32);
	// stw r30,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, r30.u32);
	// b 0x821ae5d4
	goto loc_821AE5D4;
loc_821AE5B8:
	// lwz r11,672(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 672);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r3,632(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 632);
	// ori r4,r11,4
	ctx.r4.u64 = r11.u64 | 4;
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821aed10
	if (cr0.getLT()) goto loc_821AED10;
loc_821AE5D4:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x821aec64
	if (cr6.getEQ()) goto loc_821AEC64;
	// ble cr6,0x821aed00
	if (!cr6.getGT()) goto loc_821AED00;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// ble cr6,0x821aec5c
	if (!cr6.getGT()) goto loc_821AEC5C;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// beq cr6,0x821ae620
	if (cr6.getEQ()) goto loc_821AE620;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// beq cr6,0x821ae618
	if (cr6.getEQ()) goto loc_821AE618;
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// beq cr6,0x821ae60c
	if (cr6.getEQ()) goto loc_821AE60C;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// bne cr6,0x821aed00
	if (!cr6.getEQ()) goto loc_821AED00;
loc_821AE60C:
	// li r11,1
	r11.s64 = 1;
	// stw r11,76(r28)
	PPC_STORE_U32(r28.u32 + 76, r11.u32);
	// b 0x821aed10
	goto loc_821AED10;
loc_821AE618:
	// li r3,280
	ctx.r3.s64 = 280;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE620:
	// lwz r11,88(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821aeb18
	if (cr6.getEQ()) goto loc_821AEB18;
	// lwz r11,92(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 92);
	// li r29,1
	r29.s64 = 1;
	// stw r30,88(r28)
	PPC_STORE_U32(r28.u32 + 88, r30.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lwz r11,648(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 648);
	// stw r30,100(r28)
	PPC_STORE_U32(r28.u32 + 100, r30.u32);
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// beq cr6,0x821ae96c
	if (cr6.getEQ()) goto loc_821AE96C;
	// cmpwi cr6,r10,100
	cr6.compare<int32_t>(ctx.r10.s32, 100, xer);
	// beq cr6,0x821ae930
	if (cr6.getEQ()) goto loc_821AE930;
	// cmpwi cr6,r10,101
	cr6.compare<int32_t>(ctx.r10.s32, 101, xer);
	// beq cr6,0x821ae830
	if (cr6.getEQ()) goto loc_821AE830;
	// cmpwi cr6,r10,105
	cr6.compare<int32_t>(ctx.r10.s32, 105, xer);
	// beq cr6,0x821ae734
	if (cr6.getEQ()) goto loc_821AE734;
	// cmpwi cr6,r10,108
	cr6.compare<int32_t>(ctx.r10.s32, 108, xer);
	// beq cr6,0x821ae6f8
	if (cr6.getEQ()) goto loc_821AE6F8;
	// cmpwi cr6,r10,112
	cr6.compare<int32_t>(ctx.r10.s32, 112, xer);
	// beq cr6,0x821ae6bc
	if (cr6.getEQ()) goto loc_821AE6BC;
	// cmpwi cr6,r10,117
	cr6.compare<int32_t>(ctx.r10.s32, 117, xer);
	// bne cr6,0x821aea28
	if (!cr6.getEQ()) goto loc_821AEA28;
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r10,r10,8556
	ctx.r10.s64 = ctx.r10.s64 + 8556;
loc_821AE688:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ae6ac
	if (cr0.getEQ()) goto loc_821AE6AC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ae688
	if (cr6.getEQ()) goto loc_821AE688;
loc_821AE6AC:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821aea28
	if (!cr0.getEQ()) goto loc_821AEA28;
	// li r3,258
	ctx.r3.s64 = 258;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE6BC:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r10,r10,8548
	ctx.r10.s64 = ctx.r10.s64 + 8548;
loc_821AE6C4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ae6e8
	if (cr0.getEQ()) goto loc_821AE6E8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ae6c4
	if (cr6.getEQ()) goto loc_821AE6C4;
loc_821AE6E8:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821aea28
	if (!cr0.getEQ()) goto loc_821AEA28;
	// li r3,270
	ctx.r3.s64 = 270;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE6F8:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r10,r10,8540
	ctx.r10.s64 = ctx.r10.s64 + 8540;
loc_821AE700:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ae724
	if (cr0.getEQ()) goto loc_821AE724;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ae700
	if (cr6.getEQ()) goto loc_821AE700;
loc_821AE724:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821aea28
	if (!cr0.getEQ()) goto loc_821AEA28;
	// li r3,259
	ctx.r3.s64 = 259;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE734:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8536
	ctx.r9.s64 = ctx.r10.s64 + 8536;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AE740:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821ae764
	if (cr0.getEQ()) goto loc_821AE764;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821ae740
	if (cr6.getEQ()) goto loc_821AE740;
loc_821AE764:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x821ae774
	if (!cr0.getEQ()) goto loc_821AE774;
	// li r3,262
	ctx.r3.s64 = 262;
	// b 0x821ae86c
	goto loc_821AE86C;
loc_821AE774:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8528
	ctx.r9.s64 = ctx.r10.s64 + 8528;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AE780:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821ae7a4
	if (cr0.getEQ()) goto loc_821AE7A4;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821ae780
	if (cr6.getEQ()) goto loc_821AE780;
loc_821AE7A4:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x821ae7b4
	if (!cr0.getEQ()) goto loc_821AE7B4;
	// li r3,263
	ctx.r3.s64 = 263;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE7B4:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8520
	ctx.r9.s64 = ctx.r10.s64 + 8520;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AE7C0:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821ae7e4
	if (cr0.getEQ()) goto loc_821AE7E4;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821ae7c0
	if (cr6.getEQ()) goto loc_821AE7C0;
loc_821AE7E4:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x821ae7f4
	if (!cr0.getEQ()) goto loc_821AE7F4;
	// li r3,264
	ctx.r3.s64 = 264;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE7F4:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r10,r10,8512
	ctx.r10.s64 = ctx.r10.s64 + 8512;
loc_821AE7FC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ae820
	if (cr0.getEQ()) goto loc_821AE820;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ae7fc
	if (cr6.getEQ()) goto loc_821AE7FC;
loc_821AE820:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821aea28
	if (!cr0.getEQ()) goto loc_821AEA28;
	// li r3,260
	ctx.r3.s64 = 260;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE830:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8504
	ctx.r9.s64 = ctx.r10.s64 + 8504;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AE83C:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821ae860
	if (cr0.getEQ()) goto loc_821AE860;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821ae83c
	if (cr6.getEQ()) goto loc_821AE83C;
loc_821AE860:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x821ae874
	if (!cr0.getEQ()) goto loc_821AE874;
loc_821AE868:
	// li r3,265
	ctx.r3.s64 = 265;
loc_821AE86C:
	// stw r29,100(r28)
	PPC_STORE_U32(r28.u32 + 100, r29.u32);
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE874:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8496
	ctx.r9.s64 = ctx.r10.s64 + 8496;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AE880:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821ae8a4
	if (cr0.getEQ()) goto loc_821AE8A4;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821ae880
	if (cr6.getEQ()) goto loc_821AE880;
loc_821AE8A4:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x821ae8b4
	if (!cr0.getEQ()) goto loc_821AE8B4;
loc_821AE8AC:
	// li r3,266
	ctx.r3.s64 = 266;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE8B4:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8488
	ctx.r9.s64 = ctx.r10.s64 + 8488;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AE8C0:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821ae8e4
	if (cr0.getEQ()) goto loc_821AE8E4;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821ae8c0
	if (cr6.getEQ()) goto loc_821AE8C0;
loc_821AE8E4:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x821ae8f4
	if (!cr0.getEQ()) goto loc_821AE8F4;
loc_821AE8EC:
	// li r3,267
	ctx.r3.s64 = 267;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE8F4:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r10,r10,8352
	ctx.r10.s64 = ctx.r10.s64 + 8352;
loc_821AE8FC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ae920
	if (cr0.getEQ()) goto loc_821AE920;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ae8fc
	if (cr6.getEQ()) goto loc_821AE8FC;
loc_821AE920:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821aea28
	if (!cr0.getEQ()) goto loc_821AEA28;
	// li r3,261
	ctx.r3.s64 = 261;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE930:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r10,r10,8480
	ctx.r10.s64 = ctx.r10.s64 + 8480;
loc_821AE938:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821ae95c
	if (cr0.getEQ()) goto loc_821AE95C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ae938
	if (cr6.getEQ()) goto loc_821AE938;
loc_821AE95C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821aea28
	if (!cr0.getEQ()) goto loc_821AEA28;
	// li r3,257
	ctx.r3.s64 = 257;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE96C:
	// cmpwi cr6,r10,101
	cr6.compare<int32_t>(ctx.r10.s32, 101, xer);
	// beq cr6,0x821aea34
	if (cr6.getEQ()) goto loc_821AEA34;
	// cmpwi cr6,r10,105
	cr6.compare<int32_t>(ctx.r10.s32, 105, xer);
	// bne cr6,0x821aea28
	if (!cr6.getEQ()) goto loc_821AEA28;
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8536
	ctx.r9.s64 = ctx.r10.s64 + 8536;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AE988:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821ae9ac
	if (cr0.getEQ()) goto loc_821AE9AC;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821ae988
	if (cr6.getEQ()) goto loc_821AE988;
loc_821AE9AC:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x821ae9bc
	if (!cr0.getEQ()) goto loc_821AE9BC;
loc_821AE9B4:
	// li r3,268
	ctx.r3.s64 = 268;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AE9BC:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8528
	ctx.r9.s64 = ctx.r10.s64 + 8528;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AE9C8:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821ae9ec
	if (cr0.getEQ()) goto loc_821AE9EC;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821ae9c8
	if (cr6.getEQ()) goto loc_821AE9C8;
loc_821AE9EC:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x821ae9b4
	if (cr0.getEQ()) goto loc_821AE9B4;
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r10,r10,8520
	ctx.r10.s64 = ctx.r10.s64 + 8520;
loc_821AE9FC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821aea20
	if (cr0.getEQ()) goto loc_821AEA20;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821ae9fc
	if (cr6.getEQ()) goto loc_821AE9FC;
loc_821AEA20:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821ae9b4
	if (cr0.getEQ()) goto loc_821AE9B4;
loc_821AEA28:
	// li r11,1
	r11.s64 = 1;
	// stw r11,88(r28)
	PPC_STORE_U32(r28.u32 + 88, r11.u32);
	// b 0x821aec54
	goto loc_821AEC54;
loc_821AEA34:
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8496
	ctx.r9.s64 = ctx.r10.s64 + 8496;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AEA40:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821aea64
	if (cr0.getEQ()) goto loc_821AEA64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821aea40
	if (cr6.getEQ()) goto loc_821AEA40;
loc_821AEA64:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x821ae8ac
	if (cr0.getEQ()) goto loc_821AE8AC;
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r9,r10,8488
	ctx.r9.s64 = ctx.r10.s64 + 8488;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821AEA78:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821aea9c
	if (cr0.getEQ()) goto loc_821AEA9C;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821aea78
	if (cr6.getEQ()) goto loc_821AEA78;
loc_821AEA9C:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x821ae8ec
	if (cr0.getEQ()) goto loc_821AE8EC;
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// addi r10,r10,8504
	ctx.r10.s64 = ctx.r10.s64 + 8504;
loc_821AEAAC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821aead0
	if (cr0.getEQ()) goto loc_821AEAD0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821aeaac
	if (cr6.getEQ()) goto loc_821AEAAC;
loc_821AEAD0:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821aea28
	if (!cr0.getEQ()) goto loc_821AEA28;
	// lwz r11,628(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 628);
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821aeaf0
	if (!cr0.getEQ()) goto loc_821AEAF0;
	// stw r29,92(r28)
	PPC_STORE_U32(r28.u32 + 92, r29.u32);
	// b 0x821ae868
	goto loc_821AE868;
loc_821AEAF0:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi r10,0
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x821aeb10
	if (cr0.getEQ()) goto loc_821AEB10;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x821aeb10
	if (!cr6.getEQ()) goto loc_821AEB10;
	// stw r10,92(r28)
	PPC_STORE_U32(r28.u32 + 92, ctx.r10.u32);
	// b 0x821ae868
	goto loc_821AE868;
loc_821AEB10:
	// li r3,269
	ctx.r3.s64 = 269;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AEB18:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lwz r4,648(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 648);
	// addi r10,r11,8472
	ctx.r10.s64 = r11.s64 + 8472;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_821AEB28:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821aeb4c
	if (cr0.getEQ()) goto loc_821AEB4C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821aeb28
	if (cr6.getEQ()) goto loc_821AEB28;
loc_821AEB4C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821aeb60
	if (!cr0.getEQ()) goto loc_821AEB60;
	// li r3,271
	ctx.r3.s64 = 271;
	// stw r30,100(r28)
	PPC_STORE_U32(r28.u32 + 100, r30.u32);
	// b 0x821aed14
	goto loc_821AED14;
loc_821AEB60:
	// lwz r11,100(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 100);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821aec54
	if (cr6.getEQ()) goto loc_821AEC54;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x821adf00
	sub_821ADF00(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821aec54
	if (cr0.getEQ()) goto loc_821AEC54;
	// lwz r10,80(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 80);
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// ld r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 8);
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// ld r8,16(r31)
	ctx.r8.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ld r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// std r9,8(r11)
	PPC_STORE_U64(r11.u32 + 8, ctx.r9.u64);
	// std r8,16(r11)
	PPC_STORE_U64(r11.u32 + 16, ctx.r8.u64);
	// std r7,24(r11)
	PPC_STORE_U64(r11.u32 + 24, ctx.r7.u64);
	// bne cr6,0x821aebd8
	if (!cr6.getEQ()) goto loc_821AEBD8;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x821b15d8
	sub_821B15D8(ctx, base);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// mr r11,r30
	r11.u64 = r30.u64;
	// beq cr6,0x821aebdc
	if (cr6.getEQ()) goto loc_821AEBDC;
loc_821AEBD8:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_821AEBDC:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,80(r28)
	PPC_STORE_U32(r28.u32 + 80, r11.u32);
	// bne cr6,0x821aec00
	if (!cr6.getEQ()) goto loc_821AEC00;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x821ae548
	sub_821AE548(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x821aec04
	if (cr6.getEQ()) goto loc_821AEC04;
	// mr r11,r30
	r11.u64 = r30.u64;
	// b 0x821aec08
	goto loc_821AEC08;
loc_821AEC00:
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_821AEC04:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_821AEC08:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,80(r28)
	PPC_STORE_U32(r28.u32 + 80, r11.u32);
	// beq cr6,0x821aed14
	if (cr6.getEQ()) goto loc_821AED14;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// stw r29,84(r28)
	PPC_STORE_U32(r28.u32 + 84, r29.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// li r5,1518
	ctx.r5.s64 = 1518;
	// addi r6,r10,8420
	ctx.r6.s64 = ctx.r10.s64 + 8420;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// ld r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r3,r28,24
	ctx.r3.s64 = r28.s64 + 24;
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// ld r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// ld r11,24(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 24);
	// std r10,0(r31)
	PPC_STORE_U64(r31.u32 + 0, ctx.r10.u64);
	// std r9,8(r31)
	PPC_STORE_U64(r31.u32 + 8, ctx.r9.u64);
	// std r8,16(r31)
	PPC_STORE_U64(r31.u32 + 16, ctx.r8.u64);
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
loc_821AEC54:
	// li r3,278
	ctx.r3.s64 = 278;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AEC5C:
	// li r3,279
	ctx.r3.s64 = 279;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AEC64:
	// lbz r11,649(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 649);
	// extsb. r10,r11
	ctx.r10.s64 = r11.s8;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x821aec7c
	if (!cr0.getEQ()) goto loc_821AEC7C;
	// lbz r11,648(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 648);
	// extsb r3,r11
	ctx.r3.s64 = r11.s8;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AEC7C:
	// lbz r11,650(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 650);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821aed00
	if (!cr6.getEQ()) goto loc_821AED00;
	// lbz r11,648(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 648);
	// cmpwi cr6,r10,61
	cr6.compare<int32_t>(ctx.r10.s32, 61, xer);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// bne cr6,0x821aecd8
	if (!cr6.getEQ()) goto loc_821AECD8;
	// cmpwi cr6,r11,33
	cr6.compare<int32_t>(r11.s32, 33, xer);
	// beq cr6,0x821aecd0
	if (cr6.getEQ()) goto loc_821AECD0;
	// cmpwi cr6,r11,60
	cr6.compare<int32_t>(r11.s32, 60, xer);
	// beq cr6,0x821aecc8
	if (cr6.getEQ()) goto loc_821AECC8;
	// cmpwi cr6,r11,61
	cr6.compare<int32_t>(r11.s32, 61, xer);
	// beq cr6,0x821aecc0
	if (cr6.getEQ()) goto loc_821AECC0;
	// cmpwi cr6,r11,62
	cr6.compare<int32_t>(r11.s32, 62, xer);
	// bne cr6,0x821aed00
	if (!cr6.getEQ()) goto loc_821AED00;
	// li r3,273
	ctx.r3.s64 = 273;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AECC0:
	// li r3,274
	ctx.r3.s64 = 274;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AECC8:
	// li r3,272
	ctx.r3.s64 = 272;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AECD0:
	// li r3,275
	ctx.r3.s64 = 275;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AECD8:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bne cr6,0x821aed00
	if (!cr6.getEQ()) goto loc_821AED00;
	// cmpwi cr6,r11,38
	cr6.compare<int32_t>(r11.s32, 38, xer);
	// beq cr6,0x821aecf8
	if (cr6.getEQ()) goto loc_821AECF8;
	// cmpwi cr6,r11,124
	cr6.compare<int32_t>(r11.s32, 124, xer);
	// bne cr6,0x821aed00
	if (!cr6.getEQ()) goto loc_821AED00;
	// li r3,277
	ctx.r3.s64 = 277;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AECF8:
	// li r3,276
	ctx.r3.s64 = 276;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AED00:
	// li r3,281
	ctx.r3.s64 = 281;
	// b 0x821aed14
	goto loc_821AED14;
loc_821AED08:
	// li r11,12
	r11.s64 = 12;
	// stw r11,640(r28)
	PPC_STORE_U32(r28.u32 + 640, r11.u32);
loc_821AED10:
	// li r3,-1
	ctx.r3.s64 = -1;
loc_821AED14:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821AED20"))) PPC_WEAK_FUNC(sub_821AED20);
PPC_FUNC_IMPL(__imp__sub_821AED20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// bl 0x821adb20
	sub_821ADB20(ctx, base);
	// addi r11,r3,30
	r11.s64 = ctx.r3.s64 + 30;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = r11.u64 + ctx.r9.u64;
	// b 0x821aed8c
	goto loc_821AED8C;
loc_821AED48:
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
loc_821AED54:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// beq 0x821aed78
	if (cr0.getEQ()) goto loc_821AED78;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821aed54
	if (cr6.getEQ()) goto loc_821AED54;
loc_821AED78:
	// cmpwi r7,0
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// blt 0x821aedb8
	if (cr0.getLT()) goto loc_821AEDB8;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x821aed9c
	if (cr6.getEQ()) goto loc_821AED9C;
	// addi r9,r6,12
	ctx.r9.s64 = ctx.r6.s64 + 12;
loc_821AED8C:
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821aed48
	if (!cr6.getEQ()) goto loc_821AED48;
	// b 0x821aedb8
	goto loc_821AEDB8;
loc_821AED9C:
	// lwz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// bl 0x821ae0e8
	sub_821AE0E8(ctx, base);
loc_821AEDB8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AEDD0"))) PPC_WEAK_FUNC(sub_821AEDD0);
PPC_FUNC_IMPL(__imp__sub_821AEDD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821aee20
	if (cr0.getEQ()) goto loc_821AEE20;
	// lwz r10,92(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stw r9,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// b 0x821aee24
	goto loc_821AEE24;
loc_821AEE20:
	// li r11,0
	r11.s64 = 0;
loc_821AEE24:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821aee38
	if (!cr6.getEQ()) goto loc_821AEE38;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x821aee70
	goto loc_821AEE70;
loc_821AEE38:
	// lwz r10,628(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 628);
	// lwz r10,56(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// lwz r10,628(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 628);
	// stw r11,56(r10)
	PPC_STORE_U32(ctx.r10.u32 + 56, r11.u32);
	// lwz r11,92(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821aee64
	if (cr6.getEQ()) goto loc_821AEE64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x821aee68
	if (!cr6.getEQ()) goto loc_821AEE68;
loc_821AEE64:
	// li r11,0
	r11.s64 = 0;
loc_821AEE68:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
loc_821AEE70:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AEE88"))) PPC_WEAK_FUNC(sub_821AEE88);
PPC_FUNC_IMPL(__imp__sub_821AEE88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,628(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 628);
	// lwz r3,56(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x821aeed8
	if (!cr0.getEQ()) goto loc_821AEED8;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,1510
	ctx.r5.s64 = 1510;
	// addi r6,r11,8564
	ctx.r6.s64 = r11.s64 + 8564;
	// addi r4,r31,640
	ctx.r4.s64 = r31.s64 + 640;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// b 0x821aeefc
	goto loc_821AEEFC;
loc_821AEED8:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r10,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r10.u32);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// stw r10,56(r11)
	PPC_STORE_U32(r11.u32 + 56, ctx.r10.u32);
	// stw r9,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
	// bl 0x821ae0e8
	sub_821AE0E8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
loc_821AEEFC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AEF10"))) PPC_WEAK_FUNC(sub_821AEF10);
PPC_FUNC_IMPL(__imp__sub_821AEF10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r31,640
	r30.s64 = r31.s64 + 640;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,672(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 672);
	// lwz r3,632(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821aefec
	if (cr0.getLT()) goto loc_821AEFEC;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x821aef58
	if (cr6.getEQ()) goto loc_821AEF58;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// bne cr6,0x821aefcc
	if (!cr6.getEQ()) goto loc_821AEFCC;
loc_821AEF58:
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821aef84
	if (cr0.getEQ()) goto loc_821AEF84;
	// lwz r10,68(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 68);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r9,648(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// b 0x821aef88
	goto loc_821AEF88;
loc_821AEF84:
	// li r11,0
	r11.s64 = 0;
loc_821AEF88:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r11.u32);
	// bne cr6,0x821aefa0
	if (!cr6.getEQ()) goto loc_821AEFA0;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x821aefec
	goto loc_821AEFEC;
loc_821AEFA0:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,672(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 672);
	// lwz r3,632(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821aefec
	if (cr0.getLT()) goto loc_821AEFEC;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// beq cr6,0x821aefe8
	if (cr6.getEQ()) goto loc_821AEFE8;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// beq cr6,0x821aefe8
	if (cr6.getEQ()) goto loc_821AEFE8;
loc_821AEFCC:
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// beq cr6,0x821aefe8
	if (cr6.getEQ()) goto loc_821AEFE8;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// beq cr6,0x821aefe8
	if (cr6.getEQ()) goto loc_821AEFE8;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,632(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// bl 0x821b8768
	sub_821B8768(ctx, base);
loc_821AEFE8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821AEFEC:
	// li r11,1
	r11.s64 = 1;
	// stw r11,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AF010"))) PPC_WEAK_FUNC(sub_821AF010);
PPC_FUNC_IMPL(__imp__sub_821AF010) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed120
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r27,r29,640
	r27.s64 = r29.s64 + 640;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821af3dc
	if (cr0.getLT()) goto loc_821AF3DC;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821af3b8
	if (!cr6.getEQ()) goto loc_821AF3B8;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r31,r29,648
	r31.s64 = r29.s64 + 648;
	// addi r10,r11,25744
	ctx.r10.s64 = r11.s64 + 25744;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_821AF058:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af07c
	if (cr0.getEQ()) goto loc_821AF07C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af058
	if (cr6.getEQ()) goto loc_821AF058;
loc_821AF07C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821af3b8
	if (!cr0.getEQ()) goto loc_821AF3B8;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821af3dc
	if (cr0.getLT()) goto loc_821AF3DC;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x821af3b8
	if (!cr6.getEQ()) goto loc_821AF3B8;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r22,0(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821af3dc
	if (cr0.getLT()) goto loc_821AF3DC;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821af3b8
	if (!cr6.getEQ()) goto loc_821AF3B8;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r24,r11,8588
	r24.s64 = r11.s64 + 8588;
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_821AF0E0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af104
	if (cr0.getEQ()) goto loc_821AF104;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af0e0
	if (cr6.getEQ()) goto loc_821AF0E0;
loc_821AF104:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821af3b8
	if (!cr0.getEQ()) goto loc_821AF3B8;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821af3dc
	if (cr0.getLT()) goto loc_821AF3DC;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x821af3b8
	if (!cr6.getEQ()) goto loc_821AF3B8;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lwz r23,0(r31)
	r23.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r26,0
	r26.s64 = 0;
	// addi r28,r1,96
	r28.s64 = ctx.r1.s64 + 96;
	// addi r25,r11,8584
	r25.s64 = r11.s64 + 8584;
loc_821AF144:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// li r30,0
	r30.s64 = 0;
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821af3dc
	if (cr0.getLT()) goto loc_821AF3DC;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821af3b8
	if (!cr6.getEQ()) goto loc_821AF3B8;
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_821AF174:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af198
	if (cr0.getEQ()) goto loc_821AF198;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af174
	if (cr6.getEQ()) goto loc_821AF174;
loc_821AF198:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821af3b8
	if (!cr0.getEQ()) goto loc_821AF3B8;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821af3dc
	if (cr0.getLT()) goto loc_821AF3DC;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821af214
	if (!cr6.getEQ()) goto loc_821AF214;
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_821AF1CC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af1f0
	if (cr0.getEQ()) goto loc_821AF1F0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af1cc
	if (cr6.getEQ()) goto loc_821AF1CC;
loc_821AF1F0:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821af214
	if (!cr0.getEQ()) goto loc_821AF214;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// li r30,1
	r30.s64 = 1;
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821af3dc
	if (cr0.getLT()) goto loc_821AF3DC;
loc_821AF214:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x821af254
	if (cr6.getEQ()) goto loc_821AF254;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x821af244
	if (cr6.getEQ()) goto loc_821AF244;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x821af254
	if (cr6.getEQ()) goto loc_821AF254;
	// addi r11,r11,-5
	r11.s64 = r11.s64 + -5;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bgt cr6,0x821af3b8
	if (cr6.getGT()) goto loc_821AF3B8;
	// lfd f0,0(r31)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// b 0x821af264
	goto loc_821AF264;
loc_821AF244:
	// lwa r11,0(r31)
	r11.s64 = int32_t(PPC_LOAD_U32(r31.u32 + 0));
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// b 0x821af260
	goto loc_821AF260;
loc_821AF254:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
loc_821AF260:
	// fcfid f0,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(f0.s64);
loc_821AF264:
	// stfd f0,0(r28)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r28.u32 + 0, f0.u64);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// beq cr6,0x821af278
	if (cr6.getEQ()) goto loc_821AF278;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
	// stfd f0,0(r28)
	PPC_STORE_U64(r28.u32 + 0, f0.u64);
loc_821AF278:
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// cmplwi cr6,r26,4
	cr6.compare<uint32_t>(r26.u32, 4, xer);
	// blt cr6,0x821af144
	if (cr6.getLT()) goto loc_821AF144;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821af3dc
	if (cr0.getLT()) goto loc_821AF3DC;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821af3b8
	if (!cr6.getEQ()) goto loc_821AF3B8;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r11,8340
	ctx.r10.s64 = r11.s64 + 8340;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_821AF2B8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af2dc
	if (cr0.getEQ()) goto loc_821AF2DC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af2b8
	if (cr6.getEQ()) goto loc_821AF2B8;
loc_821AF2DC:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821af3b8
	if (!cr0.getEQ()) goto loc_821AF3B8;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r4,672(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 672);
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821af3dc
	if (cr0.getLT()) goto loc_821AF3DC;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// beq cr6,0x821af310
	if (cr6.getEQ()) goto loc_821AF310;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// bne cr6,0x821af3b8
	if (!cr6.getEQ()) goto loc_821AF3B8;
loc_821AF310:
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821af334
	if (cr0.getEQ()) goto loc_821AF334;
	// li r11,0
	r11.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, r11.u32);
	// b 0x821af338
	goto loc_821AF338;
loc_821AF334:
	// li r31,0
	r31.s64 = 0;
loc_821AF338:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x821af34c
	if (!cr6.getEQ()) goto loc_821AF34C;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x821af3dc
	goto loc_821AF3DC;
loc_821AF34C:
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// stw r22,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r22.u32);
	// addi r30,r29,64
	r30.s64 = r29.s64 + 64;
	// stw r23,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r23.u32);
	// ld r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// ld r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// ld r11,24(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 24);
	// std r10,8(r31)
	PPC_STORE_U64(r31.u32 + 8, ctx.r10.u64);
	// std r9,16(r31)
	PPC_STORE_U64(r31.u32 + 16, ctx.r9.u64);
	// std r8,24(r31)
	PPC_STORE_U64(r31.u32 + 24, ctx.r8.u64);
	// std r11,32(r31)
	PPC_STORE_U64(r31.u32 + 32, r11.u64);
	// b 0x821af39c
	goto loc_821AF39C;
loc_821AF380:
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x823f07a0
	sub_823F07A0(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x821af3a8
	if (!cr0.getLT()) goto loc_821AF3A8;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r30,r11,40
	r30.s64 = r11.s64 + 40;
loc_821AF39C:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x821af380
	if (!cr0.getEQ()) goto loc_821AF380;
loc_821AF3A8:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// stw r31,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r31.u32);
	// b 0x821af3d8
	goto loc_821AF3D8;
loc_821AF3B8:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// beq cr6,0x821af3d8
	if (cr6.getEQ()) goto loc_821AF3D8;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// beq cr6,0x821af3d8
	if (cr6.getEQ()) goto loc_821AF3D8;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,632(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 632);
	// bl 0x821b8768
	sub_821B8768(ctx, base);
loc_821AF3D8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821AF3DC:
	// li r11,1
	r11.s64 = 1;
	// stw r11,76(r29)
	PPC_STORE_U32(r29.u32 + 76, r11.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x823ed170
	return;
}

__attribute__((alias("__imp__sub_821AF3F0"))) PPC_WEAK_FUNC(sub_821AF3F0);
PPC_FUNC_IMPL(__imp__sub_821AF3F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r29,0(r28)
	r29.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x821adb20
	sub_821ADB20(ctx, base);
	// addi r11,r3,30
	r11.s64 = ctx.r3.s64 + 30;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r30
	r31.u64 = r11.u64 + r30.u64;
	// b 0x821af464
	goto loc_821AF464;
loc_821AF420:
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r11,r29
	r11.u64 = r29.u64;
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
loc_821AF42C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af450
	if (cr0.getEQ()) goto loc_821AF450;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af42c
	if (cr6.getEQ()) goto loc_821AF42C;
loc_821AF450:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// blt 0x821af4c4
	if (cr0.getLT()) goto loc_821AF4C4;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af474
	if (cr6.getEQ()) goto loc_821AF474;
	// addi r31,r7,12
	r31.s64 = ctx.r7.s64 + 12;
loc_821AF464:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821af420
	if (!cr6.getEQ()) goto loc_821AF420;
	// b 0x821af4c4
	goto loc_821AF4C4;
loc_821AF474:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821adb70
	sub_821ADB70(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x821af4a8
	if (!cr0.getEQ()) goto loc_821AF4A8;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// addi r6,r11,8592
	ctx.r6.s64 = r11.s64 + 8592;
	// li r5,1519
	ctx.r5.s64 = 1519;
	// addi r4,r30,640
	ctx.r4.s64 = r30.s64 + 640;
	// addi r3,r30,24
	ctx.r3.s64 = r30.s64 + 24;
	// bl 0x821b8300
	sub_821B8300(ctx, base);
loc_821AF4A8:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// bl 0x821ae0e8
	sub_821AE0E8(ctx, base);
loc_821AF4C4:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,12(r28)
	PPC_STORE_U32(r28.u32 + 12, r11.u32);
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821AF4E0"))) PPC_WEAK_FUNC(sub_821AF4E0);
PPC_FUNC_IMPL(__imp__sub_821AF4E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821af530
	if (cr0.getEQ()) goto loc_821AF530;
	// li r11,0
	r11.s64 = 0;
	// stw r31,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r31.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// stw r11,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r11.u32);
	// b 0x821af534
	goto loc_821AF534;
loc_821AF530:
	// li r4,0
	ctx.r4.s64 = 0;
loc_821AF534:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AF558"))) PPC_WEAK_FUNC(sub_821AF558);
PPC_FUNC_IMPL(__imp__sub_821AF558) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// li r31,0
	r31.s64 = 0;
	// beq 0x821af5a4
	if (cr0.getEQ()) goto loc_821AF5A4;
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r31,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r31.u32);
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// stw r31,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r31.u32);
	// stw r31,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r31.u32);
	// b 0x821af5a8
	goto loc_821AF5A8;
loc_821AF5A4:
	// mr r30,r31
	r30.u64 = r31.u64;
loc_821AF5A8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x821af5bc
	if (!cr6.getEQ()) goto loc_821AF5BC;
loc_821AF5B0:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x821af610
	goto loc_821AF610;
loc_821AF5BC:
	// li r11,2
	r11.s64 = 2;
	// stw r29,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r29.u32);
	// li r3,48
	ctx.r3.s64 = 48;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x821d5a70
	sub_821D5A70(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821af5e4
	if (cr0.getEQ()) goto loc_821AF5E4;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x821d6090
	sub_821D6090(ctx, base);
	// b 0x821af5e8
	goto loc_821AF5E8;
loc_821AF5E4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_821AF5E8:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r3.u32);
	// bne cr6,0x821af604
	if (!cr6.getEQ()) goto loc_821AF604;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821ae0e8
	sub_821AE0E8(ctx, base);
	// b 0x821af5b0
	goto loc_821AF5B0;
loc_821AF604:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
loc_821AF610:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_821AF618"))) PPC_WEAK_FUNC(sub_821AF618);
PPC_FUNC_IMPL(__imp__sub_821AF618) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r11,r11,8692
	r11.s64 = r11.s64 + 8692;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
loc_821AF638:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af65c
	if (cr0.getEQ()) goto loc_821AF65C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af638
	if (cr6.getEQ()) goto loc_821AF638;
loc_821AF65C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821af66c
	if (!cr0.getEQ()) goto loc_821AF66C;
	// lwz r3,660(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 660);
	// b 0x821af6fc
	goto loc_821AF6FC;
loc_821AF66C:
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821adf00
	sub_821ADF00(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x821af6f8
	if (cr0.getEQ()) goto loc_821AF6F8;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821af6b0
	if (cr6.getEQ()) goto loc_821AF6B0;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,1517
	ctx.r5.s64 = 1517;
	// addi r6,r11,8624
	ctx.r6.s64 = r11.s64 + 8624;
	// addi r4,r31,640
	ctx.r4.s64 = r31.s64 + 640;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x821af6fc
	goto loc_821AF6FC;
loc_821AF6B0:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821af6e0
	if (cr6.getEQ()) goto loc_821AF6E0;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x821af6e0
	if (!cr6.getEQ()) goto loc_821AF6E0;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
	// cmplwi cr6,r10,2
	cr6.compare<uint32_t>(ctx.r10.u32, 2, xer);
	// bgt cr6,0x821af6e0
	if (cr6.getGT()) goto loc_821AF6E0;
	// lwz r3,24(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// b 0x821af6fc
	goto loc_821AF6FC;
loc_821AF6E0:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,1518
	ctx.r5.s64 = 1518;
	// addi r6,r11,8420
	ctx.r6.s64 = r11.s64 + 8420;
	// addi r4,r31,640
	ctx.r4.s64 = r31.s64 + 640;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
loc_821AF6F8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821AF6FC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AF710"))) PPC_WEAK_FUNC(sub_821AF710);
PPC_FUNC_IMPL(__imp__sub_821AF710) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,1
	r11.s64 = 1;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// lwz r10,92(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 92);
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x821af7e4
	if (cr6.getEQ()) goto loc_821AF7E4;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r11,8744
	ctx.r10.s64 = r11.s64 + 8744;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_821AF748:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af76c
	if (cr0.getEQ()) goto loc_821AF76C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af748
	if (cr6.getEQ()) goto loc_821AF748;
loc_821AF76C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821af7b8
	if (!cr0.getEQ()) goto loc_821AF7B8;
	// lwz r11,88(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x821af7a4
	if (cr6.getEQ()) goto loc_821AF7A4;
	// lwz r11,640(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 640);
	// addi r4,r31,640
	ctx.r4.s64 = r31.s64 + 640;
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x821af7a4
	if (!cr6.getEQ()) goto loc_821AF7A4;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lwz r7,648(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 648);
	// li r5,1504
	ctx.r5.s64 = 1504;
	// addi r6,r11,8708
	ctx.r6.s64 = r11.s64 + 8708;
	// b 0x821af7dc
	goto loc_821AF7DC;
loc_821AF7A4:
	// addi r5,r31,640
	ctx.r5.s64 = r31.s64 + 640;
	// li r4,1500
	ctx.r4.s64 = 1500;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b85d0
	sub_821B85D0(ctx, base);
	// b 0x821af7e4
	goto loc_821AF7E4;
loc_821AF7B8:
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// li r4,256
	ctx.r4.s64 = 256;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821abbb8
	sub_821ABBB8(ctx, base);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r11,8704
	ctx.r6.s64 = r11.s64 + 8704;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r31,640
	ctx.r4.s64 = r31.s64 + 640;
loc_821AF7DC:
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
loc_821AF7E4:
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AF7F8"))) PPC_WEAK_FUNC(sub_821AF7F8);
PPC_FUNC_IMPL(__imp__sub_821AF7F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed124
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// li r24,0
	r24.s64 = 0;
	// beq 0x821af844
	if (cr0.getEQ()) goto loc_821AF844;
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// stw r24,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r24.u32);
	// stw r24,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r24.u32);
	// stw r24,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r24.u32);
	// stw r24,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r24.u32);
	// b 0x821af848
	goto loc_821AF848;
loc_821AF844:
	// mr r23,r24
	r23.u64 = r24.u64;
loc_821AF848:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x821afb20
	if (cr6.getEQ()) goto loc_821AFB20;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x821afa34
	if (cr6.getEQ()) goto loc_821AFA34;
	// lwz r3,632(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x821afa34
	if (!cr6.getLT()) goto loc_821AFA34;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r11,40
	cr6.compare<uint32_t>(r11.u32, 40, xer);
	// bne cr6,0x821afa34
	if (!cr6.getEQ()) goto loc_821AFA34;
	// addi r28,r31,640
	r28.s64 = r31.s64 + 640;
	// lwz r4,672(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 672);
	// addi r27,r23,4
	r27.s64 = r23.s64 + 4;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r29,r27
	r29.u64 = r27.u64;
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821afb50
	if (cr0.getLT()) goto loc_821AFB50;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r26,r11,8588
	r26.s64 = r11.s64 + 8588;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r25,r11,8760
	r25.s64 = r11.s64 + 8760;
loc_821AF8AC:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r4,672(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 672);
	// lwz r3,632(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821afb50
	if (cr0.getLT()) goto loc_821AFB50;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmpwi cr6,r11,9
	cr6.compare<int32_t>(r11.s32, 9, xer);
	// bne cr6,0x821af9f8
	if (!cr6.getEQ()) goto loc_821AF9F8;
	// lwz r6,0(r27)
	ctx.r6.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq 0x821af938
	if (cr0.getEQ()) goto loc_821AF938;
	// lwz r7,648(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 648);
loc_821AF8E0:
	// lwz r10,24(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 24);
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_821AF8E8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af90c
	if (cr0.getEQ()) goto loc_821AF90C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af8e8
	if (cr6.getEQ()) goto loc_821AF8E8;
loc_821AF90C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821af924
	if (cr0.getEQ()) goto loc_821AF924;
	// lwz r6,12(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne 0x821af8e0
	if (!cr0.getEQ()) goto loc_821AF8E0;
	// b 0x821af938
	goto loc_821AF938;
loc_821AF924:
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// li r5,1511
	ctx.r5.s64 = 1511;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b80c8
	sub_821B80C8(ctx, base);
loc_821AF938:
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x821d5a70
	sub_821D5A70(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821af954
	if (cr0.getEQ()) goto loc_821AF954;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x821d6090
	sub_821D6090(ctx, base);
	// b 0x821af958
	goto loc_821AF958;
loc_821AF954:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
loc_821AF958:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// beq cr6,0x821afb20
	if (cr6.getEQ()) goto loc_821AFB20;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r4,672(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 672);
	// addi r29,r3,12
	r29.s64 = ctx.r3.s64 + 12;
	// lwz r3,632(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821afb50
	if (cr0.getLT()) goto loc_821AFB50;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x821af9f8
	if (!cr6.getEQ()) goto loc_821AF9F8;
	// addi r10,r31,648
	ctx.r10.s64 = r31.s64 + 648;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_821AF994:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af9b8
	if (cr0.getEQ()) goto loc_821AF9B8;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af994
	if (cr6.getEQ()) goto loc_821AF994;
loc_821AF9B8:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821af8ac
	if (cr0.getEQ()) goto loc_821AF8AC;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r31,648
	ctx.r10.s64 = r31.s64 + 648;
	// addi r11,r11,8340
	r11.s64 = r11.s64 + 8340;
loc_821AF9CC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821af9f0
	if (cr0.getEQ()) goto loc_821AF9F0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821af9cc
	if (cr6.getEQ()) goto loc_821AF9CC;
loc_821AF9F0:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x821afa34
	if (cr0.getEQ()) goto loc_821AFA34;
loc_821AF9F8:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r29,1
	r29.s64 = 1;
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// beq cr6,0x821afa10
	if (cr6.getEQ()) goto loc_821AFA10;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// bne cr6,0x821afa14
	if (!cr6.getEQ()) goto loc_821AFA14;
loc_821AFA10:
	// stw r29,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r29.u32);
loc_821AFA14:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,1500
	ctx.r4.s64 = 1500;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x821b85d0
	sub_821B85D0(ctx, base);
	// lis r30,-32768
	r30.s64 = -2147483648;
	// stw r29,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r29.u32);
	// ori r30,r30,16389
	r30.u64 = r30.u64 | 16389;
	// b 0x821afb50
	goto loc_821AFB50;
loc_821AFA34:
	// addi r28,r23,8
	r28.s64 = r23.s64 + 8;
loc_821AFA38:
	// lwz r10,112(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// addi r29,r31,640
	r29.s64 = r31.s64 + 640;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x821afa90
	if (cr0.getEQ()) goto loc_821AFA90;
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r9,0(r29)
	PPC_STORE_U64(r29.u32 + 0, ctx.r9.u64);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// std r9,8(r29)
	PPC_STORE_U64(r29.u32 + 8, ctx.r9.u64);
	// ld r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// std r9,16(r29)
	PPC_STORE_U64(r29.u32 + 16, ctx.r9.u64);
	// ld r11,24(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 24);
	// std r11,24(r29)
	PPC_STORE_U64(r29.u32 + 24, r11.u64);
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// stw r11,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r11.u32);
	// stw r24,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, r24.u32);
	// lwz r11,632(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// stw r10,656(r31)
	PPC_STORE_U32(r31.u32 + 656, ctx.r10.u32);
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// stw r11,660(r31)
	PPC_STORE_U32(r31.u32 + 660, r11.u32);
	// b 0x821afaa8
	goto loc_821AFAA8;
loc_821AFA90:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r4,672(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 672);
	// lwz r3,632(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821afb50
	if (cr0.getLT()) goto loc_821AFB50;
loc_821AFAA8:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// beq cr6,0x821afb2c
	if (cr6.getEQ()) goto loc_821AFB2C;
	// cmpwi cr6,r11,13
	cr6.compare<int32_t>(r11.s32, 13, xer);
	// beq cr6,0x821afb2c
	if (cr6.getEQ()) goto loc_821AFB2C;
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x821d5a70
	sub_821D5A70(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821afadc
	if (cr0.getEQ()) goto loc_821AFADC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x821d6090
	sub_821D6090(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// b 0x821afae0
	goto loc_821AFAE0;
loc_821AFADC:
	// mr r30,r24
	r30.u64 = r24.u64;
loc_821AFAE0:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x821afb20
	if (cr6.getEQ()) goto loc_821AFB20;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,668(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 668);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821d5928
	sub_821D5928(ctx, base);
	// mr. r29,r3
	r29.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// beq 0x821afb20
	if (cr0.getEQ()) goto loc_821AFB20;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,668(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 668);
	// lwz r4,664(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 664);
	// bl 0x823ee010
	sub_823EE010(ctx, base);
	// stw r29,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r29.u32);
	// stw r30,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r30.u32);
	// addi r28,r30,12
	r28.s64 = r30.s64 + 12;
	// b 0x821afa38
	goto loc_821AFA38;
loc_821AFB20:
	// lis r30,-32761
	r30.s64 = -2147024896;
	// ori r30,r30,14
	r30.u64 = r30.u64 | 14;
	// b 0x821afb50
	goto loc_821AFB50;
loc_821AFB2C:
	// li r11,1
	r11.s64 = 1;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r11.u32);
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821afb50
	if (cr0.getLT()) goto loc_821AFB50;
	// mr r23,r24
	r23.u64 = r24.u64;
	// mr r30,r24
	r30.u64 = r24.u64;
loc_821AFB50:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x821afb64
	if (cr6.getEQ()) goto loc_821AFB64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x821ae0e8
	sub_821AE0E8(ctx, base);
loc_821AFB64:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed174
	return;
}

__attribute__((alias("__imp__sub_821AFB70"))) PPC_WEAK_FUNC(sub_821AFB70);
PPC_FUNC_IMPL(__imp__sub_821AFB70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r31,r30,640
	r31.s64 = r30.s64 + 640;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r4,672(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 672);
	// lwz r3,632(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 632);
	// bl 0x821b9380
	sub_821B9380(ctx, base);
	// li r29,1
	r29.s64 = 1;
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x821afce4
	if (cr0.getLT()) goto loc_821AFCE4;
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r6,9
	cr6.compare<int32_t>(ctx.r6.s32, 9, xer);
	// bne cr6,0x821afcc4
	if (!cr6.getEQ()) goto loc_821AFCC4;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lwz r7,648(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 648);
	// stw r29,72(r30)
	PPC_STORE_U32(r30.u32 + 72, r29.u32);
	// addi r10,r11,8816
	ctx.r10.s64 = r11.s64 + 8816;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_821AFBC0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821afbe4
	if (cr0.getEQ()) goto loc_821AFBE4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821afbc0
	if (cr6.getEQ()) goto loc_821AFBC0;
loc_821AFBE4:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821afbf8
	if (!cr0.getEQ()) goto loc_821AFBF8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821ad3a0
	sub_821AD3A0(ctx, base);
	// b 0x821afce8
	goto loc_821AFCE8;
loc_821AFBF8:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r11,8808
	ctx.r10.s64 = r11.s64 + 8808;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_821AFC04:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821afc28
	if (cr0.getEQ()) goto loc_821AFC28;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821afc04
	if (cr6.getEQ()) goto loc_821AFC04;
loc_821AFC28:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821afc3c
	if (!cr0.getEQ()) goto loc_821AFC3C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821ad598
	sub_821AD598(ctx, base);
	// b 0x821afce8
	goto loc_821AFCE8;
loc_821AFC3C:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r11,8804
	ctx.r10.s64 = r11.s64 + 8804;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_821AFC48:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821afc6c
	if (cr0.getEQ()) goto loc_821AFC6C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821afc48
	if (cr6.getEQ()) goto loc_821AFC48;
loc_821AFC6C:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821afc80
	if (!cr0.getEQ()) goto loc_821AFC80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821af010
	sub_821AF010(ctx, base);
	// b 0x821afce8
	goto loc_821AFCE8;
loc_821AFC80:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// addi r10,r11,8792
	ctx.r10.s64 = r11.s64 + 8792;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
loc_821AFC8C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq 0x821afcb0
	if (cr0.getEQ()) goto loc_821AFCB0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x821afc8c
	if (cr6.getEQ()) goto loc_821AFC8C;
loc_821AFCB0:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x821afcc4
	if (!cr0.getEQ()) goto loc_821AFCC4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821aef10
	sub_821AEF10(ctx, base);
	// b 0x821afce8
	goto loc_821AFCE8;
loc_821AFCC4:
	// cmpwi cr6,r6,12
	cr6.compare<int32_t>(ctx.r6.s32, 12, xer);
	// beq cr6,0x821afce0
	if (cr6.getEQ()) goto loc_821AFCE0;
	// cmpwi cr6,r6,13
	cr6.compare<int32_t>(ctx.r6.s32, 13, xer);
	// beq cr6,0x821afce0
	if (cr6.getEQ()) goto loc_821AFCE0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,632(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 632);
	// bl 0x821b8768
	sub_821B8768(ctx, base);
loc_821AFCE0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_821AFCE4:
	// stw r29,76(r30)
	PPC_STORE_U32(r30.u32 + 76, r29.u32);
loc_821AFCE8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_821AFCF0"))) PPC_WEAK_FUNC(sub_821AFCF0);
PPC_FUNC_IMPL(__imp__sub_821AFCF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// lwz r3,3032(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3032);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x821af710
	sub_821AF710(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_821AFD40"))) PPC_WEAK_FUNC(sub_821AFD40);
PPC_FUNC_IMPL(__imp__sub_821AFD40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed12c
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// li r31,0
	r31.s64 = 0;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// std r31,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r31.u64);
	// std r31,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r31.u64);
	// std r31,16(r11)
	PPC_STORE_U64(r11.u32 + 16, r31.u64);
	// std r31,24(r11)
	PPC_STORE_U64(r11.u32 + 24, r31.u64);
	// lwz r25,632(r27)
	r25.u64 = PPC_LOAD_U32(r27.u32 + 632);
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821afda8
	if (cr0.getEQ()) goto loc_821AFDA8;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r31,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r31.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// addi r11,r11,8932
	r11.s64 = r11.s64 + 8932;
	// stw r31,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r31.u32);
	// stw r31,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r31.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x821afdac
	goto loc_821AFDAC;
loc_821AFDA8:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_821AFDAC:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821b006c
	if (cr6.getEQ()) goto loc_821B006C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b0178
	if (cr0.getLT()) goto loc_821B0178;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821afe00
	if (cr0.getEQ()) goto loc_821AFE00;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r31,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r31.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// addi r11,r11,8924
	r11.s64 = r11.s64 + 8924;
	// stw r31,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r31.u32);
	// stw r31,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r31.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x821afe04
	goto loc_821AFE04;
loc_821AFE00:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_821AFE04:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821b006c
	if (cr6.getEQ()) goto loc_821B006C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b0178
	if (cr0.getLT()) goto loc_821B0178;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821afe58
	if (cr0.getEQ()) goto loc_821AFE58;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r31,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r31.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// addi r11,r11,8908
	r11.s64 = r11.s64 + 8908;
	// stw r31,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r31.u32);
	// stw r31,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r31.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x821afe5c
	goto loc_821AFE5C;
loc_821AFE58:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_821AFE5C:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821b006c
	if (cr6.getEQ()) goto loc_821B006C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b0178
	if (cr0.getLT()) goto loc_821B0178;
	// li r28,2
	r28.s64 = 2;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// stw r28,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r28.u32);
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821afebc
	if (cr0.getEQ()) goto loc_821AFEBC;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r31,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r31.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// addi r11,r11,8892
	r11.s64 = r11.s64 + 8892;
	// stw r31,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r31.u32);
	// stw r31,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r31.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x821afec0
	goto loc_821AFEC0;
loc_821AFEBC:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_821AFEC0:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821b006c
	if (cr6.getEQ()) goto loc_821B006C;
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x821d5a70
	sub_821D5A70(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821afee4
	if (cr0.getEQ()) goto loc_821AFEE4;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x821d6090
	sub_821D6090(ctx, base);
	// b 0x821afee8
	goto loc_821AFEE8;
loc_821AFEE4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_821AFEE8:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r3.u32);
	// beq cr6,0x821b006c
	if (cr6.getEQ()) goto loc_821B006C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b0178
	if (cr0.getLT()) goto loc_821B0178;
	// li r11,2560
	r11.s64 = 2560;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821aff4c
	if (cr0.getEQ()) goto loc_821AFF4C;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r31,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r31.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// addi r11,r11,8872
	r11.s64 = r11.s64 + 8872;
	// stw r31,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r31.u32);
	// stw r31,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r31.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x821aff50
	goto loc_821AFF50;
loc_821AFF4C:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_821AFF50:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821b006c
	if (cr6.getEQ()) goto loc_821B006C;
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x821d5a70
	sub_821D5A70(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821aff74
	if (cr0.getEQ()) goto loc_821AFF74;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x821d6090
	sub_821D6090(ctx, base);
	// b 0x821aff78
	goto loc_821AFF78;
loc_821AFF74:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_821AFF78:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r3.u32);
	// beq cr6,0x821b006c
	if (cr6.getEQ()) goto loc_821B006C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b0178
	if (cr0.getLT()) goto loc_821B0178;
	// li r11,2589
	r11.s64 = 2589;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821affdc
	if (cr0.getEQ()) goto loc_821AFFDC;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r31,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r31.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// addi r11,r11,8856
	r11.s64 = r11.s64 + 8856;
	// stw r31,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r31.u32);
	// stw r31,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r31.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x821affe0
	goto loc_821AFFE0;
loc_821AFFDC:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_821AFFE0:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x821b006c
	if (cr6.getEQ()) goto loc_821B006C;
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x821d5a70
	sub_821D5A70(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821b0004
	if (cr0.getEQ()) goto loc_821B0004;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x821d6090
	sub_821D6090(ctx, base);
	// b 0x821b0008
	goto loc_821B0008;
loc_821B0004:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_821B0008:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r3.u32);
	// beq cr6,0x821b006c
	if (cr6.getEQ()) goto loc_821B006C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b0178
	if (cr0.getLT()) goto loc_821B0178;
	// lis r4,9345
	ctx.r4.s64 = 612433920;
	// li r3,20
	ctx.r3.s64 = 20;
	// bl 0x8209d000
	sub_8209D000(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x821b0060
	if (cr0.getEQ()) goto loc_821B0060;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// stw r31,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r31.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// addi r11,r11,8848
	r11.s64 = r11.s64 + 8848;
	// stw r31,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r31.u32);
	// stw r31,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r31.u32);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// b 0x821b0064
	goto loc_821B0064;
loc_821B0060:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_821B0064:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x821b0078
	if (!cr6.getEQ()) goto loc_821B0078;
loc_821B006C:
	// lis r30,-32761
	r30.s64 = -2147024896;
	// ori r30,r30,14
	r30.u64 = r30.u64 | 14;
	// b 0x821b0178
	goto loc_821B0178;
loc_821B0078:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821af3f0
	sub_821AF3F0(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b0178
	if (cr0.getLT()) goto loc_821B0178;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r4,r11,8840
	ctx.r4.s64 = r11.s64 + 8840;
	// mr r29,r31
	r29.u64 = r31.u64;
	// bl 0x821af4e0
	sub_821AF4E0(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b0178
	if (cr0.getLT()) goto loc_821B0178;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// li r5,200
	ctx.r5.s64 = 200;
	// addi r4,r11,8828
	ctx.r4.s64 = r11.s64 + 8828;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821af558
	sub_821AF558(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b0178
	if (cr0.getLT()) goto loc_821B0178;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x821b0174
	if (cr6.getEQ()) goto loc_821B0174;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821b7558
	sub_821B7558(ctx, base);
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// stw r11,632(r27)
	PPC_STORE_U32(r27.u32 + 632, r11.u32);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x821b016c
	if (cr6.getEQ()) goto loc_821B016C;
	// addi r28,r27,24
	r28.s64 = r27.s64 + 24;
loc_821B00EC:
	// lwz r4,4(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplwi r4,0
	cr0.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq 0x821b0120
	if (cr0.getEQ()) goto loc_821B0120;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_821B0100:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x821b0100
	if (!cr6.getEQ()) goto loc_821B0100;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// b 0x821b0124
	goto loc_821B0124;
loc_821B0120:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
loc_821B0124:
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821b7580
	sub_821B7580(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b019c
	if (cr0.getLT()) goto loc_821B019C;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r4,0(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821af7f8
	sub_821AF7F8(ctx, base);
	// mr. r30,r3
	r30.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// blt 0x821b019c
	if (cr0.getLT()) goto loc_821B019C;
	// addi r26,r26,8
	r26.s64 = r26.s64 + 8;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x821b00ec
	if (!cr6.getEQ()) goto loc_821B00EC;
loc_821B016C:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x823d22e0
	sub_823D22E0(ctx, base);
loc_821B0174:
	// mr r30,r31
	r30.u64 = r31.u64;
loc_821B0178:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// stw r25,632(r27)
	PPC_STORE_U32(r27.u32 + 632, r25.u32);
	// beq cr6,0x821b0190
	if (cr6.getEQ()) goto loc_821B0190;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x821ae0e8
	sub_821AE0E8(ctx, base);
loc_821B0190:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x823ed17c
	return;
loc_821B019C:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x823d22e0
	sub_823D22E0(ctx, base);
	// b 0x821b0178
	goto loc_821B0178;
}

__attribute__((alias("__imp__sub_821B01A8"))) PPC_WEAK_FUNC(sub_821B01A8);
PPC_FUNC_IMPL(__imp__sub_821B01A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x821af710
	sub_821AF710(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

