#include "ppc_recomp_shared.h"

__attribute__((alias("__imp____savefpr_19"))) PPC_WEAK_FUNC(__savefpr_19);
PPC_FUNC_IMPL(__imp____savefpr_19) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f19,-104(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -104, f19.u64);
	// stfd f20,-96(r12)
	PPC_STORE_U64(r12.u32 + -96, f20.u64);
	// stfd f21,-88(r12)
	PPC_STORE_U64(r12.u32 + -88, f21.u64);
	// stfd f22,-80(r12)
	PPC_STORE_U64(r12.u32 + -80, f22.u64);
	// stfd f23,-72(r12)
	PPC_STORE_U64(r12.u32 + -72, f23.u64);
	// stfd f24,-64(r12)
	PPC_STORE_U64(r12.u32 + -64, f24.u64);
	// stfd f25,-56(r12)
	PPC_STORE_U64(r12.u32 + -56, f25.u64);
	// stfd f26,-48(r12)
	PPC_STORE_U64(r12.u32 + -48, f26.u64);
	// stfd f27,-40(r12)
	PPC_STORE_U64(r12.u32 + -40, f27.u64);
	// stfd f28,-32(r12)
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_20"))) PPC_WEAK_FUNC(__savefpr_20);
PPC_FUNC_IMPL(__imp____savefpr_20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f20,-96(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -96, f20.u64);
	// stfd f21,-88(r12)
	PPC_STORE_U64(r12.u32 + -88, f21.u64);
	// stfd f22,-80(r12)
	PPC_STORE_U64(r12.u32 + -80, f22.u64);
	// stfd f23,-72(r12)
	PPC_STORE_U64(r12.u32 + -72, f23.u64);
	// stfd f24,-64(r12)
	PPC_STORE_U64(r12.u32 + -64, f24.u64);
	// stfd f25,-56(r12)
	PPC_STORE_U64(r12.u32 + -56, f25.u64);
	// stfd f26,-48(r12)
	PPC_STORE_U64(r12.u32 + -48, f26.u64);
	// stfd f27,-40(r12)
	PPC_STORE_U64(r12.u32 + -40, f27.u64);
	// stfd f28,-32(r12)
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_21"))) PPC_WEAK_FUNC(__savefpr_21);
PPC_FUNC_IMPL(__imp____savefpr_21) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f21,-88(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -88, f21.u64);
	// stfd f22,-80(r12)
	PPC_STORE_U64(r12.u32 + -80, f22.u64);
	// stfd f23,-72(r12)
	PPC_STORE_U64(r12.u32 + -72, f23.u64);
	// stfd f24,-64(r12)
	PPC_STORE_U64(r12.u32 + -64, f24.u64);
	// stfd f25,-56(r12)
	PPC_STORE_U64(r12.u32 + -56, f25.u64);
	// stfd f26,-48(r12)
	PPC_STORE_U64(r12.u32 + -48, f26.u64);
	// stfd f27,-40(r12)
	PPC_STORE_U64(r12.u32 + -40, f27.u64);
	// stfd f28,-32(r12)
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_22"))) PPC_WEAK_FUNC(__savefpr_22);
PPC_FUNC_IMPL(__imp____savefpr_22) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f22,-80(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -80, f22.u64);
	// stfd f23,-72(r12)
	PPC_STORE_U64(r12.u32 + -72, f23.u64);
	// stfd f24,-64(r12)
	PPC_STORE_U64(r12.u32 + -64, f24.u64);
	// stfd f25,-56(r12)
	PPC_STORE_U64(r12.u32 + -56, f25.u64);
	// stfd f26,-48(r12)
	PPC_STORE_U64(r12.u32 + -48, f26.u64);
	// stfd f27,-40(r12)
	PPC_STORE_U64(r12.u32 + -40, f27.u64);
	// stfd f28,-32(r12)
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_23"))) PPC_WEAK_FUNC(__savefpr_23);
PPC_FUNC_IMPL(__imp____savefpr_23) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f23,-72(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -72, f23.u64);
	// stfd f24,-64(r12)
	PPC_STORE_U64(r12.u32 + -64, f24.u64);
	// stfd f25,-56(r12)
	PPC_STORE_U64(r12.u32 + -56, f25.u64);
	// stfd f26,-48(r12)
	PPC_STORE_U64(r12.u32 + -48, f26.u64);
	// stfd f27,-40(r12)
	PPC_STORE_U64(r12.u32 + -40, f27.u64);
	// stfd f28,-32(r12)
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_24"))) PPC_WEAK_FUNC(__savefpr_24);
PPC_FUNC_IMPL(__imp____savefpr_24) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f24,-64(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -64, f24.u64);
	// stfd f25,-56(r12)
	PPC_STORE_U64(r12.u32 + -56, f25.u64);
	// stfd f26,-48(r12)
	PPC_STORE_U64(r12.u32 + -48, f26.u64);
	// stfd f27,-40(r12)
	PPC_STORE_U64(r12.u32 + -40, f27.u64);
	// stfd f28,-32(r12)
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_25"))) PPC_WEAK_FUNC(__savefpr_25);
PPC_FUNC_IMPL(__imp____savefpr_25) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f25,-56(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -56, f25.u64);
	// stfd f26,-48(r12)
	PPC_STORE_U64(r12.u32 + -48, f26.u64);
	// stfd f27,-40(r12)
	PPC_STORE_U64(r12.u32 + -40, f27.u64);
	// stfd f28,-32(r12)
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_26"))) PPC_WEAK_FUNC(__savefpr_26);
PPC_FUNC_IMPL(__imp____savefpr_26) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f26,-48(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -48, f26.u64);
	// stfd f27,-40(r12)
	PPC_STORE_U64(r12.u32 + -40, f27.u64);
	// stfd f28,-32(r12)
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_27"))) PPC_WEAK_FUNC(__savefpr_27);
PPC_FUNC_IMPL(__imp____savefpr_27) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f27,-40(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -40, f27.u64);
	// stfd f28,-32(r12)
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_28"))) PPC_WEAK_FUNC(__savefpr_28);
PPC_FUNC_IMPL(__imp____savefpr_28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f28,-32(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -32, f28.u64);
	// stfd f29,-24(r12)
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_29"))) PPC_WEAK_FUNC(__savefpr_29);
PPC_FUNC_IMPL(__imp____savefpr_29) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f29,-24(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -24, f29.u64);
	// stfd f30,-16(r12)
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_30"))) PPC_WEAK_FUNC(__savefpr_30);
PPC_FUNC_IMPL(__imp____savefpr_30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f30{};
	PPCRegister f31{};
	// stfd f30,-16(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -16, f30.u64);
	// stfd f31,-8(r12)
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____savefpr_31"))) PPC_WEAK_FUNC(__savefpr_31);
PPC_FUNC_IMPL(__imp____savefpr_31) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f31{};
	// stfd f31,-8(r12)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r12.u32 + -8, f31.u64);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_14"))) PPC_WEAK_FUNC(__restfpr_14);
PPC_FUNC_IMPL(__imp____restfpr_14) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f14{};
	PPCRegister f15{};
	PPCRegister f16{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f14,-144(r12)
	ctx.fpscr.disableFlushMode();
	f14.u64 = PPC_LOAD_U64(r12.u32 + -144);
	// lfd f15,-136(r12)
	f15.u64 = PPC_LOAD_U64(r12.u32 + -136);
	// lfd f16,-128(r12)
	f16.u64 = PPC_LOAD_U64(r12.u32 + -128);
	// lfd f17,-120(r12)
	f17.u64 = PPC_LOAD_U64(r12.u32 + -120);
	// lfd f18,-112(r12)
	f18.u64 = PPC_LOAD_U64(r12.u32 + -112);
	// lfd f19,-104(r12)
	f19.u64 = PPC_LOAD_U64(r12.u32 + -104);
	// lfd f20,-96(r12)
	f20.u64 = PPC_LOAD_U64(r12.u32 + -96);
	// lfd f21,-88(r12)
	f21.u64 = PPC_LOAD_U64(r12.u32 + -88);
	// lfd f22,-80(r12)
	f22.u64 = PPC_LOAD_U64(r12.u32 + -80);
	// lfd f23,-72(r12)
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_15"))) PPC_WEAK_FUNC(__restfpr_15);
PPC_FUNC_IMPL(__imp____restfpr_15) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f15{};
	PPCRegister f16{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f15,-136(r12)
	ctx.fpscr.disableFlushMode();
	f15.u64 = PPC_LOAD_U64(r12.u32 + -136);
	// lfd f16,-128(r12)
	f16.u64 = PPC_LOAD_U64(r12.u32 + -128);
	// lfd f17,-120(r12)
	f17.u64 = PPC_LOAD_U64(r12.u32 + -120);
	// lfd f18,-112(r12)
	f18.u64 = PPC_LOAD_U64(r12.u32 + -112);
	// lfd f19,-104(r12)
	f19.u64 = PPC_LOAD_U64(r12.u32 + -104);
	// lfd f20,-96(r12)
	f20.u64 = PPC_LOAD_U64(r12.u32 + -96);
	// lfd f21,-88(r12)
	f21.u64 = PPC_LOAD_U64(r12.u32 + -88);
	// lfd f22,-80(r12)
	f22.u64 = PPC_LOAD_U64(r12.u32 + -80);
	// lfd f23,-72(r12)
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_16"))) PPC_WEAK_FUNC(__restfpr_16);
PPC_FUNC_IMPL(__imp____restfpr_16) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f16{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f16,-128(r12)
	ctx.fpscr.disableFlushMode();
	f16.u64 = PPC_LOAD_U64(r12.u32 + -128);
	// lfd f17,-120(r12)
	f17.u64 = PPC_LOAD_U64(r12.u32 + -120);
	// lfd f18,-112(r12)
	f18.u64 = PPC_LOAD_U64(r12.u32 + -112);
	// lfd f19,-104(r12)
	f19.u64 = PPC_LOAD_U64(r12.u32 + -104);
	// lfd f20,-96(r12)
	f20.u64 = PPC_LOAD_U64(r12.u32 + -96);
	// lfd f21,-88(r12)
	f21.u64 = PPC_LOAD_U64(r12.u32 + -88);
	// lfd f22,-80(r12)
	f22.u64 = PPC_LOAD_U64(r12.u32 + -80);
	// lfd f23,-72(r12)
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_17"))) PPC_WEAK_FUNC(__restfpr_17);
PPC_FUNC_IMPL(__imp____restfpr_17) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f17,-120(r12)
	ctx.fpscr.disableFlushMode();
	f17.u64 = PPC_LOAD_U64(r12.u32 + -120);
	// lfd f18,-112(r12)
	f18.u64 = PPC_LOAD_U64(r12.u32 + -112);
	// lfd f19,-104(r12)
	f19.u64 = PPC_LOAD_U64(r12.u32 + -104);
	// lfd f20,-96(r12)
	f20.u64 = PPC_LOAD_U64(r12.u32 + -96);
	// lfd f21,-88(r12)
	f21.u64 = PPC_LOAD_U64(r12.u32 + -88);
	// lfd f22,-80(r12)
	f22.u64 = PPC_LOAD_U64(r12.u32 + -80);
	// lfd f23,-72(r12)
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_18"))) PPC_WEAK_FUNC(__restfpr_18);
PPC_FUNC_IMPL(__imp____restfpr_18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f18,-112(r12)
	ctx.fpscr.disableFlushMode();
	f18.u64 = PPC_LOAD_U64(r12.u32 + -112);
	// lfd f19,-104(r12)
	f19.u64 = PPC_LOAD_U64(r12.u32 + -104);
	// lfd f20,-96(r12)
	f20.u64 = PPC_LOAD_U64(r12.u32 + -96);
	// lfd f21,-88(r12)
	f21.u64 = PPC_LOAD_U64(r12.u32 + -88);
	// lfd f22,-80(r12)
	f22.u64 = PPC_LOAD_U64(r12.u32 + -80);
	// lfd f23,-72(r12)
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_19"))) PPC_WEAK_FUNC(__restfpr_19);
PPC_FUNC_IMPL(__imp____restfpr_19) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f19,-104(r12)
	ctx.fpscr.disableFlushMode();
	f19.u64 = PPC_LOAD_U64(r12.u32 + -104);
	// lfd f20,-96(r12)
	f20.u64 = PPC_LOAD_U64(r12.u32 + -96);
	// lfd f21,-88(r12)
	f21.u64 = PPC_LOAD_U64(r12.u32 + -88);
	// lfd f22,-80(r12)
	f22.u64 = PPC_LOAD_U64(r12.u32 + -80);
	// lfd f23,-72(r12)
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_20"))) PPC_WEAK_FUNC(__restfpr_20);
PPC_FUNC_IMPL(__imp____restfpr_20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f20,-96(r12)
	ctx.fpscr.disableFlushMode();
	f20.u64 = PPC_LOAD_U64(r12.u32 + -96);
	// lfd f21,-88(r12)
	f21.u64 = PPC_LOAD_U64(r12.u32 + -88);
	// lfd f22,-80(r12)
	f22.u64 = PPC_LOAD_U64(r12.u32 + -80);
	// lfd f23,-72(r12)
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_21"))) PPC_WEAK_FUNC(__restfpr_21);
PPC_FUNC_IMPL(__imp____restfpr_21) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f21,-88(r12)
	ctx.fpscr.disableFlushMode();
	f21.u64 = PPC_LOAD_U64(r12.u32 + -88);
	// lfd f22,-80(r12)
	f22.u64 = PPC_LOAD_U64(r12.u32 + -80);
	// lfd f23,-72(r12)
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_22"))) PPC_WEAK_FUNC(__restfpr_22);
PPC_FUNC_IMPL(__imp____restfpr_22) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f22,-80(r12)
	ctx.fpscr.disableFlushMode();
	f22.u64 = PPC_LOAD_U64(r12.u32 + -80);
	// lfd f23,-72(r12)
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_23"))) PPC_WEAK_FUNC(__restfpr_23);
PPC_FUNC_IMPL(__imp____restfpr_23) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f23,-72(r12)
	ctx.fpscr.disableFlushMode();
	f23.u64 = PPC_LOAD_U64(r12.u32 + -72);
	// lfd f24,-64(r12)
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_24"))) PPC_WEAK_FUNC(__restfpr_24);
PPC_FUNC_IMPL(__imp____restfpr_24) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f24,-64(r12)
	ctx.fpscr.disableFlushMode();
	f24.u64 = PPC_LOAD_U64(r12.u32 + -64);
	// lfd f25,-56(r12)
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_25"))) PPC_WEAK_FUNC(__restfpr_25);
PPC_FUNC_IMPL(__imp____restfpr_25) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f25,-56(r12)
	ctx.fpscr.disableFlushMode();
	f25.u64 = PPC_LOAD_U64(r12.u32 + -56);
	// lfd f26,-48(r12)
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_26"))) PPC_WEAK_FUNC(__restfpr_26);
PPC_FUNC_IMPL(__imp____restfpr_26) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f26,-48(r12)
	ctx.fpscr.disableFlushMode();
	f26.u64 = PPC_LOAD_U64(r12.u32 + -48);
	// lfd f27,-40(r12)
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_27"))) PPC_WEAK_FUNC(__restfpr_27);
PPC_FUNC_IMPL(__imp____restfpr_27) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f27,-40(r12)
	ctx.fpscr.disableFlushMode();
	f27.u64 = PPC_LOAD_U64(r12.u32 + -40);
	// lfd f28,-32(r12)
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_28"))) PPC_WEAK_FUNC(__restfpr_28);
PPC_FUNC_IMPL(__imp____restfpr_28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f28,-32(r12)
	ctx.fpscr.disableFlushMode();
	f28.u64 = PPC_LOAD_U64(r12.u32 + -32);
	// lfd f29,-24(r12)
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_29"))) PPC_WEAK_FUNC(__restfpr_29);
PPC_FUNC_IMPL(__imp____restfpr_29) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f29,-24(r12)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(r12.u32 + -24);
	// lfd f30,-16(r12)
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_30"))) PPC_WEAK_FUNC(__restfpr_30);
PPC_FUNC_IMPL(__imp____restfpr_30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f30{};
	PPCRegister f31{};
	// lfd f30,-16(r12)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(r12.u32 + -16);
	// lfd f31,-8(r12)
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____restfpr_31"))) PPC_WEAK_FUNC(__restfpr_31);
PPC_FUNC_IMPL(__imp____restfpr_31) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister f31{};
	// lfd f31,-8(r12)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(r12.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823ED5A8"))) PPC_WEAK_FUNC(sub_823ED5A8);
PPC_FUNC_IMPL(__imp__sub_823ED5A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x823ed5fc
	if (!cr6.getEQ()) goto loc_823ED5FC;
loc_823ED5CC:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x823ed704
	goto loc_823ED704;
loc_823ED5FC:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x823ed60c
	if (cr6.getEQ()) goto loc_823ED60C;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x823ed5cc
	if (cr6.getEQ()) goto loc_823ED5CC;
loc_823ED60C:
	// lis r10,16383
	ctx.r10.s64 = 1073676288;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// li r9,66
	ctx.r9.s64 = 66;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// ble cr6,0x823ed638
	if (!cr6.getGT()) goto loc_823ED638;
	// lis r10,32767
	ctx.r10.s64 = 2147418112;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// b 0x823ed63c
	goto loc_823ED63C;
loc_823ED638:
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
loc_823ED63C:
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x823ed668
	if (!cr6.getEQ()) goto loc_823ED668;
loc_823ED660:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// b 0x823ed704
	goto loc_823ED704;
loc_823ED668:
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// blt cr6,0x823ed6e0
	if (cr6.getLT()) goto loc_823ED6E0;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// blt 0x823ed69c
	if (cr0.getLT()) goto loc_823ED69C;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stb r30,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r30.u8);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x823ed6b0
	goto loc_823ED6B0;
loc_823ED69C:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f5e28
	sub_823F5E28(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x823ed6e0
	if (cr6.getEQ()) goto loc_823ED6E0;
loc_823ED6B0:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// blt 0x823ed6cc
	if (cr0.getLT()) goto loc_823ED6CC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stb r30,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r30.u8);
	// b 0x823ed660
	goto loc_823ED660;
loc_823ED6CC:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f5e28
	sub_823F5E28(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// bne cr6,0x823ed660
	if (!cr6.getEQ()) goto loc_823ED660;
loc_823ED6E0:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cntlzw r10,r11
	ctx.r10.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r11,r28,1,0,30
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// add r9,r11,r31
	ctx.r9.u64 = r11.u64 + r31.u64;
	// rlwinm r11,r10,27,31,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// sth r30,-2(r9)
	PPC_STORE_U16(ctx.r9.u32 + -2, r30.u16);
	// addi r3,r11,-2
	ctx.r3.s64 = r11.s64 + -2;
loc_823ED704:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_823ED710"))) PPC_WEAK_FUNC(sub_823ED710);
PPC_FUNC_IMPL(__imp__sub_823ED710) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// lis r11,-32193
	r11.s64 = -2109800448;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r11,24672
	ctx.r3.s64 = r11.s64 + 24672;
	// bl 0x823ed5a8
	sub_823ED5A8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x823ed748
	if (!cr0.getLT()) goto loc_823ED748;
	// li r3,-1
	ctx.r3.s64 = -1;
loc_823ED748:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823ED758"))) PPC_WEAK_FUNC(sub_823ED758);
PPC_FUNC_IMPL(__imp__sub_823ED758) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x823ed784
	if (!cr6.getEQ()) goto loc_823ED784;
loc_823ED778:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r10,22
	ctx.r10.s64 = 22;
	// b 0x823ed7d0
	goto loc_823ED7D0;
loc_823ED784:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x823ed778
	if (cr6.getEQ()) goto loc_823ED778;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x823ed778
	if (cr6.getEQ()) goto loc_823ED778;
	// lis r11,-32193
	r11.s64 = -2109800448;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r11,27560
	ctx.r3.s64 = r11.s64 + 27560;
	// bl 0x823ed5a8
	sub_823ED5A8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bge 0x823ed7c0
	if (!cr0.getLT()) goto loc_823ED7C0;
	// li r11,0
	r11.s64 = 0;
	// sth r11,0(r31)
	PPC_STORE_U16(r31.u32 + 0, r11.u16);
loc_823ED7C0:
	// cmpwi cr6,r3,-2
	cr6.compare<int32_t>(ctx.r3.s32, -2, xer);
	// bne cr6,0x823ed7f4
	if (!cr6.getEQ()) goto loc_823ED7F4;
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r10,34
	ctx.r10.s64 = 34;
loc_823ED7D0:
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
loc_823ED7F4:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823ED808"))) PPC_WEAK_FUNC(sub_823ED808);
PPC_FUNC_IMPL(__imp__sub_823ED808) {
	PPC_FUNC_PROLOGUE();
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x823ed758
	sub_823ED758(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823ED818"))) PPC_WEAK_FUNC(sub_823ED818);
PPC_FUNC_IMPL(__imp__sub_823ED818) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x823ed840
	if (cr6.getEQ()) goto loc_823ED840;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x823ed8e4
	if (cr6.getEQ()) goto loc_823ED8E4;
	// sth r3,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r3.u16);
loc_823ED840:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x823ed878
	if (!cr6.getEQ()) goto loc_823ED878;
loc_823ED848:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x823ed8e4
	goto loc_823ED8E4;
loc_823ED878:
	// lis r10,32767
	ctx.r10.s64 = 2147418112;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// cmplw cr6,r5,r10
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r10.u32, xer);
	// bgt cr6,0x823ed848
	if (cr6.getGT()) goto loc_823ED848;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x823ed8c0
	if (cr6.getEQ()) goto loc_823ED8C0;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x823ed8e4
	if (cr6.getEQ()) goto loc_823ED8E4;
loc_823ED898:
	// lbzx r10,r3,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + ctx.r4.u32);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// lbzx r10,r3,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + ctx.r4.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x823ed8e4
	if (cr6.getEQ()) goto loc_823ED8E4;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplw cr6,r3,r5
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r5.u32, xer);
	// blt cr6,0x823ed898
	if (cr6.getLT()) goto loc_823ED898;
	// b 0x823ed8e4
	goto loc_823ED8E4;
loc_823ED8C0:
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_823ED8C8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x823ed8c8
	if (!cr6.getEQ()) goto loc_823ED8C8;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
loc_823ED8E4:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823ED8F8"))) PPC_WEAK_FUNC(sub_823ED8F8);
PPC_FUNC_IMPL(__imp__sub_823ED8F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed130
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r28,0
	r28.s64 = 0;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// mr r26,r28
	r26.u64 = r28.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x823ed934
	if (!cr6.getEQ()) goto loc_823ED934;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x823ed970
	if (cr6.getEQ()) goto loc_823ED970;
	// b 0x823ed93c
	goto loc_823ED93C;
loc_823ED934:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x823ed96c
	if (!cr6.getEQ()) goto loc_823ED96C;
loc_823ED93C:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,22
	ctx.r3.s64 = 22;
	// b 0x823eda30
	goto loc_823EDA30;
loc_823ED96C:
	// sth r28,0(r31)
	PPC_STORE_U16(r31.u32 + 0, r28.u16);
loc_823ED970:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x823ed97c
	if (cr6.getEQ()) goto loc_823ED97C;
	// stw r28,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r28.u32);
loc_823ED97C:
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bgt cr6,0x823ed98c
	if (cr6.getGT()) goto loc_823ED98C;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
loc_823ED98C:
	// lis r11,-32015
	r11.s64 = -2098135040;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r6,r11,-19456
	ctx.r6.s64 = r11.s64 + -19456;
	// bl 0x823ed818
	sub_823ED818(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// bne cr6,0x823ed9bc
	if (!cr6.getEQ()) goto loc_823ED9BC;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x823ed9b0
	if (cr6.getEQ()) goto loc_823ED9B0;
	// sth r28,0(r31)
	PPC_STORE_U16(r31.u32 + 0, r28.u16);
loc_823ED9B0:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// b 0x823eda30
	goto loc_823EDA30;
loc_823ED9BC:
	// addi r11,r3,1
	r11.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x823eda20
	if (cr6.getEQ()) goto loc_823EDA20;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// ble cr6,0x823eda14
	if (!cr6.getGT()) goto loc_823EDA14;
	// cmpwi cr6,r29,-1
	cr6.compare<int32_t>(r29.s32, -1, xer);
	// beq cr6,0x823eda0c
	if (cr6.getEQ()) goto loc_823EDA0C;
	// sth r28,0(r31)
	PPC_STORE_U16(r31.u32 + 0, r28.u16);
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,34
	ctx.r10.s64 = 34;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,34
	ctx.r3.s64 = 34;
	// b 0x823eda30
	goto loc_823EDA30;
loc_823EDA0C:
	// mr r11,r30
	r11.u64 = r30.u64;
	// li r26,80
	r26.s64 = 80;
loc_823EDA14:
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// sth r28,-2(r10)
	PPC_STORE_U16(ctx.r10.u32 + -2, r28.u16);
loc_823EDA20:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x823eda2c
	if (cr6.getEQ()) goto loc_823EDA2C;
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
loc_823EDA2C:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
loc_823EDA30:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x823ed180
	return;
}

__attribute__((alias("__imp__sub_823EDA38"))) PPC_WEAK_FUNC(sub_823EDA38);
PPC_FUNC_IMPL(__imp__sub_823EDA38) {
	PPC_FUNC_PROLOGUE();
	// li r8,0
	ctx.r8.s64 = 0;
	// b 0x823ed8f8
	sub_823ED8F8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EDA40"))) PPC_WEAK_FUNC(sub_823EDA40);
PPC_FUNC_IMPL(__imp__sub_823EDA40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// b 0x823eda54
	goto loc_823EDA54;
loc_823EDA44:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x823eda64
	if (cr6.getEQ()) goto loc_823EDA64;
	// addi r4,r4,2
	ctx.r4.s64 = ctx.r4.s64 + 2;
	// addi r3,r3,2
	ctx.r3.s64 = ctx.r3.s64 + 2;
loc_823EDA54:
	// lhz r11,0(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// lhz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// subf. r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x823eda44
	if (cr0.getEQ()) goto loc_823EDA44;
loc_823EDA64:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x823eda74
	if (!cr6.getLT()) goto loc_823EDA74;
	// li r11,-1
	r11.s64 = -1;
	// b 0x823eda7c
	goto loc_823EDA7C;
loc_823EDA74:
	// ble cr6,0x823eda7c
	if (!cr6.getGT()) goto loc_823EDA7C;
	// li r11,1
	r11.s64 = 1;
loc_823EDA7C:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDA88"))) PPC_WEAK_FUNC(sub_823EDA88);
PPC_FUNC_IMPL(__imp__sub_823EDA88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,136
	ctx.r10.s64 = ctx.r1.s64 + 136;
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x823ed758
	sub_823ED758(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDAD0"))) PPC_WEAK_FUNC(sub_823EDAD0);
PPC_FUNC_IMPL(__imp__sub_823EDAD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x823edaf0
	if (cr6.getEQ()) goto loc_823EDAF0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x823edb20
	if (!cr6.getEQ()) goto loc_823EDB20;
loc_823EDAF0:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,22
	ctx.r3.s64 = 22;
	// b 0x823edba0
	goto loc_823EDBA0;
loc_823EDB20:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x823edb5c
	if (!cr6.getEQ()) goto loc_823EDB5C;
	// li r11,0
	r11.s64 = 0;
	// sth r11,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r11.u16);
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r31,22
	r31.s64 = 22;
loc_823EDB38:
	// stw r31,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r31.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// b 0x823edba0
	goto loc_823EDBA0;
loc_823EDB5C:
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_823EDB60:
	// lhz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// beq 0x823edb80
	if (cr0.getEQ()) goto loc_823EDB80;
	// addic. r4,r4,-1
	xer.ca = ctx.r4.u32 > 0;
	ctx.r4.s64 = ctx.r4.s64 + -1;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne 0x823edb60
	if (!cr0.getEQ()) goto loc_823EDB60;
loc_823EDB80:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x823edb9c
	if (!cr6.getEQ()) goto loc_823EDB9C;
	// li r11,0
	r11.s64 = 0;
	// sth r11,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r11.u16);
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r31,34
	r31.s64 = 34;
	// b 0x823edb38
	goto loc_823EDB38;
loc_823EDB9C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_823EDBA0:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDBB8"))) PPC_WEAK_FUNC(sub_823EDBB8);
PPC_FUNC_IMPL(__imp__sub_823EDBB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_823EDBBC:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne 0x823edbbc
	if (!cr0.getEQ()) goto loc_823EDBBC;
	// subf r11,r3,r11
	r11.s64 = r11.s64 - ctx.r3.s64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addi r3,r11,-1
	ctx.r3.s64 = r11.s64 + -1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDBE0"))) PPC_WEAK_FUNC(sub_823EDBE0);
PPC_FUNC_IMPL(__imp__sub_823EDBE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r11,-32015
	r11.s64 = -2098135040;
	// fabs f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = ctx.f1.u64 & ~0x8000000000000000;
	// stfd f12,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.f12.u64);
	// addi r11,r11,-21568
	r11.s64 = r11.s64 + -21568;
	// lfs f0,28(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 28);
	f0.f64 = double(temp.f32);
	// lfs f9,32(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 32);
	ctx.f9.f64 = double(temp.f32);
	// fsel f9,f1,f0,f9
	ctx.f9.f64 = ctx.f1.f64 >= 0.0 ? f0.f64 : ctx.f9.f64;
	// lfd f0,8(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// fmul f0,f0,f12
	f0.f64 = f0.f64 * ctx.f12.f64;
	// lfd f13,40(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 40);
	// lfd f11,48(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + 48);
	// lfd f10,112(r11)
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + 112);
	// fctid f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvtsd_si64(_mm_load_sd(&f0.f64));
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fnmsub f13,f13,f0,f12
	ctx.f13.f64 = -(ctx.f13.f64 * f0.f64 - ctx.f12.f64);
	// fctidz f8,f0
	ctx.f8.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&f0.f64));
	// stfd f8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f8.u64);
	// fnmsub f13,f11,f0,f13
	ctx.f13.f64 = -(ctx.f11.f64 * f0.f64 - ctx.f13.f64);
	// lfd f11,104(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + 104);
	// fmul f0,f13,f13
	f0.f64 = ctx.f13.f64 * ctx.f13.f64;
	// fmadd f10,f10,f0,f11
	ctx.f10.f64 = ctx.f10.f64 * f0.f64 + ctx.f11.f64;
	// lfd f11,96(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + 96);
	// fmadd f10,f10,f0,f11
	ctx.f10.f64 = ctx.f10.f64 * f0.f64 + ctx.f11.f64;
	// lfd f11,88(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + 88);
	// fmadd f10,f10,f0,f11
	ctx.f10.f64 = ctx.f10.f64 * f0.f64 + ctx.f11.f64;
	// lfd f11,80(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + 80);
	// ld r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// clrldi r10,r10,63
	ctx.r10.u64 = ctx.r10.u64 & 0x1;
	// cmpdi cr6,r10,0
	cr6.compare<int64_t>(ctx.r10.s64, 0, xer);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// fmadd f10,f10,f0,f11
	ctx.f10.f64 = ctx.f10.f64 * f0.f64 + ctx.f11.f64;
	// lfd f11,72(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + 72);
	// fmadd f10,f10,f0,f11
	ctx.f10.f64 = ctx.f10.f64 * f0.f64 + ctx.f11.f64;
	// lfd f11,64(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + 64);
	// fmadd f10,f10,f0,f11
	ctx.f10.f64 = ctx.f10.f64 * f0.f64 + ctx.f11.f64;
	// lfd f11,56(r11)
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + 56);
	// fmadd f10,f10,f0,f11
	ctx.f10.f64 = ctx.f10.f64 * f0.f64 + ctx.f11.f64;
	// lfd f11,2728(r10)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r10.u32 + 2728);
	// fmadd f0,f10,f0,f11
	f0.f64 = ctx.f10.f64 * f0.f64 + ctx.f11.f64;
	// fmul f0,f0,f13
	f0.f64 = f0.f64 * ctx.f13.f64;
	// beq cr6,0x823edc88
	if (cr6.getEQ()) goto loc_823EDC88;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_823EDC88:
	// ld r10,-8(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fmul f13,f0,f9
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = f0.f64 * ctx.f9.f64;
	// cmpdi cr6,r10,0
	cr6.compare<int64_t>(ctx.r10.s64, 0, xer);
	// beqlr cr6
	if (cr6.getEQ()) return;
	// lfd f0,16(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// lis r11,-32015
	r11.s64 = -2098135040;
	// fsub f12,f12,f0
	ctx.f12.f64 = ctx.f12.f64 - f0.f64;
	// lfd f0,-19440(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -19440);
	// fsel f1,f12,f0,f13
	ctx.f1.f64 = ctx.f12.f64 >= 0.0 ? f0.f64 : ctx.f13.f64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDCB0"))) PPC_WEAK_FUNC(sub_823EDCB0);
PPC_FUNC_IMPL(__imp__sub_823EDCB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// fabs f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = ctx.f1.u64 & ~0x8000000000000000;
	// lis r11,-32015
	r11.s64 = -2098135040;
	// addi r11,r11,-21568
	r11.s64 = r11.s64 + -21568;
	// lfd f0,0(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// lfs f13,36(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 36);
	ctx.f13.f64 = double(temp.f32);
	// lfd f10,40(r11)
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + 40);
	// fadd f11,f0,f12
	ctx.f11.f64 = f0.f64 + ctx.f12.f64;
	// lfd f0,8(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 8);
	// lfd f9,48(r11)
	ctx.f9.u64 = PPC_LOAD_U64(r11.u32 + 48);
	// lfd f8,112(r11)
	ctx.f8.u64 = PPC_LOAD_U64(r11.u32 + 112);
	// fmul f0,f0,f11
	f0.f64 = f0.f64 * ctx.f11.f64;
	// fctid f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvtsd_si64(_mm_load_sd(&f0.f64));
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fmr f7,f0
	ctx.f7.f64 = f0.f64;
	// fsub f0,f0,f13
	f0.f64 = f0.f64 - ctx.f13.f64;
	// fctidz f13,f7
	ctx.f13.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f7.f64));
	// stfd f13,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f13.u64);
	// fnmsub f13,f10,f0,f12
	ctx.f13.f64 = -(ctx.f10.f64 * f0.f64 - ctx.f12.f64);
	// lfd f10,104(r11)
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + 104);
	// fnmsub f13,f9,f0,f13
	ctx.f13.f64 = -(ctx.f9.f64 * f0.f64 - ctx.f13.f64);
	// fmul f0,f13,f13
	f0.f64 = ctx.f13.f64 * ctx.f13.f64;
	// fmadd f9,f8,f0,f10
	ctx.f9.f64 = ctx.f8.f64 * f0.f64 + ctx.f10.f64;
	// lfd f10,96(r11)
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + 96);
	// fmadd f9,f9,f0,f10
	ctx.f9.f64 = ctx.f9.f64 * f0.f64 + ctx.f10.f64;
	// lfd f10,88(r11)
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + 88);
	// ld r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fmadd f9,f9,f0,f10
	ctx.f9.f64 = ctx.f9.f64 * f0.f64 + ctx.f10.f64;
	// lfd f10,80(r11)
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + 80);
	// clrldi r10,r10,63
	ctx.r10.u64 = ctx.r10.u64 & 0x1;
	// cmpdi cr6,r10,0
	cr6.compare<int64_t>(ctx.r10.s64, 0, xer);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// fmadd f9,f9,f0,f10
	ctx.f9.f64 = ctx.f9.f64 * f0.f64 + ctx.f10.f64;
	// lfd f10,72(r11)
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + 72);
	// fmadd f9,f9,f0,f10
	ctx.f9.f64 = ctx.f9.f64 * f0.f64 + ctx.f10.f64;
	// lfd f10,64(r11)
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + 64);
	// fmadd f9,f9,f0,f10
	ctx.f9.f64 = ctx.f9.f64 * f0.f64 + ctx.f10.f64;
	// lfd f10,56(r11)
	ctx.f10.u64 = PPC_LOAD_U64(r11.u32 + 56);
	// fmadd f9,f9,f0,f10
	ctx.f9.f64 = ctx.f9.f64 * f0.f64 + ctx.f10.f64;
	// lfd f10,2728(r10)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r10.u32 + 2728);
	// fmadd f0,f9,f0,f10
	f0.f64 = ctx.f9.f64 * f0.f64 + ctx.f10.f64;
	// fmul f0,f0,f13
	f0.f64 = f0.f64 * ctx.f13.f64;
	// beq cr6,0x823edd5c
	if (cr6.getEQ()) goto loc_823EDD5C;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_823EDD5C:
	// lfs f13,24(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// bne cr6,0x823edd70
	if (!cr6.getEQ()) goto loc_823EDD70;
	// lfs f1,28(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 28);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
loc_823EDD70:
	// lfd f13,16(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// lis r11,-32015
	r11.s64 = -2098135040;
	// fsub f12,f11,f13
	ctx.f12.f64 = ctx.f11.f64 - ctx.f13.f64;
	// lfd f13,-19440(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + -19440);
	// fsel f1,f12,f13,f0
	ctx.f1.f64 = ctx.f12.f64 >= 0.0 ? ctx.f13.f64 : f0.f64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDD88"))) PPC_WEAK_FUNC(sub_823EDD88);
PPC_FUNC_IMPL(__imp__sub_823EDD88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,4
	ctx.r4.s64 = 4;
	// li r3,32
	ctx.r3.s64 = 32;
	// bl 0x823f1f38
	sub_823F1F38(ctx, base);
	// lis r10,-31987
	ctx.r10.s64 = -2096300032;
	// mr. r11,r3
	r11.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,-15560(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15560, r11.u32);
	// lis r10,-31987
	ctx.r10.s64 = -2096300032;
	// stw r11,-15564(r10)
	PPC_STORE_U32(ctx.r10.u32 + -15564, r11.u32);
	// bne 0x823eddc0
	if (!cr0.getEQ()) goto loc_823EDDC0;
	// li r3,24
	ctx.r3.s64 = 24;
	// b 0x823eddcc
	goto loc_823EDDCC;
loc_823EDDC0:
	// li r10,0
	ctx.r10.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_823EDDCC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDDE0"))) PPC_WEAK_FUNC(sub_823EDDE0);
PPC_FUNC_IMPL(__imp__sub_823EDDE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r18{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r18,-1044(0)
	r18.u64 = PPC_LOAD_U32(-1044);
	// lwz r16,29392(r7)
	r16.u64 = PPC_LOAD_U32(ctx.r7.u32 + 29392);
	// mflr r12
	// bl 0x823ed124
	// addi r31,r1,-176
	r31.s64 = ctx.r1.s64 + -176;
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// bl 0x823ef650
	sub_823EF650(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lis r25,-31987
	r25.s64 = -2096300032;
	// lwz r28,-15560(r25)
	r28.u64 = PPC_LOAD_U32(r25.u32 + -15560);
	// lis r24,-31987
	r24.s64 = -2096300032;
	// lwz r30,-15564(r24)
	r30.u64 = PPC_LOAD_U32(r24.u32 + -15564);
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// blt cr6,0x823edeac
	if (cr6.getLT()) goto loc_823EDEAC;
	// subf r26,r28,r30
	r26.s64 = r30.s64 - r28.s64;
	// addi r27,r26,4
	r27.s64 = r26.s64 + 4;
	// cmplwi cr6,r27,4
	cr6.compare<uint32_t>(r27.u32, 4, xer);
	// blt cr6,0x823edeac
	if (cr6.getLT()) goto loc_823EDEAC;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823f7a98
	sub_823F7A98(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplw cr6,r29,r27
	cr6.compare<uint32_t>(r29.u32, r27.u32, xer);
	// bge cr6,0x823ede98
	if (!cr6.getLT()) goto loc_823EDE98;
	// cmplwi cr6,r29,2048
	cr6.compare<uint32_t>(r29.u32, 2048, xer);
	// mr r11,r29
	r11.u64 = r29.u64;
	// blt cr6,0x823ede50
	if (cr6.getLT()) goto loc_823EDE50;
	// li r11,2048
	r11.s64 = 2048;
loc_823EDE50:
	// add r4,r11,r29
	ctx.r4.u64 = r11.u64 + r29.u64;
	// cmplw cr6,r4,r29
	cr6.compare<uint32_t>(ctx.r4.u32, r29.u32, xer);
	// blt cr6,0x823ede6c
	if (cr6.getLT()) goto loc_823EDE6C;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823f78f0
	sub_823F78F0(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x823ede88
	if (!cr0.getEQ()) goto loc_823EDE88;
loc_823EDE6C:
	// addi r4,r29,16
	ctx.r4.s64 = r29.s64 + 16;
	// cmplw cr6,r4,r29
	cr6.compare<uint32_t>(ctx.r4.u32, r29.u32, xer);
	// blt cr6,0x823edeac
	if (cr6.getLT()) goto loc_823EDEAC;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823f78f0
	sub_823F78F0(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x823edeac
	if (cr0.getEQ()) goto loc_823EDEAC;
loc_823EDE88:
	// stw r3,-15560(r25)
	PPC_STORE_U32(r25.u32 + -15560, ctx.r3.u32);
	// srawi r11,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r11.s64 = r26.s32 >> 2;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
loc_823EDE98:
	// stw r23,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r23.u32);
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// stw r11,-15564(r24)
	PPC_STORE_U32(r24.u32 + -15564, r11.u32);
	// stw r23,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r23.u32);
	// b 0x823edeb4
	goto loc_823EDEB4;
loc_823EDEAC:
	// li r11,0
	r11.s64 = 0;
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
loc_823EDEB4:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r12,r31,176
	r12.s64 = r31.s64 + 176;
	// bl 0x823edecc
	sub_823EDECC(ctx, base);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// addi r1,r31,176
	ctx.r1.s64 = r31.s64 + 176;
	// b 0x823ed174
	return;
}

__attribute__((alias("__imp__sub_823EDDE8"))) PPC_WEAK_FUNC(sub_823EDDE8);
PPC_FUNC_IMPL(__imp__sub_823EDDE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed124
	// addi r31,r1,-176
	r31.s64 = ctx.r1.s64 + -176;
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// bl 0x823ef650
	sub_823EF650(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lis r25,-31987
	r25.s64 = -2096300032;
	// lwz r28,-15560(r25)
	r28.u64 = PPC_LOAD_U32(r25.u32 + -15560);
	// lis r24,-31987
	r24.s64 = -2096300032;
	// lwz r30,-15564(r24)
	r30.u64 = PPC_LOAD_U32(r24.u32 + -15564);
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// blt cr6,0x823edeac
	if (cr6.getLT()) goto loc_823EDEAC;
	// subf r26,r28,r30
	r26.s64 = r30.s64 - r28.s64;
	// addi r27,r26,4
	r27.s64 = r26.s64 + 4;
	// cmplwi cr6,r27,4
	cr6.compare<uint32_t>(r27.u32, 4, xer);
	// blt cr6,0x823edeac
	if (cr6.getLT()) goto loc_823EDEAC;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823f7a98
	sub_823F7A98(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplw cr6,r29,r27
	cr6.compare<uint32_t>(r29.u32, r27.u32, xer);
	// bge cr6,0x823ede98
	if (!cr6.getLT()) goto loc_823EDE98;
	// cmplwi cr6,r29,2048
	cr6.compare<uint32_t>(r29.u32, 2048, xer);
	// mr r11,r29
	r11.u64 = r29.u64;
	// blt cr6,0x823ede50
	if (cr6.getLT()) goto loc_823EDE50;
	// li r11,2048
	r11.s64 = 2048;
loc_823EDE50:
	// add r4,r11,r29
	ctx.r4.u64 = r11.u64 + r29.u64;
	// cmplw cr6,r4,r29
	cr6.compare<uint32_t>(ctx.r4.u32, r29.u32, xer);
	// blt cr6,0x823ede6c
	if (cr6.getLT()) goto loc_823EDE6C;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823f78f0
	sub_823F78F0(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x823ede88
	if (!cr0.getEQ()) goto loc_823EDE88;
loc_823EDE6C:
	// addi r4,r29,16
	ctx.r4.s64 = r29.s64 + 16;
	// cmplw cr6,r4,r29
	cr6.compare<uint32_t>(ctx.r4.u32, r29.u32, xer);
	// blt cr6,0x823edeac
	if (cr6.getLT()) goto loc_823EDEAC;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x823f78f0
	sub_823F78F0(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x823edeac
	if (cr0.getEQ()) goto loc_823EDEAC;
loc_823EDE88:
	// stw r3,-15560(r25)
	PPC_STORE_U32(r25.u32 + -15560, ctx.r3.u32);
	// srawi r11,r26,2
	xer.ca = (r26.s32 < 0) & ((r26.u32 & 0x3) != 0);
	r11.s64 = r26.s32 >> 2;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
loc_823EDE98:
	// stw r23,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r23.u32);
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// stw r11,-15564(r24)
	PPC_STORE_U32(r24.u32 + -15564, r11.u32);
	// stw r23,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r23.u32);
	// b 0x823edeb4
	goto loc_823EDEB4;
loc_823EDEAC:
	// li r11,0
	r11.s64 = 0;
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
loc_823EDEB4:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r12,r31,176
	r12.s64 = r31.s64 + 176;
	// bl 0x823edecc
	sub_823EDECC(ctx, base);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// addi r1,r31,176
	ctx.r1.s64 = r31.s64 + 176;
	// b 0x823ed174
	return;
}

__attribute__((alias("__imp__sub_823EDECC"))) PPC_WEAK_FUNC(sub_823EDECC);
PPC_FUNC_IMPL(__imp__sub_823EDECC) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x823ef658
	sub_823EF658(ctx, base);
	// lwz r1,0(r1)
	ctx.r1.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDEF0"))) PPC_WEAK_FUNC(sub_823EDEF0);
PPC_FUNC_IMPL(__imp__sub_823EDEF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x823edde8
	sub_823EDDE8(ctx, base);
	// cntlzw r11,r3
	r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// neg r3,r11
	ctx.r3.s64 = -r11.s64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDF20"))) PPC_WEAK_FUNC(sub_823EDF20);
PPC_FUNC_IMPL(__imp__sub_823EDF20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31987
	r11.s64 = -2096300032;
	// lwz r11,-20032(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -20032);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x823edf44
	if (cr0.getEQ()) goto loc_823EDF44;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_823EDF44:
	// li r3,25
	ctx.r3.s64 = 25;
	// bl 0x823f4d50
	sub_823F4D50(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f7bb8
	sub_823F7BB8(ctx, base);
	// bl 0x823f7b08
	sub_823F7B08(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EDF70"))) PPC_WEAK_FUNC(sub_823EDF70);
PPC_FUNC_IMPL(__imp__sub_823EDF70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r0{};
	// addi r0,r5,1
	r0.s64 = ctx.r5.s64 + 1;
	// mtctr r0
	ctr.u64 = r0.u64;
	// ori r6,r3,0
	ctx.r6.u64 = ctx.r3.u64 | 0;
	// b 0x823edf8c
	goto loc_823EDF8C;
loc_823EDF80:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// stb r4,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r4.u8);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
loc_823EDF8C:
	// andi. r0,r6,3
	r0.u64 = ctx.r6.u64 & 3;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// bdnzf eq,0x823edf80
	--ctr.u64;
	if (ctr.u32 != 0 && !cr0.getEQ()) goto loc_823EDF80;
	// rlwimi r4,r4,8,16,23
	ctx.r4.u64 = (__builtin_rotateleft32(ctx.r4.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// rlwinm. r0,r5,28,4,31
	r0.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 28) & 0xFFFFFFF;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// rlwimi r4,r4,16,0,15
	ctx.r4.u64 = (__builtin_rotateleft32(ctx.r4.u32, 16) & 0xFFFF0000) | (ctx.r4.u64 & 0xFFFFFFFF0000FFFF);
	// beq+ 0x823edfc0
	if (cr0.getEQ()) goto loc_823EDFC0;
	// mtctr r0
	ctr.u64 = r0.u64;
loc_823EDFA8:
	// stw r4,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r4.u32);
	// stw r4,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r4.u32);
	// stw r4,8(r6)
	PPC_STORE_U32(ctx.r6.u32 + 8, ctx.r4.u32);
	// stw r4,12(r6)
	PPC_STORE_U32(ctx.r6.u32 + 12, ctx.r4.u32);
	// addi r6,r6,16
	ctx.r6.s64 = ctx.r6.s64 + 16;
	// bdnz+ 0x823edfa8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EDFA8;
loc_823EDFC0:
	// rlwinm. r0,r5,30,30,31
	r0.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// beq- 0x823edfec
	if (cr0.getEQ()) goto loc_823EDFEC;
	// mtctr r0
	ctr.u64 = r0.u64;
	// stw r4,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r4.u32);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bdz- 0x823edfec
	--ctr.u64;
	if (ctr.u32 == 0) goto loc_823EDFEC;
	// stw r4,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r4.u32);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bdz- 0x823edfec
	--ctr.u64;
	if (ctr.u32 == 0) goto loc_823EDFEC;
	// stw r4,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r4.u32);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
loc_823EDFEC:
	// andi. r0,r5,3
	r0.u64 = ctx.r5.u64 & 3;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// mtctr r0
	ctr.u64 = r0.u64;
	// beqlr+ 
	if (cr0.getEQ()) return;
	// stb r4,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r4.u8);
	// bdzlr- 
	--ctr.u64;
	if (ctr.u32 == 0) return;
}

__attribute__((alias("__imp__sub_823EE000"))) PPC_WEAK_FUNC(sub_823EE000);
PPC_FUNC_IMPL(__imp__sub_823EE000) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	// stb r4,1(r6)
	PPC_STORE_U8(ctx.r6.u32 + 1, ctx.r4.u8);
	// bdzlr- 
	--ctr.u64;
	if (ctr.u32 == 0) return;
}

__attribute__((alias("__imp__sub_823EE008"))) PPC_WEAK_FUNC(sub_823EE008);
PPC_FUNC_IMPL(__imp__sub_823EE008) {
	PPC_FUNC_PROLOGUE();
	// stb r4,2(r6)
	PPC_STORE_U8(ctx.r6.u32 + 2, ctx.r4.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EE010"))) PPC_WEAK_FUNC(sub_823EE010);
PPC_FUNC_IMPL(__imp__sub_823EE010) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr1{};
	PPCCRRegister cr6{};
	PPCCRRegister cr7{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// std r3,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r3.u64);
	// clrlwi r6,r3,29
	ctx.r6.u64 = ctx.r3.u32 & 0x7;
	// dcbt r0,r4
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// subfic r6,r6,8
	xer.ca = ctx.r6.u32 <= 8;
	ctx.r6.s64 = 8 - ctx.r6.s64;
	// beq 0x823ee074
	if (cr0.getEQ()) goto loc_823EE074;
	// cmplw r5,r6
	cr0.compare<uint32_t>(ctx.r5.u32, ctx.r6.u32, xer);
	// ble 0x823ee090
	if (!cr0.getGT()) goto loc_823EE090;
	// cmplwi r6,4
	cr0.compare<uint32_t>(ctx.r6.u32, 4, xer);
	// beq 0x823ee060
	if (cr0.getEQ()) goto loc_823EE060;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_823EE048:
	// lbzu r6,1(r4)
	ea = 1 + ctx.r4.u32;
	ctx.r6.u64 = PPC_LOAD_U8(ea);
	ctx.r4.u32 = ea;
	// stbu r6,1(r3)
	ea = 1 + ctx.r3.u32;
	PPC_STORE_U8(ea, ctx.r6.u8);
	ctx.r3.u32 = ea;
	// bdnz 0x823ee048
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE048;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// b 0x823ee074
	goto loc_823EE074;
loc_823EE060:
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
	// lwz r6,0(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// stw r6,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r6.u32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
loc_823EE074:
	// clrlwi r6,r4,29
	ctx.r6.u64 = ctx.r4.u32 & 0x7;
	// cmplwi cr6,r6,4
	cr6.compare<uint32_t>(ctx.r6.u32, 4, xer);
	// cmplwi cr1,r6,0
	cr1.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// cmplwi cr7,r5,128
	cr7.compare<uint32_t>(ctx.r5.u32, 128, xer);
	// beq cr6,0x823ee258
	if (cr6.getEQ()) goto loc_823EE258;
	// bne cr1,0x823ee388
	if (!cr1.getEQ()) goto loc_823EE388;
	// bge cr7,0x823ee12c
	if (!cr7.getLT()) goto loc_823EE12C;
loc_823EE090:
	// dcbtst r0,r3
	// addi r4,r4,-8
	ctx.r4.s64 = ctx.r4.s64 + -8;
	// addi r3,r3,-8
	ctx.r3.s64 = ctx.r3.s64 + -8;
loc_823EE09C:
	// rlwinm r7,r5,29,28,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0xF;
	// clrlwi r6,r5,29
	ctx.r6.u64 = ctx.r5.u32 & 0x7;
	// cmplwi cr1,r7,0
	cr1.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr1,0x823ee0c0
	if (cr1.getEQ()) goto loc_823EE0C0;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_823EE0B4:
	// ldu r7,8(r4)
	ea = 8 + ctx.r4.u32;
	ctx.r7.u64 = PPC_LOAD_U64(ea);
	ctx.r4.u32 = ea;
	// stdu r7,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U64(ea, ctx.r7.u64);
	ctx.r3.u32 = ea;
	// bdnz 0x823ee0b4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE0B4;
loc_823EE0C0:
	// cmplwi cr1,r6,4
	cr1.compare<uint32_t>(ctx.r6.u32, 4, xer);
	// beq cr6,0x823ee0e4
	if (cr6.getEQ()) goto loc_823EE0E4;
	// beq cr1,0x823ee0ec
	if (cr1.getEQ()) goto loc_823EE0EC;
	// addi r3,r3,7
	ctx.r3.s64 = ctx.r3.s64 + 7;
	// addi r4,r4,7
	ctx.r4.s64 = ctx.r4.s64 + 7;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_823EE0D8:
	// lbzu r7,1(r4)
	ea = 1 + ctx.r4.u32;
	ctx.r7.u64 = PPC_LOAD_U8(ea);
	ctx.r4.u32 = ea;
	// stbu r7,1(r3)
	ea = 1 + ctx.r3.u32;
	PPC_STORE_U8(ea, ctx.r7.u8);
	ctx.r3.u32 = ea;
	// bdnz 0x823ee0d8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE0D8;
loc_823EE0E4:
	// ld r3,-8(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_823EE0EC:
	// clrlwi r6,r3,30
	ctx.r6.u64 = ctx.r3.u32 & 0x3;
	// lwz r5,8(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne 0x823ee108
	if (!cr0.getEQ()) goto loc_823EE108;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// ld r3,-8(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_823EE108:
	// lbz r8,8(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 8);
	// lbz r7,9(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 9);
	// lbz r6,10(r4)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r4.u32 + 10);
	// stb r8,8(r3)
	PPC_STORE_U8(ctx.r3.u32 + 8, ctx.r8.u8);
	// stb r7,9(r3)
	PPC_STORE_U8(ctx.r3.u32 + 9, ctx.r7.u8);
	// stb r6,10(r3)
	PPC_STORE_U8(ctx.r3.u32 + 10, ctx.r6.u8);
	// stb r5,11(r3)
	PPC_STORE_U8(ctx.r3.u32 + 11, ctx.r5.u8);
	// ld r3,-8(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_823EE12C:
	// clrlwi r6,r3,25
	ctx.r6.u64 = ctx.r3.u32 & 0x7F;
	// addi r3,r3,-8
	ctx.r3.s64 = ctx.r3.s64 + -8;
	// addi r4,r4,-8
	ctx.r4.s64 = ctx.r4.s64 + -8;
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// subfic r6,r6,128
	xer.ca = ctx.r6.u32 <= 128;
	ctx.r6.s64 = 128 - ctx.r6.s64;
	// beq 0x823ee15c
	if (cr0.getEQ()) goto loc_823EE15C;
	// rlwinm r7,r6,29,3,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFF;
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_823EE150:
	// ldu r7,8(r4)
	ea = 8 + ctx.r4.u32;
	ctx.r7.u64 = PPC_LOAD_U64(ea);
	ctx.r4.u32 = ea;
	// stdu r7,8(r3)
	ea = 8 + ctx.r3.u32;
	PPC_STORE_U64(ea, ctx.r7.u64);
	ctx.r3.u32 = ea;
	// bdnz 0x823ee150
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE150;
loc_823EE15C:
	// rlwinm r6,r5,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq 0x823ee09c
	if (cr0.getEQ()) goto loc_823EE09C;
	// addi r10,r5,127
	ctx.r10.s64 = ctx.r5.s64 + 127;
	// clrlwi r8,r5,25
	ctx.r8.u64 = ctx.r5.u32 & 0x7F;
	// rlwinm r10,r10,25,7,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr1,r8,0
	cr1.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_823EE18C:
	// dcbt r9,r4
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// bdnz 0x823ee18c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE18C;
	// add r12,r4,r5
	r12.u64 = ctx.r4.u64 + ctx.r5.u64;
	// li r10,8
	ctx.r10.s64 = 8;
	// subf r11,r9,r12
	r11.s64 = r12.s64 - ctx.r9.s64;
	// add r12,r3,r5
	r12.u64 = ctx.r3.u64 + ctx.r5.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_823EE1AC:
	// ld r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r4.u32 + 8);
	// ld r7,16(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 16);
	// ld r8,24(r4)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r4.u32 + 24);
	// std r6,8(r3)
	PPC_STORE_U64(ctx.r3.u32 + 8, ctx.r6.u64);
	// ld r6,32(r4)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r4.u32 + 32);
	// std r7,16(r3)
	PPC_STORE_U64(ctx.r3.u32 + 16, ctx.r7.u64);
	// ld r7,40(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 40);
	// std r8,24(r3)
	PPC_STORE_U64(ctx.r3.u32 + 24, ctx.r8.u64);
	// ld r8,48(r4)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r4.u32 + 48);
	// std r6,32(r3)
	PPC_STORE_U64(ctx.r3.u32 + 32, ctx.r6.u64);
	// ld r6,56(r4)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r4.u32 + 56);
	// std r7,40(r3)
	PPC_STORE_U64(ctx.r3.u32 + 40, ctx.r7.u64);
	// ld r7,64(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 64);
	// std r8,48(r3)
	PPC_STORE_U64(ctx.r3.u32 + 48, ctx.r8.u64);
	// ld r8,72(r4)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r4.u32 + 72);
	// std r6,56(r3)
	PPC_STORE_U64(ctx.r3.u32 + 56, ctx.r6.u64);
	// ld r6,80(r4)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r4.u32 + 80);
	// std r7,64(r3)
	PPC_STORE_U64(ctx.r3.u32 + 64, ctx.r7.u64);
	// ld r7,88(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 88);
	// std r8,72(r3)
	PPC_STORE_U64(ctx.r3.u32 + 72, ctx.r8.u64);
	// ld r8,96(r4)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r4.u32 + 96);
	// std r6,80(r3)
	PPC_STORE_U64(ctx.r3.u32 + 80, ctx.r6.u64);
	// ld r6,104(r4)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r4.u32 + 104);
	// std r7,88(r3)
	PPC_STORE_U64(ctx.r3.u32 + 88, ctx.r7.u64);
	// ld r7,112(r4)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r4.u32 + 112);
	// std r8,96(r3)
	PPC_STORE_U64(ctx.r3.u32 + 96, ctx.r8.u64);
	// ld r8,120(r4)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r4.u32 + 120);
	// std r6,104(r3)
	PPC_STORE_U64(ctx.r3.u32 + 104, ctx.r6.u64);
	// ldu r6,128(r4)
	ea = 128 + ctx.r4.u32;
	ctx.r6.u64 = PPC_LOAD_U64(ea);
	ctx.r4.u32 = ea;
	// std r7,112(r3)
	PPC_STORE_U64(ctx.r3.u32 + 112, ctx.r7.u64);
	// std r8,120(r3)
	PPC_STORE_U64(ctx.r3.u32 + 120, ctx.r8.u64);
	// stdu r6,128(r3)
	ea = 128 + ctx.r3.u32;
	PPC_STORE_U64(ea, ctx.r6.u64);
	ctx.r3.u32 = ea;
	// cmplw r4,r11
	cr0.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// bge 0x823ee240
	if (!cr0.getLT()) goto loc_823EE240;
	// dcbt r9,r4
	// bdnz 0x823ee1ac
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE1AC;
	// b 0x823ee09c
	goto loc_823EE09C;
loc_823EE240:
	// beq cr1,0x823ee250
	if (cr1.getEQ()) goto loc_823EE250;
	// li r8,-1
	ctx.r8.s64 = -1;
	// dcbtst r8,r12
	// cmplwi cr1,r8,0
	cr1.compare<uint32_t>(ctx.r8.u32, 0, xer);
loc_823EE250:
	// bdnz 0x823ee1ac
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE1AC;
	// b 0x823ee09c
	goto loc_823EE09C;
loc_823EE258:
	// addi r4,r4,-4
	ctx.r4.s64 = ctx.r4.s64 + -4;
	// bge cr7,0x823ee2b0
	if (!cr7.getLT()) goto loc_823EE2B0;
	// dcbtst r0,r3
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
loc_823EE268:
	// rlwinm r7,r5,30,27,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x1F;
	// clrlwi r6,r5,30
	ctx.r6.u64 = ctx.r5.u32 & 0x3;
	// cmplwi cr1,r7,0
	cr1.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr1,0x823ee28c
	if (cr1.getEQ()) goto loc_823EE28C;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_823EE280:
	// lwzu r7,4(r4)
	ea = 4 + ctx.r4.u32;
	ctx.r7.u64 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// bdnz 0x823ee280
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE280;
loc_823EE28C:
	// beq cr6,0x823ee2a8
	if (cr6.getEQ()) goto loc_823EE2A8;
	// addi r3,r3,3
	ctx.r3.s64 = ctx.r3.s64 + 3;
	// addi r4,r4,3
	ctx.r4.s64 = ctx.r4.s64 + 3;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_823EE29C:
	// lbzu r7,1(r4)
	ea = 1 + ctx.r4.u32;
	ctx.r7.u64 = PPC_LOAD_U8(ea);
	ctx.r4.u32 = ea;
	// stbu r7,1(r3)
	ea = 1 + ctx.r3.u32;
	PPC_STORE_U8(ea, ctx.r7.u8);
	ctx.r3.u32 = ea;
	// bdnz 0x823ee29c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE29C;
loc_823EE2A8:
	// ld r3,-8(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_823EE2B0:
	// clrlwi r6,r3,25
	ctx.r6.u64 = ctx.r3.u32 & 0x7F;
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// subfic r6,r6,128
	xer.ca = ctx.r6.u32 <= 128;
	ctx.r6.s64 = 128 - ctx.r6.s64;
	// beq 0x823ee2dc
	if (cr0.getEQ()) goto loc_823EE2DC;
	// rlwinm r7,r6,30,2,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_823EE2D0:
	// lwzu r7,4(r4)
	ea = 4 + ctx.r4.u32;
	ctx.r7.u64 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// bdnz 0x823ee2d0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE2D0;
loc_823EE2DC:
	// rlwinm r6,r5,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq 0x823ee268
	if (cr0.getEQ()) goto loc_823EE268;
	// addi r10,r5,127
	ctx.r10.s64 = ctx.r5.s64 + 127;
	// clrlwi r8,r5,25
	ctx.r8.u64 = ctx.r5.u32 & 0x7F;
	// rlwinm r10,r10,25,7,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr1,r8,0
	cr1.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r9,4
	ctx.r9.s64 = 4;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_823EE30C:
	// dcbt r9,r4
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// bdnz 0x823ee30c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE30C;
	// add r12,r4,r5
	r12.u64 = ctx.r4.u64 + ctx.r5.u64;
	// li r10,8
	ctx.r10.s64 = 8;
	// subf r11,r9,r12
	r11.s64 = r12.s64 - ctx.r9.s64;
	// add r12,r3,r5
	r12.u64 = ctx.r3.u64 + ctx.r5.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_823EE32C:
	// li r6,8
	ctx.r6.s64 = 8;
loc_823EE330:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lwz r0,4(r4)
	r0.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r7,8(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r8,12(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// stw r0,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r0.u32);
	// lwzu r0,16(r4)
	ea = 16 + ctx.r4.u32;
	r0.u64 = PPC_LOAD_U32(ea);
	ctx.r4.u32 = ea;
	// stw r7,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r7.u32);
	// stw r8,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r8.u32);
	// stwu r0,16(r3)
	ea = 16 + ctx.r3.u32;
	PPC_STORE_U32(ea, r0.u32);
	ctx.r3.u32 = ea;
	// bne 0x823ee330
	if (!cr0.getEQ()) goto loc_823EE330;
	// cmplw r4,r11
	cr0.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// bge 0x823ee370
	if (!cr0.getLT()) goto loc_823EE370;
	// dcbt r9,r4
	// bdnz 0x823ee32c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE32C;
	// b 0x823ee268
	goto loc_823EE268;
loc_823EE370:
	// beq cr1,0x823ee380
	if (cr1.getEQ()) goto loc_823EE380;
	// li r8,-1
	ctx.r8.s64 = -1;
	// dcbtst r8,r12
	// cmplwi cr1,r8,0
	cr1.compare<uint32_t>(ctx.r8.u32, 0, xer);
loc_823EE380:
	// bdnz 0x823ee32c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE32C;
	// b 0x823ee268
	goto loc_823EE268;
loc_823EE388:
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// bge cr7,0x823ee3bc
	if (!cr7.getLT()) goto loc_823EE3BC;
	// dcbtst r0,r3
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
loc_823EE398:
	// clrlwi r6,r5,25
	ctx.r6.u64 = ctx.r5.u32 & 0x7F;
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// beq 0x823ee3b4
	if (cr0.getEQ()) goto loc_823EE3B4;
loc_823EE3A8:
	// lbzu r6,1(r4)
	ea = 1 + ctx.r4.u32;
	ctx.r6.u64 = PPC_LOAD_U8(ea);
	ctx.r4.u32 = ea;
	// stbu r6,1(r3)
	ea = 1 + ctx.r3.u32;
	PPC_STORE_U8(ea, ctx.r6.u8);
	ctx.r3.u32 = ea;
	// bdnz 0x823ee3a8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE3A8;
loc_823EE3B4:
	// ld r3,-8(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_823EE3BC:
	// clrlwi r6,r3,25
	ctx.r6.u64 = ctx.r3.u32 & 0x7F;
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// subfic r6,r6,128
	xer.ca = ctx.r6.u32 <= 128;
	ctx.r6.s64 = 128 - ctx.r6.s64;
	// beq 0x823ee3e4
	if (cr0.getEQ()) goto loc_823EE3E4;
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_823EE3D8:
	// lbzu r6,1(r4)
	ea = 1 + ctx.r4.u32;
	ctx.r6.u64 = PPC_LOAD_U8(ea);
	ctx.r4.u32 = ea;
	// stbu r6,1(r3)
	ea = 1 + ctx.r3.u32;
	PPC_STORE_U8(ea, ctx.r6.u8);
	ctx.r3.u32 = ea;
	// bdnz 0x823ee3d8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE3D8;
loc_823EE3E4:
	// rlwinm r6,r5,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq 0x823ee398
	if (cr0.getEQ()) goto loc_823EE398;
	// addi r10,r5,127
	ctx.r10.s64 = ctx.r5.s64 + 127;
	// clrlwi r8,r5,25
	ctx.r8.u64 = ctx.r5.u32 & 0x7F;
	// rlwinm r10,r10,25,7,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr1,r8,0
	cr1.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r9,1
	ctx.r9.s64 = 1;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_823EE414:
	// dcbt r9,r4
	// addi r9,r9,128
	ctx.r9.s64 = ctx.r9.s64 + 128;
	// bdnz 0x823ee414
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE414;
	// add r12,r4,r5
	r12.u64 = ctx.r4.u64 + ctx.r5.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// subf r11,r9,r12
	r11.s64 = r12.s64 - ctx.r9.s64;
	// add r12,r3,r5
	r12.u64 = ctx.r3.u64 + ctx.r5.u64;
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
loc_823EE434:
	// li r6,32
	ctx.r6.s64 = 32;
loc_823EE438:
	// lbz r7,4(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// lbz r8,3(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 3);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// rlwimi r7,r8,8,16,23
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 8) & 0xFF00) | (ctx.r7.u64 & 0xFFFFFFFFFFFF00FF);
	// lbz r9,2(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// cmplwi r6,0
	cr0.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// rlwimi r7,r9,16,8,15
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFF0000) | (ctx.r7.u64 & 0xFFFFFFFFFF00FFFF);
	// lbz r10,1(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// rlwimi r7,r10,24,0,7
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r10.u32, 24) & 0xFF000000) | (ctx.r7.u64 & 0xFFFFFFFF00FFFFFF);
	// stw r7,1(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1, ctx.r7.u32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// bne 0x823ee438
	if (!cr0.getEQ()) goto loc_823EE438;
	// cmplw r4,r11
	cr0.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// bge 0x823ee480
	if (!cr0.getLT()) goto loc_823EE480;
	// dcbt r9,r4
	// bdnz 0x823ee434
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE434;
	// b 0x823ee398
	goto loc_823EE398;
loc_823EE480:
	// beq cr1,0x823ee490
	if (cr1.getEQ()) goto loc_823EE490;
	// li r8,-1
	ctx.r8.s64 = -1;
	// dcbtst r8,r12
	// cmplwi cr1,r8,0
	cr1.compare<uint32_t>(ctx.r8.u32, 0, xer);
loc_823EE490:
	// bdnz 0x823ee434
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE434;
	// b 0x823ee398
	goto loc_823EE398;
}

__attribute__((alias("__imp__sub_823EE498"))) PPC_WEAK_FUNC(sub_823EE498);
PPC_FUNC_IMPL(__imp__sub_823EE498) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r0{};
	// addi r0,r5,1
	r0.s64 = ctx.r5.s64 + 1;
	// ori r6,r3,0
	ctx.r6.u64 = ctx.r3.u64 | 0;
	// mtctr r0
	ctr.u64 = r0.u64;
	// b 0x823ee4bc
	goto loc_823EE4BC;
loc_823EE4A8:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbz r0,0(r4)
	r0.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stb r0,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, r0.u8);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
loc_823EE4BC:
	// andi. r0,r6,3
	r0.u64 = ctx.r6.u64 & 3;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// bdnzf eq,0x823ee4a8
	--ctr.u64;
	if (ctr.u32 != 0 && !cr0.getEQ()) goto loc_823EE4A8;
	// rlwinm. r0,r5,30,2,31
	r0.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// beq- 0x823ee4ec
	if (cr0.getEQ()) goto loc_823EE4EC;
	// mtctr r0
	ctr.u64 = r0.u64;
	// andi. r0,r4,3
	r0.u64 = ctx.r4.u64 & 3;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// bne- 0x823ee510
	if (!cr0.getEQ()) goto loc_823EE510;
loc_823EE4D8:
	// lwz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// stw r7,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r7.u32);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bdnz+ 0x823ee4d8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE4D8;
loc_823EE4EC:
	// andi. r0,r5,3
	r0.u64 = ctx.r5.u64 & 3;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// mtctr r0
	ctr.u64 = r0.u64;
	// beqlr+ 
	if (cr0.getEQ()) return;
loc_823EE4F8:
	// lbz r0,0(r4)
	r0.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stb r0,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, r0.u8);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// bdnz+ 0x823ee4f8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE4F8;
	// blr 
	return;
loc_823EE510:
	// lbz r7,3(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 3);
	// lbz r8,2(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// rlwimi r7,r8,8,16,23
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 8) & 0xFF00) | (ctx.r7.u64 & 0xFFFFFFFFFFFF00FF);
	// lbz r9,1(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// rlwimi r7,r9,16,8,15
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFF0000) | (ctx.r7.u64 & 0xFFFFFFFFFF00FFFF);
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rlwimi r7,r10,24,0,7
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r10.u32, 24) & 0xFF000000) | (ctx.r7.u64 & 0xFFFFFFFF00FFFFFF);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// stw r7,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r7.u32);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bdnz 0x823ee510
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EE510;
	// b 0x823ee4ec
	goto loc_823EE4EC;
}

__attribute__((alias("__imp__sub_823EE540"))) PPC_WEAK_FUNC(sub_823EE540);
PPC_FUNC_IMPL(__imp__sub_823EE540) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x823ee560
	if (cr6.getEQ()) goto loc_823EE560;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x823ee590
	if (!cr6.getEQ()) goto loc_823EE590;
loc_823EE560:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,22
	ctx.r3.s64 = 22;
	// b 0x823ee610
	goto loc_823EE610;
loc_823EE590:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x823ee5cc
	if (!cr6.getEQ()) goto loc_823EE5CC;
	// li r11,0
	r11.s64 = 0;
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r31,22
	r31.s64 = 22;
loc_823EE5A8:
	// stw r31,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r31.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// b 0x823ee610
	goto loc_823EE610;
loc_823EE5CC:
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_823EE5D0:
	// lbz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// beq 0x823ee5f0
	if (cr0.getEQ()) goto loc_823EE5F0;
	// addic. r4,r4,-1
	xer.ca = ctx.r4.u32 > 0;
	ctx.r4.s64 = ctx.r4.s64 + -1;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne 0x823ee5d0
	if (!cr0.getEQ()) goto loc_823EE5D0;
loc_823EE5F0:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x823ee60c
	if (!cr6.getEQ()) goto loc_823EE60C;
	// li r11,0
	r11.s64 = 0;
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r31,34
	r31.s64 = 34;
	// b 0x823ee5a8
	goto loc_823EE5A8;
loc_823EE60C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_823EE610:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EE630"))) PPC_WEAK_FUNC(sub_823EE630);
PPC_FUNC_IMPL(__imp__sub_823EE630) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr1{};
	PPCRegister r0{};
	uint32_t ea{};
	// mr. r0,r5
	r0.u64 = ctx.r5.u64;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// mtctr r5
	ctr.u64 = ctx.r5.u64;
	// ble 0x823ee674
	if (!cr0.getGT()) goto loc_823EE674;
	// lbz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr1,r8,0
	cr1.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subfc. r3,r7,r8
	xer.ca = ctx.r8.u32 >= ctx.r7.u32;
	ctx.r3.s64 = ctx.r8.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bdnzf 4*cr1+eq,0x823ee658
	--ctr.u64;
	if (ctr.u32 != 0 && !cr1.getEQ()) goto loc_823EE658;
	// blr 
	return;
loc_823EE658:
	// bnelr 
	if (!cr0.getEQ()) return;
	// lbzu r8,1(r10)
	ea = 1 + ctx.r10.u32;
	ctx.r8.u64 = PPC_LOAD_U8(ea);
	ctx.r10.u32 = ea;
	// lbzu r7,1(r4)
	ea = 1 + ctx.r4.u32;
	ctx.r7.u64 = PPC_LOAD_U8(ea);
	ctx.r4.u32 = ea;
	// cmpwi cr1,r8,0
	cr1.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subfc. r3,r7,r8
	xer.ca = ctx.r8.u32 >= ctx.r7.u32;
	ctx.r3.s64 = ctx.r8.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bdnzf 4*cr1+eq,0x823ee658
	--ctr.u64;
	if (ctr.u32 != 0 && !cr1.getEQ()) goto loc_823EE658;
	// blr 
	return;
loc_823EE674:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EE680"))) PPC_WEAK_FUNC(sub_823EE680);
PPC_FUNC_IMPL(__imp__sub_823EE680) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x823f2960
	sub_823F2960(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lis r10,3
	ctx.r10.s64 = 196608;
	// ori r10,r10,17405
	ctx.r10.u64 = ctx.r10.u64 | 17405;
	// lwz r9,20(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addis r10,r10,39
	ctx.r10.s64 = ctx.r10.s64 + 2555904;
	// addi r10,r10,-24893
	ctx.r10.s64 = ctx.r10.s64 + -24893;
	// rlwinm r3,r10,16,17,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0x7FFF;
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EE6C8"))) PPC_WEAK_FUNC(sub_823EE6C8);
PPC_FUNC_IMPL(__imp__sub_823EE6C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x823ee70c
	if (!cr6.getEQ()) goto loc_823EE70C;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x823ee714
	if (!cr6.getEQ()) goto loc_823EE714;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x823ee71c
	if (!cr6.getEQ()) goto loc_823EE71C;
loc_823EE6F4:
	// li r3,0
	ctx.r3.s64 = 0;
loc_823EE6F8:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_823EE70C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x823ee71c
	if (cr6.getEQ()) goto loc_823EE71C;
loc_823EE714:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x823ee74c
	if (!cr6.getEQ()) goto loc_823EE74C;
loc_823EE71C:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,22
	ctx.r3.s64 = 22;
	// b 0x823ee6f8
	goto loc_823EE6F8;
loc_823EE74C:
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x823ee760
	if (!cr6.getEQ()) goto loc_823EE760;
	// li r11,0
	r11.s64 = 0;
	// sth r11,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r11.u16);
	// b 0x823ee6f4
	goto loc_823EE6F4;
loc_823EE760:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x823ee79c
	if (!cr6.getEQ()) goto loc_823EE79C;
	// li r11,0
	r11.s64 = 0;
	// sth r11,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, r11.u16);
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r31,22
	r31.s64 = 22;
loc_823EE778:
	// stw r31,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r31.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// b 0x823ee6f8
	goto loc_823EE6F8;
loc_823EE79C:
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r6,-1
	cr6.compare<int32_t>(ctx.r6.s32, -1, xer);
	// bne cr6,0x823ee7d4
	if (!cr6.getEQ()) goto loc_823EE7D4;
loc_823EE7B0:
	// lhz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// beq 0x823ee808
	if (cr0.getEQ()) goto loc_823EE808;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x823ee7b0
	if (!cr0.getEQ()) goto loc_823EE7B0;
	// b 0x823ee808
	goto loc_823EE808;
loc_823EE7D4:
	// lhz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// beq 0x823ee7fc
	if (cr0.getEQ()) goto loc_823EE7FC;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x823ee7fc
	if (cr0.getEQ()) goto loc_823EE7FC;
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// bne 0x823ee7d4
	if (!cr0.getEQ()) goto loc_823EE7D4;
loc_823EE7FC:
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x823ee808
	if (!cr6.getEQ()) goto loc_823EE808;
	// sth r7,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r7.u16);
loc_823EE808:
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x823ee6f4
	if (!cr6.getEQ()) goto loc_823EE6F4;
	// cmpwi cr6,r6,-1
	cr6.compare<int32_t>(ctx.r6.s32, -1, xer);
	// bne cr6,0x823ee82c
	if (!cr6.getEQ()) goto loc_823EE82C;
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// li r3,80
	ctx.r3.s64 = 80;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// sth r7,-2(r11)
	PPC_STORE_U16(r11.u32 + -2, ctx.r7.u16);
	// b 0x823ee6f8
	goto loc_823EE6F8;
loc_823EE82C:
	// sth r7,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r7.u16);
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r31,34
	r31.s64 = 34;
	// b 0x823ee778
	goto loc_823EE778;
}

__attribute__((alias("__imp__sub_823EE840"))) PPC_WEAK_FUNC(sub_823EE840);
PPC_FUNC_IMPL(__imp__sub_823EE840) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x823ee864
	if (cr6.getEQ()) goto loc_823EE864;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x823ee894
	if (!cr6.getEQ()) goto loc_823EE894;
loc_823EE864:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,22
	ctx.r3.s64 = 22;
	// b 0x823ee940
	goto loc_823EE940;
loc_823EE894:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x823ee8a8
	if (!cr6.getEQ()) goto loc_823EE8A8;
	// li r11,0
	r11.s64 = 0;
	// sth r11,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r11.u16);
	// b 0x823ee864
	goto loc_823EE864;
loc_823EE8A8:
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_823EE8AC:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq 0x823ee8c4
	if (cr0.getEQ()) goto loc_823EE8C4;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bne 0x823ee8ac
	if (!cr0.getEQ()) goto loc_823EE8AC;
loc_823EE8C4:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x823ee900
	if (!cr6.getEQ()) goto loc_823EE900;
	// li r11,0
	r11.s64 = 0;
	// sth r11,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r11.u16);
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r31,22
	r31.s64 = 22;
loc_823EE8DC:
	// stw r31,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r31.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// b 0x823ee940
	goto loc_823EE940;
loc_823EE900:
	// lhz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r5.u32 + 0);
	// addi r5,r5,2
	ctx.r5.s64 = ctx.r5.s64 + 2;
	// cmplwi r9,0
	cr0.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// beq 0x823ee920
	if (cr0.getEQ()) goto loc_823EE920;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne 0x823ee900
	if (!cr0.getEQ()) goto loc_823EE900;
loc_823EE920:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x823ee93c
	if (!cr6.getEQ()) goto loc_823EE93C;
	// li r11,0
	r11.s64 = 0;
	// sth r11,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, r11.u16);
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r31,34
	r31.s64 = 34;
	// b 0x823ee8dc
	goto loc_823EE8DC;
loc_823EE93C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_823EE940:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EE958"))) PPC_WEAK_FUNC(sub_823EE958);
PPC_FUNC_IMPL(__imp__sub_823EE958) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister f0{};
	// fctidz f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f1.f64));
	// lis r11,-32249
	r11.s64 = -2113470464;
	// fabs f13,f1
	ctx.f13.u64 = ctx.f1.u64 & ~0x8000000000000000;
	// lfd f12,8808(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 8808);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fsub f12,f12,f13
	ctx.f12.f64 = ctx.f12.f64 - ctx.f13.f64;
	// fneg f11,f13
	ctx.f11.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfd f13,2728(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 2728);
	// fsub f10,f0,f1
	ctx.f10.f64 = f0.f64 - ctx.f1.f64;
	// fadd f13,f0,f13
	ctx.f13.f64 = f0.f64 + ctx.f13.f64;
	// fsel f0,f10,f0,f13
	f0.f64 = ctx.f10.f64 >= 0.0 ? f0.f64 : ctx.f13.f64;
	// fsel f0,f12,f0,f1
	f0.f64 = ctx.f12.f64 >= 0.0 ? f0.f64 : ctx.f1.f64;
	// fsel f1,f11,f1,f0
	ctx.f1.f64 = ctx.f11.f64 >= 0.0 ? ctx.f1.f64 : f0.f64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EE998"))) PPC_WEAK_FUNC(sub_823EE998);
PPC_FUNC_IMPL(__imp__sub_823EE998) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// std r5,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r5.u64);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x823ee9fc
	if (!cr6.getEQ()) goto loc_823EE9FC;
loc_823EE9CC:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x823eea70
	goto loc_823EEA70;
loc_823EE9FC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x823ee9cc
	if (cr6.getEQ()) goto loc_823EE9CC;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// lis r8,32767
	ctx.r8.s64 = 2147418112;
	// li r5,0
	ctx.r5.s64 = 0;
	// ori r8,r8,65535
	ctx.r8.u64 = ctx.r8.u64 | 65535;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// li r10,66
	ctx.r10.s64 = 66;
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r8,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r8.u32);
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// bl 0x823f52b0
	sub_823F52B0(ctx, base);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// blt 0x823eea60
	if (cr0.getLT()) goto loc_823EEA60;
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// li r11,0
	r11.s64 = 0;
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// b 0x823eea6c
	goto loc_823EEA6C;
loc_823EEA60:
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f5e28
	sub_823F5E28(ctx, base);
loc_823EEA6C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_823EEA70:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EEA88"))) PPC_WEAK_FUNC(sub_823EEA88);
PPC_FUNC_IMPL(__imp__sub_823EEA88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r6,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r6.u64);
	// std r7,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r7.u64);
	// std r8,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.r8.u64);
	// std r9,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.r9.u64);
	// std r10,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.r10.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,136
	ctx.r10.s64 = ctx.r1.s64 + 136;
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x823f7d08
	sub_823F7D08(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EEAD0"))) PPC_WEAK_FUNC(sub_823EEAD0);
PPC_FUNC_IMPL(__imp__sub_823EEAD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed124
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x823eeaf8
	if (cr6.getEQ()) goto loc_823EEAF8;
	// stw r25,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r25.u32);
loc_823EEAF8:
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x823eeb2c
	if (!cr6.getEQ()) goto loc_823EEB2C;
loc_823EEB00:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// b 0x823eedb8
	goto loc_823EEDB8;
loc_823EEB2C:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x823eeb44
	if (cr6.getEQ()) goto loc_823EEB44;
	// cmpwi cr6,r28,2
	cr6.compare<int32_t>(r28.s32, 2, xer);
	// blt cr6,0x823eeb00
	if (cr6.getLT()) goto loc_823EEB00;
	// cmpwi cr6,r28,36
	cr6.compare<int32_t>(r28.s32, 36, xer);
	// bgt cr6,0x823eeb00
	if (cr6.getGT()) goto loc_823EEB00;
loc_823EEB44:
	// lis r11,-32015
	r11.s64 = -2098135040;
	// lbz r31,0(r25)
	r31.u64 = PPC_LOAD_U8(r25.u32 + 0);
	// li r26,0
	r26.s64 = 0;
	// addi r30,r11,-19456
	r30.s64 = r11.s64 + -19456;
	// addi r29,r25,1
	r29.s64 = r25.s64 + 1;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
loc_823EEB5C:
	// lwz r11,172(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 172);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// ble cr6,0x823eeb80
	if (!cr6.getGT()) goto loc_823EEB80;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// clrlwi r3,r31,24
	ctx.r3.u64 = r31.u32 & 0xFF;
	// bl 0x823f7db8
	sub_823F7DB8(ctx, base);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// b 0x823eeb90
	goto loc_823EEB90;
loc_823EEB80:
	// lwz r11,200(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 200);
	// rlwinm r9,r31,1,23,30
	ctx.r9.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0x1FE;
	// lhzx r11,r9,r11
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + r11.u32);
	// rlwinm r3,r11,0,28,28
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
loc_823EEB90:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x823eeba4
	if (cr6.getEQ()) goto loc_823EEBA4;
	// lbz r31,0(r29)
	r31.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// b 0x823eeb5c
	goto loc_823EEB5C;
loc_823EEBA4:
	// extsb r11,r31
	r11.s64 = r31.s8;
	// cmpwi cr6,r11,45
	cr6.compare<int32_t>(r11.s32, 45, xer);
	// bne cr6,0x823eebb8
	if (!cr6.getEQ()) goto loc_823EEBB8;
	// ori r24,r24,2
	r24.u64 = r24.u64 | 2;
	// b 0x823eebc0
	goto loc_823EEBC0;
loc_823EEBB8:
	// cmpwi cr6,r11,43
	cr6.compare<int32_t>(r11.s32, 43, xer);
	// bne cr6,0x823eebc8
	if (!cr6.getEQ()) goto loc_823EEBC8;
loc_823EEBC0:
	// lbz r31,0(r29)
	r31.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
loc_823EEBC8:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// blt cr6,0x823eedac
	if (cr6.getLT()) goto loc_823EEDAC;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// beq cr6,0x823eedac
	if (cr6.getEQ()) goto loc_823EEDAC;
	// cmpwi cr6,r28,36
	cr6.compare<int32_t>(r28.s32, 36, xer);
	// bgt cr6,0x823eedac
	if (cr6.getGT()) goto loc_823EEDAC;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x823eec24
	if (!cr6.getEQ()) goto loc_823EEC24;
	// extsb r11,r31
	r11.s64 = r31.s8;
	// cmpwi cr6,r11,48
	cr6.compare<int32_t>(r11.s32, 48, xer);
	// beq cr6,0x823eebfc
	if (cr6.getEQ()) goto loc_823EEBFC;
	// li r28,10
	r28.s64 = 10;
	// b 0x823eec5c
	goto loc_823EEC5C;
loc_823EEBFC:
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,120
	cr6.compare<int32_t>(r11.s32, 120, xer);
	// beq cr6,0x823eec1c
	if (cr6.getEQ()) goto loc_823EEC1C;
	// cmpwi cr6,r11,88
	cr6.compare<int32_t>(r11.s32, 88, xer);
	// beq cr6,0x823eec1c
	if (cr6.getEQ()) goto loc_823EEC1C;
	// li r28,8
	r28.s64 = 8;
	// b 0x823eec5c
	goto loc_823EEC5C;
loc_823EEC1C:
	// li r28,16
	r28.s64 = 16;
	// b 0x823eec2c
	goto loc_823EEC2C;
loc_823EEC24:
	// cmpwi cr6,r28,16
	cr6.compare<int32_t>(r28.s32, 16, xer);
	// bne cr6,0x823eec5c
	if (!cr6.getEQ()) goto loc_823EEC5C;
loc_823EEC2C:
	// extsb r11,r31
	r11.s64 = r31.s8;
	// cmpwi cr6,r11,48
	cr6.compare<int32_t>(r11.s32, 48, xer);
	// bne cr6,0x823eec5c
	if (!cr6.getEQ()) goto loc_823EEC5C;
	// lbz r11,0(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,120
	cr6.compare<int32_t>(r11.s32, 120, xer);
	// beq cr6,0x823eec50
	if (cr6.getEQ()) goto loc_823EEC50;
	// cmpwi cr6,r11,88
	cr6.compare<int32_t>(r11.s32, 88, xer);
	// bne cr6,0x823eec5c
	if (!cr6.getEQ()) goto loc_823EEC5C;
loc_823EEC50:
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// lbz r31,0(r11)
	r31.u64 = PPC_LOAD_U8(r11.u32 + 0);
loc_823EEC5C:
	// li r27,-1
	r27.s64 = -1;
	// lwz r8,200(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 200);
	// twllei r28,0
	// divwu r9,r27,r28
	ctx.r9.u32 = r27.u32 / r28.u32;
loc_823EEC6C:
	// rlwinm r11,r31,1,23,30
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0x1FE;
	// lhzx r11,r11,r8
	r11.u64 = PPC_LOAD_U16(r11.u32 + ctx.r8.u32);
	// rlwinm. r10,r11,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x823eec88
	if (cr0.getEQ()) goto loc_823EEC88;
	// extsb r11,r31
	r11.s64 = r31.s8;
	// addi r11,r11,-48
	r11.s64 = r11.s64 + -48;
	// b 0x823eecb0
	goto loc_823EECB0;
loc_823EEC88:
	// andi. r11,r11,259
	r11.u64 = r11.u64 & 259;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// cmpwi r11,0
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x823eecf0
	if (cr0.getEQ()) goto loc_823EECF0;
	// extsb r11,r31
	r11.s64 = r31.s8;
	// cmpwi cr6,r11,97
	cr6.compare<int32_t>(r11.s32, 97, xer);
	// blt cr6,0x823eecac
	if (cr6.getLT()) goto loc_823EECAC;
	// cmpwi cr6,r11,122
	cr6.compare<int32_t>(r11.s32, 122, xer);
	// bgt cr6,0x823eecac
	if (cr6.getGT()) goto loc_823EECAC;
	// addi r11,r11,-32
	r11.s64 = r11.s64 + -32;
loc_823EECAC:
	// addi r11,r11,-55
	r11.s64 = r11.s64 + -55;
loc_823EECB0:
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// bge cr6,0x823eecf0
	if (!cr6.getLT()) goto loc_823EECF0;
	// ori r24,r24,8
	r24.u64 = r24.u64 | 8;
	// cmplw cr6,r26,r9
	cr6.compare<uint32_t>(r26.u32, ctx.r9.u32, xer);
	// blt cr6,0x823eed10
	if (cr6.getLT()) goto loc_823EED10;
	// bne cr6,0x823eece4
	if (!cr6.getEQ()) goto loc_823EECE4;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// twllei r28,0
	// divwu r7,r10,r28
	ctx.r7.u32 = ctx.r10.u32 / r28.u32;
	// mullw r7,r7,r28
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(r28.s32);
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x823eed10
	if (!cr6.getGT()) goto loc_823EED10;
loc_823EECE4:
	// ori r24,r24,4
	r24.u64 = r24.u64 | 4;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x823eed18
	if (!cr6.getEQ()) goto loc_823EED18;
loc_823EECF0:
	// rlwinm. r11,r24,0,28,28
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// bne 0x823eed24
	if (!cr0.getEQ()) goto loc_823EED24;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x823eed08
	if (cr6.getEQ()) goto loc_823EED08;
	// mr r29,r25
	r29.u64 = r25.u64;
loc_823EED08:
	// li r26,0
	r26.s64 = 0;
	// b 0x823eed8c
	goto loc_823EED8C;
loc_823EED10:
	// mullw r10,r26,r28
	ctx.r10.s64 = int64_t(r26.s32) * int64_t(r28.s32);
	// add r26,r10,r11
	r26.u64 = ctx.r10.u64 + r11.u64;
loc_823EED18:
	// lbz r31,0(r29)
	r31.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// b 0x823eec6c
	goto loc_823EEC6C;
loc_823EED24:
	// lis r10,32767
	ctx.r10.s64 = 2147418112;
	// rlwinm. r11,r24,0,29,29
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ori r31,r10,65535
	r31.u64 = ctx.r10.u64 | 65535;
	// lis r30,-32768
	r30.s64 = -2147483648;
	// bne 0x823eed60
	if (!cr0.getEQ()) goto loc_823EED60;
	// clrlwi. r11,r24,31
	r11.u64 = r24.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x823eed8c
	if (!cr0.getEQ()) goto loc_823EED8C;
	// rlwinm. r11,r24,0,30,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x823eed50
	if (cr0.getEQ()) goto loc_823EED50;
	// cmplw cr6,r26,r30
	cr6.compare<uint32_t>(r26.u32, r30.u32, xer);
	// bgt cr6,0x823eed60
	if (cr6.getGT()) goto loc_823EED60;
loc_823EED50:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x823eed8c
	if (!cr6.getEQ()) goto loc_823EED8C;
	// cmplw cr6,r26,r31
	cr6.compare<uint32_t>(r26.u32, r31.u32, xer);
	// ble cr6,0x823eed8c
	if (!cr6.getGT()) goto loc_823EED8C;
loc_823EED60:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// li r11,34
	r11.s64 = 34;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// clrlwi. r10,r24,31
	ctx.r10.u64 = r24.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x823eed7c
	if (cr0.getEQ()) goto loc_823EED7C;
	// mr r26,r27
	r26.u64 = r27.u64;
	// b 0x823eed8c
	goto loc_823EED8C;
loc_823EED7C:
	// rlwinm. r11,r24,0,30,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r26,r30
	r26.u64 = r30.u64;
	// bne 0x823eed8c
	if (!cr0.getEQ()) goto loc_823EED8C;
	// mr r26,r31
	r26.u64 = r31.u64;
loc_823EED8C:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x823eed98
	if (cr6.getEQ()) goto loc_823EED98;
	// stw r29,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r29.u32);
loc_823EED98:
	// rlwinm. r11,r24,0,30,30
	r11.u64 = __builtin_rotateleft64(r24.u32 | (r24.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x823eeda4
	if (cr0.getEQ()) goto loc_823EEDA4;
	// neg r26,r26
	r26.s64 = -r26.s64;
loc_823EEDA4:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// b 0x823eedbc
	goto loc_823EEDBC;
loc_823EEDAC:
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x823eedb8
	if (cr6.getEQ()) goto loc_823EEDB8;
	// stw r25,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r25.u32);
loc_823EEDB8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_823EEDBC:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x823ed174
	return;
}

__attribute__((alias("__imp__sub_823EEDC8"))) PPC_WEAK_FUNC(sub_823EEDC8);
PPC_FUNC_IMPL(__imp__sub_823EEDC8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lis r10,-32015
	ctx.r10.s64 = -2098135040;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// addi r3,r10,-19456
	ctx.r3.s64 = ctx.r10.s64 + -19456;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// b 0x823eead0
	sub_823EEAD0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EEDE8"))) PPC_WEAK_FUNC(sub_823EEDE8);
PPC_FUNC_IMPL(__imp__sub_823EEDE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lis r10,-32015
	ctx.r10.s64 = -2098135040;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// addi r3,r10,-19456
	ctx.r3.s64 = ctx.r10.s64 + -19456;
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// b 0x823eead0
	sub_823EEAD0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EEE10"))) PPC_WEAK_FUNC(sub_823EEE10);
PPC_FUNC_IMPL(__imp__sub_823EEE10) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// neg r12,r3
	r12.s64 = -ctx.r3.s64;
	// neg r11,r12
	r11.s64 = -r12.s64;
	// addi r0,r11,4095
	r0.s64 = r11.s64 + 4095;
	// srawi. r0,r0,12
	xer.ca = (r0.s32 < 0) & ((r0.u32 & 0xFFF) != 0);
	r0.s64 = r0.s32 >> 12;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// blelr 
	if (!cr0.getGT()) return;
	// mr r11,r1
	r11.u64 = ctx.r1.u64;
	// mtctr r0
	ctr.u64 = r0.u64;
loc_823EEE2C:
	// lwzu r0,-4096(r11)
	ea = -4096 + r11.u32;
	r0.u64 = PPC_LOAD_U32(ea);
	r11.u32 = ea;
	// bdnz 0x823eee2c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EEE2C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EEE14"))) PPC_WEAK_FUNC(sub_823EEE14);
PPC_FUNC_IMPL(__imp__sub_823EEE14) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// neg r11,r12
	r11.s64 = -r12.s64;
	// addi r0,r11,4095
	r0.s64 = r11.s64 + 4095;
	// srawi. r0,r0,12
	xer.ca = (r0.s32 < 0) & ((r0.u32 & 0xFFF) != 0);
	r0.s64 = r0.s32 >> 12;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// blelr 
	if (!cr0.getGT()) return;
	// mr r11,r1
	r11.u64 = ctx.r1.u64;
	// mtctr r0
	ctr.u64 = r0.u64;
loc_823EEE2C:
	// lwzu r0,-4096(r11)
	ea = -4096 + r11.u32;
	r0.u64 = PPC_LOAD_U32(ea);
	r11.u32 = ea;
	// bdnz 0x823eee2c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EEE2C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EEE38"))) PPC_WEAK_FUNC(sub_823EEE38);
PPC_FUNC_IMPL(__imp__sub_823EEE38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// addi r11,r3,-65
	r11.s64 = ctx.r3.s64 + -65;
	// cmplwi cr6,r11,25
	cr6.compare<uint32_t>(r11.u32, 25, xer);
	// bgtlr cr6
	if (cr6.getGT()) return;
	// addi r3,r3,32
	ctx.r3.s64 = ctx.r3.s64 + 32;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EEE50"))) PPC_WEAK_FUNC(sub_823EEE50);
PPC_FUNC_IMPL(__imp__sub_823EEE50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x823eeea8
	if (!cr6.getEQ()) goto loc_823EEEA8;
loc_823EEE78:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x823eef2c
	goto loc_823EEF2C;
loc_823EEEA8:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x823eeeb8
	if (cr6.getEQ()) goto loc_823EEEB8;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x823eee78
	if (cr6.getEQ()) goto loc_823EEE78;
loc_823EEEB8:
	// lis r10,32767
	ctx.r10.s64 = 2147418112;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// cmplw cr6,r4,r10
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r10.u32, xer);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// bgt cr6,0x823eeed0
	if (cr6.getGT()) goto loc_823EEED0;
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
loc_823EEED0:
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// li r11,66
	r11.s64 = 66;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x823f52b0
	sub_823F52B0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x823eef28
	if (cr6.getEQ()) goto loc_823EEF28;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// blt 0x823eef1c
	if (cr0.getLT()) goto loc_823EEF1C;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r11,0
	r11.s64 = 0;
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, r11.u8);
	// b 0x823eef28
	goto loc_823EEF28;
loc_823EEF1C:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f5e28
	sub_823F5E28(ctx, base);
loc_823EEF28:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_823EEF2C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EEF48"))) PPC_WEAK_FUNC(sub_823EEF48);
PPC_FUNC_IMPL(__imp__sub_823EEF48) {
	PPC_FUNC_PROLOGUE();
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x823eee50
	sub_823EEE50(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EEF58"))) PPC_WEAK_FUNC(sub_823EEF58);
PPC_FUNC_IMPL(__imp__sub_823EEF58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x823eefb0
	if (!cr6.getEQ()) goto loc_823EEFB0;
loc_823EEF80:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
	// li r3,-1
	ctx.r3.s64 = -1;
	// b 0x823ef074
	goto loc_823EF074;
loc_823EEFB0:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x823eefc0
	if (cr6.getEQ()) goto loc_823EEFC0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x823eef80
	if (cr6.getEQ()) goto loc_823EEF80;
loc_823EEFC0:
	// lis r10,16383
	ctx.r10.s64 = 1073676288;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// li r9,66
	ctx.r9.s64 = 66;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// cmplw cr6,r4,r10
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r10.u32, xer);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// ble cr6,0x823eefec
	if (!cr6.getGT()) goto loc_823EEFEC;
	// lis r10,32767
	ctx.r10.s64 = 2147418112;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// b 0x823eeff0
	goto loc_823EEFF0;
loc_823EEFEC:
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
loc_823EEFF0:
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823f6060
	sub_823F6060(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x823ef070
	if (cr6.getEQ()) goto loc_823EF070;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r31,0
	r31.s64 = 0;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// blt 0x823ef03c
	if (cr0.getLT()) goto loc_823EF03C;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stb r31,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r31.u8);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// b 0x823ef048
	goto loc_823EF048;
loc_823EF03C:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f5e28
	sub_823F5E28(ctx, base);
loc_823EF048:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// blt 0x823ef064
	if (cr0.getLT()) goto loc_823EF064;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stb r31,0(r11)
	PPC_STORE_U8(r11.u32 + 0, r31.u8);
	// b 0x823ef070
	goto loc_823EF070;
loc_823EF064:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f5e28
	sub_823F5E28(ctx, base);
loc_823EF070:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_823EF074:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EF090"))) PPC_WEAK_FUNC(sub_823EF090);
PPC_FUNC_IMPL(__imp__sub_823EF090) {
	PPC_FUNC_PROLOGUE();
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x823eef58
	sub_823EEF58(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EF0A0"))) PPC_WEAK_FUNC(sub_823EF0A0);
PPC_FUNC_IMPL(__imp__sub_823EF0A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	// cntlzd r5,r3
	ctx.r5.u64 = ctx.r3.u64 == 0 ? 64 : __builtin_clzll(ctx.r3.u64);
	// sld r3,r3,r5
	ctx.r3.u64 = ctx.r5.u8 & 0x40 ? 0 : (ctx.r3.u64 << (ctx.r5.u8 & 0x7F));
	// cmpdi r3,0
	cr0.compare<int64_t>(ctx.r3.s64, 0, xer);
	// beq 0x823ef0bc
	if (cr0.getEQ()) goto loc_823EF0BC;
	// subfic r5,r5,1086
	xer.ca = ctx.r5.u32 <= 1086;
	ctx.r5.s64 = 1086 - ctx.r5.s64;
	// rldicl r3,r3,53,12
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u64, 53) & 0xFFFFFFFFFFFFF;
	// rldimi r3,r5,52,1
	ctx.r3.u64 = (__builtin_rotateleft64(ctx.r5.u64, 52) & 0x7FF0000000000000) | (ctx.r3.u64 & 0x800FFFFFFFFFFFFF);
loc_823EF0BC:
	// std r3,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r3.u64);
	// lfd f1,-8(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_14"))) PPC_WEAK_FUNC(__savevmx_14);
PPC_FUNC_IMPL(__imp____savevmx_14) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-288
	r11.s64 = -288;
	// stvx v14,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v14.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx v15,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx v16,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx v17,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_15"))) PPC_WEAK_FUNC(__savevmx_15);
PPC_FUNC_IMPL(__imp____savevmx_15) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-272
	r11.s64 = -272;
	// stvx v15,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx v16,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx v17,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_16"))) PPC_WEAK_FUNC(__savevmx_16);
PPC_FUNC_IMPL(__imp____savevmx_16) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-256
	r11.s64 = -256;
	// stvx v16,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v16.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx v17,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_17"))) PPC_WEAK_FUNC(__savevmx_17);
PPC_FUNC_IMPL(__imp____savevmx_17) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-240
	r11.s64 = -240;
	// stvx v17,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v17.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_18"))) PPC_WEAK_FUNC(__savevmx_18);
PPC_FUNC_IMPL(__imp____savevmx_18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-224
	r11.s64 = -224;
	// stvx v18,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v18.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_19"))) PPC_WEAK_FUNC(__savevmx_19);
PPC_FUNC_IMPL(__imp____savevmx_19) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-208
	r11.s64 = -208;
	// stvx v19,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v19.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_20"))) PPC_WEAK_FUNC(__savevmx_20);
PPC_FUNC_IMPL(__imp____savevmx_20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-192
	r11.s64 = -192;
	// stvx v20,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_21"))) PPC_WEAK_FUNC(__savevmx_21);
PPC_FUNC_IMPL(__imp____savevmx_21) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-176
	r11.s64 = -176;
	// stvx v21,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v21.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_22"))) PPC_WEAK_FUNC(__savevmx_22);
PPC_FUNC_IMPL(__imp____savevmx_22) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-160
	r11.s64 = -160;
	// stvx v22,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_23"))) PPC_WEAK_FUNC(__savevmx_23);
PPC_FUNC_IMPL(__imp____savevmx_23) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-144
	r11.s64 = -144;
	// stvx v23,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v23.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_24"))) PPC_WEAK_FUNC(__savevmx_24);
PPC_FUNC_IMPL(__imp____savevmx_24) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-128
	r11.s64 = -128;
	// stvx v24,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_25"))) PPC_WEAK_FUNC(__savevmx_25);
PPC_FUNC_IMPL(__imp____savevmx_25) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-112
	r11.s64 = -112;
	// stvx v25,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_26"))) PPC_WEAK_FUNC(__savevmx_26);
PPC_FUNC_IMPL(__imp____savevmx_26) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-96
	r11.s64 = -96;
	// stvx v26,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v26.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_27"))) PPC_WEAK_FUNC(__savevmx_27);
PPC_FUNC_IMPL(__imp____savevmx_27) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-80
	r11.s64 = -80;
	// stvx v27,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_28"))) PPC_WEAK_FUNC(__savevmx_28);
PPC_FUNC_IMPL(__imp____savevmx_28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-64
	r11.s64 = -64;
	// stvx v28,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_29"))) PPC_WEAK_FUNC(__savevmx_29);
PPC_FUNC_IMPL(__imp____savevmx_29) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-48
	r11.s64 = -48;
	// stvx v29,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_30"))) PPC_WEAK_FUNC(__savevmx_30);
PPC_FUNC_IMPL(__imp____savevmx_30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-32
	r11.s64 = -32;
	// stvx v30,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_31"))) PPC_WEAK_FUNC(__savevmx_31);
PPC_FUNC_IMPL(__imp____savevmx_31) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v31{};
	// li r11,-16
	r11.s64 = -16;
	// stvx v31,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_64"))) PPC_WEAK_FUNC(__savevmx_64);
PPC_FUNC_IMPL(__imp____savevmx_64) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v64{};
	PPCVRegister v65{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-1024
	r11.s64 = -1024;
	// stvx128 v64,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v64.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-1008
	r11.s64 = -1008;
	// stvx128 v65,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v65.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-992
	r11.s64 = -992;
	// stvx128 v66,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v66.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// stvx128 v67,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v67.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_65"))) PPC_WEAK_FUNC(__savevmx_65);
PPC_FUNC_IMPL(__imp____savevmx_65) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v65{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-1008
	r11.s64 = -1008;
	// stvx128 v65,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v65.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-992
	r11.s64 = -992;
	// stvx128 v66,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v66.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// stvx128 v67,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v67.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_66"))) PPC_WEAK_FUNC(__savevmx_66);
PPC_FUNC_IMPL(__imp____savevmx_66) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-992
	r11.s64 = -992;
	// stvx128 v66,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v66.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// stvx128 v67,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v67.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_67"))) PPC_WEAK_FUNC(__savevmx_67);
PPC_FUNC_IMPL(__imp____savevmx_67) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-976
	r11.s64 = -976;
	// stvx128 v67,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v67.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_68"))) PPC_WEAK_FUNC(__savevmx_68);
PPC_FUNC_IMPL(__imp____savevmx_68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-960
	r11.s64 = -960;
	// stvx128 v68,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v68.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_69"))) PPC_WEAK_FUNC(__savevmx_69);
PPC_FUNC_IMPL(__imp____savevmx_69) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-944
	r11.s64 = -944;
	// stvx128 v69,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v69.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_70"))) PPC_WEAK_FUNC(__savevmx_70);
PPC_FUNC_IMPL(__imp____savevmx_70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-928
	r11.s64 = -928;
	// stvx128 v70,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v70.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_71"))) PPC_WEAK_FUNC(__savevmx_71);
PPC_FUNC_IMPL(__imp____savevmx_71) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-912
	r11.s64 = -912;
	// stvx128 v71,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v71.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_72"))) PPC_WEAK_FUNC(__savevmx_72);
PPC_FUNC_IMPL(__imp____savevmx_72) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-896
	r11.s64 = -896;
	// stvx128 v72,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v72.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_73"))) PPC_WEAK_FUNC(__savevmx_73);
PPC_FUNC_IMPL(__imp____savevmx_73) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-880
	r11.s64 = -880;
	// stvx128 v73,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v73.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_74"))) PPC_WEAK_FUNC(__savevmx_74);
PPC_FUNC_IMPL(__imp____savevmx_74) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-864
	r11.s64 = -864;
	// stvx128 v74,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v74.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_75"))) PPC_WEAK_FUNC(__savevmx_75);
PPC_FUNC_IMPL(__imp____savevmx_75) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-848
	r11.s64 = -848;
	// stvx128 v75,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v75.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_76"))) PPC_WEAK_FUNC(__savevmx_76);
PPC_FUNC_IMPL(__imp____savevmx_76) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-832
	r11.s64 = -832;
	// stvx128 v76,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v76.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_77"))) PPC_WEAK_FUNC(__savevmx_77);
PPC_FUNC_IMPL(__imp____savevmx_77) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-816
	r11.s64 = -816;
	// stvx128 v77,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v77.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_78"))) PPC_WEAK_FUNC(__savevmx_78);
PPC_FUNC_IMPL(__imp____savevmx_78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-800
	r11.s64 = -800;
	// stvx128 v78,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v78.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_79"))) PPC_WEAK_FUNC(__savevmx_79);
PPC_FUNC_IMPL(__imp____savevmx_79) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-784
	r11.s64 = -784;
	// stvx128 v79,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v79.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_80"))) PPC_WEAK_FUNC(__savevmx_80);
PPC_FUNC_IMPL(__imp____savevmx_80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-768
	r11.s64 = -768;
	// stvx128 v80,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v80.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_81"))) PPC_WEAK_FUNC(__savevmx_81);
PPC_FUNC_IMPL(__imp____savevmx_81) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-752
	r11.s64 = -752;
	// stvx128 v81,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v81.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_82"))) PPC_WEAK_FUNC(__savevmx_82);
PPC_FUNC_IMPL(__imp____savevmx_82) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-736
	r11.s64 = -736;
	// stvx128 v82,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v82.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_83"))) PPC_WEAK_FUNC(__savevmx_83);
PPC_FUNC_IMPL(__imp____savevmx_83) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-720
	r11.s64 = -720;
	// stvx128 v83,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v83.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_84"))) PPC_WEAK_FUNC(__savevmx_84);
PPC_FUNC_IMPL(__imp____savevmx_84) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-704
	r11.s64 = -704;
	// stvx128 v84,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v84.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_85"))) PPC_WEAK_FUNC(__savevmx_85);
PPC_FUNC_IMPL(__imp____savevmx_85) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-688
	r11.s64 = -688;
	// stvx128 v85,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v85.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_86"))) PPC_WEAK_FUNC(__savevmx_86);
PPC_FUNC_IMPL(__imp____savevmx_86) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-672
	r11.s64 = -672;
	// stvx128 v86,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v86.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_87"))) PPC_WEAK_FUNC(__savevmx_87);
PPC_FUNC_IMPL(__imp____savevmx_87) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-656
	r11.s64 = -656;
	// stvx128 v87,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v87.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_88"))) PPC_WEAK_FUNC(__savevmx_88);
PPC_FUNC_IMPL(__imp____savevmx_88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-640
	r11.s64 = -640;
	// stvx128 v88,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_89"))) PPC_WEAK_FUNC(__savevmx_89);
PPC_FUNC_IMPL(__imp____savevmx_89) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-624
	r11.s64 = -624;
	// stvx128 v89,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v89.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_90"))) PPC_WEAK_FUNC(__savevmx_90);
PPC_FUNC_IMPL(__imp____savevmx_90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-608
	r11.s64 = -608;
	// stvx128 v90,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v90.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_91"))) PPC_WEAK_FUNC(__savevmx_91);
PPC_FUNC_IMPL(__imp____savevmx_91) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-592
	r11.s64 = -592;
	// stvx128 v91,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v91.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_92"))) PPC_WEAK_FUNC(__savevmx_92);
PPC_FUNC_IMPL(__imp____savevmx_92) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-576
	r11.s64 = -576;
	// stvx128 v92,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v92.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_93"))) PPC_WEAK_FUNC(__savevmx_93);
PPC_FUNC_IMPL(__imp____savevmx_93) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-560
	r11.s64 = -560;
	// stvx128 v93,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v93.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_94"))) PPC_WEAK_FUNC(__savevmx_94);
PPC_FUNC_IMPL(__imp____savevmx_94) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-544
	r11.s64 = -544;
	// stvx128 v94,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_95"))) PPC_WEAK_FUNC(__savevmx_95);
PPC_FUNC_IMPL(__imp____savevmx_95) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-528
	r11.s64 = -528;
	// stvx128 v95,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_96"))) PPC_WEAK_FUNC(__savevmx_96);
PPC_FUNC_IMPL(__imp____savevmx_96) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-512
	r11.s64 = -512;
	// stvx128 v96,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v96.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_97"))) PPC_WEAK_FUNC(__savevmx_97);
PPC_FUNC_IMPL(__imp____savevmx_97) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-496
	r11.s64 = -496;
	// stvx128 v97,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v97.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_98"))) PPC_WEAK_FUNC(__savevmx_98);
PPC_FUNC_IMPL(__imp____savevmx_98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-480
	r11.s64 = -480;
	// stvx128 v98,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v98.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_99"))) PPC_WEAK_FUNC(__savevmx_99);
PPC_FUNC_IMPL(__imp____savevmx_99) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-464
	r11.s64 = -464;
	// stvx128 v99,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v99.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_100"))) PPC_WEAK_FUNC(__savevmx_100);
PPC_FUNC_IMPL(__imp____savevmx_100) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-448
	r11.s64 = -448;
	// stvx128 v100,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v100.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_101"))) PPC_WEAK_FUNC(__savevmx_101);
PPC_FUNC_IMPL(__imp____savevmx_101) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-432
	r11.s64 = -432;
	// stvx128 v101,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v101.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_102"))) PPC_WEAK_FUNC(__savevmx_102);
PPC_FUNC_IMPL(__imp____savevmx_102) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-416
	r11.s64 = -416;
	// stvx128 v102,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v102.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_103"))) PPC_WEAK_FUNC(__savevmx_103);
PPC_FUNC_IMPL(__imp____savevmx_103) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-400
	r11.s64 = -400;
	// stvx128 v103,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v103.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_104"))) PPC_WEAK_FUNC(__savevmx_104);
PPC_FUNC_IMPL(__imp____savevmx_104) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-384
	r11.s64 = -384;
	// stvx128 v104,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v104.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_105"))) PPC_WEAK_FUNC(__savevmx_105);
PPC_FUNC_IMPL(__imp____savevmx_105) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-368
	r11.s64 = -368;
	// stvx128 v105,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v105.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_106"))) PPC_WEAK_FUNC(__savevmx_106);
PPC_FUNC_IMPL(__imp____savevmx_106) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-352
	r11.s64 = -352;
	// stvx128 v106,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v106.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_107"))) PPC_WEAK_FUNC(__savevmx_107);
PPC_FUNC_IMPL(__imp____savevmx_107) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-336
	r11.s64 = -336;
	// stvx128 v107,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v107.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_108"))) PPC_WEAK_FUNC(__savevmx_108);
PPC_FUNC_IMPL(__imp____savevmx_108) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-320
	r11.s64 = -320;
	// stvx128 v108,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v108.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_109"))) PPC_WEAK_FUNC(__savevmx_109);
PPC_FUNC_IMPL(__imp____savevmx_109) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-304
	r11.s64 = -304;
	// stvx128 v109,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v109.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_110"))) PPC_WEAK_FUNC(__savevmx_110);
PPC_FUNC_IMPL(__imp____savevmx_110) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-288
	r11.s64 = -288;
	// stvx128 v110,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v110.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_111"))) PPC_WEAK_FUNC(__savevmx_111);
PPC_FUNC_IMPL(__imp____savevmx_111) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-272
	r11.s64 = -272;
	// stvx128 v111,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v111.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_112"))) PPC_WEAK_FUNC(__savevmx_112);
PPC_FUNC_IMPL(__imp____savevmx_112) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-256
	r11.s64 = -256;
	// stvx128 v112,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v112.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_113"))) PPC_WEAK_FUNC(__savevmx_113);
PPC_FUNC_IMPL(__imp____savevmx_113) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-240
	r11.s64 = -240;
	// stvx128 v113,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v113.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_114"))) PPC_WEAK_FUNC(__savevmx_114);
PPC_FUNC_IMPL(__imp____savevmx_114) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-224
	r11.s64 = -224;
	// stvx128 v114,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v114.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_115"))) PPC_WEAK_FUNC(__savevmx_115);
PPC_FUNC_IMPL(__imp____savevmx_115) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-208
	r11.s64 = -208;
	// stvx128 v115,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v115.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_116"))) PPC_WEAK_FUNC(__savevmx_116);
PPC_FUNC_IMPL(__imp____savevmx_116) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-192
	r11.s64 = -192;
	// stvx128 v116,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v116.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_117"))) PPC_WEAK_FUNC(__savevmx_117);
PPC_FUNC_IMPL(__imp____savevmx_117) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-176
	r11.s64 = -176;
	// stvx128 v117,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v117.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_118"))) PPC_WEAK_FUNC(__savevmx_118);
PPC_FUNC_IMPL(__imp____savevmx_118) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-160
	r11.s64 = -160;
	// stvx128 v118,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v118.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_119"))) PPC_WEAK_FUNC(__savevmx_119);
PPC_FUNC_IMPL(__imp____savevmx_119) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-144
	r11.s64 = -144;
	// stvx128 v119,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v119.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_120"))) PPC_WEAK_FUNC(__savevmx_120);
PPC_FUNC_IMPL(__imp____savevmx_120) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-128
	r11.s64 = -128;
	// stvx128 v120,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v120.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_121"))) PPC_WEAK_FUNC(__savevmx_121);
PPC_FUNC_IMPL(__imp____savevmx_121) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-112
	r11.s64 = -112;
	// stvx128 v121,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v121.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_122"))) PPC_WEAK_FUNC(__savevmx_122);
PPC_FUNC_IMPL(__imp____savevmx_122) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-96
	r11.s64 = -96;
	// stvx128 v122,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v122.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_123"))) PPC_WEAK_FUNC(__savevmx_123);
PPC_FUNC_IMPL(__imp____savevmx_123) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-80
	r11.s64 = -80;
	// stvx128 v123,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v123.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_124"))) PPC_WEAK_FUNC(__savevmx_124);
PPC_FUNC_IMPL(__imp____savevmx_124) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-64
	r11.s64 = -64;
	// stvx128 v124,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_125"))) PPC_WEAK_FUNC(__savevmx_125);
PPC_FUNC_IMPL(__imp____savevmx_125) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-48
	r11.s64 = -48;
	// stvx128 v125,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_126"))) PPC_WEAK_FUNC(__savevmx_126);
PPC_FUNC_IMPL(__imp____savevmx_126) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-32
	r11.s64 = -32;
	// stvx128 v126,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____savevmx_127"))) PPC_WEAK_FUNC(__savevmx_127);
PPC_FUNC_IMPL(__imp____savevmx_127) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v127{};
	// li r11,-16
	r11.s64 = -16;
	// stvx128 v127,r11,r12
	_mm_store_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_14"))) PPC_WEAK_FUNC(__restvmx_14);
PPC_FUNC_IMPL(__imp____restvmx_14) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-288
	r11.s64 = -288;
	// lvx v14,r11,r12
	_mm_store_si128((__m128i*)v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx v15,r11,r12
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx v16,r11,r12
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx v17,r11,r12
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_15"))) PPC_WEAK_FUNC(__restvmx_15);
PPC_FUNC_IMPL(__imp____restvmx_15) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-272
	r11.s64 = -272;
	// lvx v15,r11,r12
	_mm_store_si128((__m128i*)v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx v16,r11,r12
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx v17,r11,r12
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_16"))) PPC_WEAK_FUNC(__restvmx_16);
PPC_FUNC_IMPL(__imp____restvmx_16) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-256
	r11.s64 = -256;
	// lvx v16,r11,r12
	_mm_store_si128((__m128i*)v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx v17,r11,r12
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_17"))) PPC_WEAK_FUNC(__restvmx_17);
PPC_FUNC_IMPL(__imp____restvmx_17) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-240
	r11.s64 = -240;
	// lvx v17,r11,r12
	_mm_store_si128((__m128i*)v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_18"))) PPC_WEAK_FUNC(__restvmx_18);
PPC_FUNC_IMPL(__imp____restvmx_18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-224
	r11.s64 = -224;
	// lvx v18,r11,r12
	_mm_store_si128((__m128i*)v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_19"))) PPC_WEAK_FUNC(__restvmx_19);
PPC_FUNC_IMPL(__imp____restvmx_19) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-208
	r11.s64 = -208;
	// lvx v19,r11,r12
	_mm_store_si128((__m128i*)v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_20"))) PPC_WEAK_FUNC(__restvmx_20);
PPC_FUNC_IMPL(__imp____restvmx_20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-192
	r11.s64 = -192;
	// lvx v20,r11,r12
	_mm_store_si128((__m128i*)v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_21"))) PPC_WEAK_FUNC(__restvmx_21);
PPC_FUNC_IMPL(__imp____restvmx_21) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-176
	r11.s64 = -176;
	// lvx v21,r11,r12
	_mm_store_si128((__m128i*)v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_22"))) PPC_WEAK_FUNC(__restvmx_22);
PPC_FUNC_IMPL(__imp____restvmx_22) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-160
	r11.s64 = -160;
	// lvx v22,r11,r12
	_mm_store_si128((__m128i*)v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_23"))) PPC_WEAK_FUNC(__restvmx_23);
PPC_FUNC_IMPL(__imp____restvmx_23) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-144
	r11.s64 = -144;
	// lvx v23,r11,r12
	_mm_store_si128((__m128i*)v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_24"))) PPC_WEAK_FUNC(__restvmx_24);
PPC_FUNC_IMPL(__imp____restvmx_24) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-128
	r11.s64 = -128;
	// lvx v24,r11,r12
	_mm_store_si128((__m128i*)v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_25"))) PPC_WEAK_FUNC(__restvmx_25);
PPC_FUNC_IMPL(__imp____restvmx_25) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-112
	r11.s64 = -112;
	// lvx v25,r11,r12
	_mm_store_si128((__m128i*)v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_26"))) PPC_WEAK_FUNC(__restvmx_26);
PPC_FUNC_IMPL(__imp____restvmx_26) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-96
	r11.s64 = -96;
	// lvx v26,r11,r12
	_mm_store_si128((__m128i*)v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_27"))) PPC_WEAK_FUNC(__restvmx_27);
PPC_FUNC_IMPL(__imp____restvmx_27) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-80
	r11.s64 = -80;
	// lvx v27,r11,r12
	_mm_store_si128((__m128i*)v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_28"))) PPC_WEAK_FUNC(__restvmx_28);
PPC_FUNC_IMPL(__imp____restvmx_28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-64
	r11.s64 = -64;
	// lvx v28,r11,r12
	_mm_store_si128((__m128i*)v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_29"))) PPC_WEAK_FUNC(__restvmx_29);
PPC_FUNC_IMPL(__imp____restvmx_29) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-48
	r11.s64 = -48;
	// lvx v29,r11,r12
	_mm_store_si128((__m128i*)v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_30"))) PPC_WEAK_FUNC(__restvmx_30);
PPC_FUNC_IMPL(__imp____restvmx_30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	// li r11,-32
	r11.s64 = -32;
	// lvx v30,r11,r12
	_mm_store_si128((__m128i*)v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_31"))) PPC_WEAK_FUNC(__restvmx_31);
PPC_FUNC_IMPL(__imp____restvmx_31) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v31{};
	// li r11,-16
	r11.s64 = -16;
	// lvx v31,r11,r12
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_64"))) PPC_WEAK_FUNC(__restvmx_64);
PPC_FUNC_IMPL(__imp____restvmx_64) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v64{};
	PPCVRegister v65{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-1024
	r11.s64 = -1024;
	// lvx128 v64,r11,r12
	_mm_store_si128((__m128i*)v64.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-1008
	r11.s64 = -1008;
	// lvx128 v65,r11,r12
	_mm_store_si128((__m128i*)v65.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-992
	r11.s64 = -992;
	// lvx128 v66,r11,r12
	_mm_store_si128((__m128i*)v66.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// lvx128 v67,r11,r12
	_mm_store_si128((__m128i*)v67.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_65"))) PPC_WEAK_FUNC(__restvmx_65);
PPC_FUNC_IMPL(__imp____restvmx_65) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v65{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-1008
	r11.s64 = -1008;
	// lvx128 v65,r11,r12
	_mm_store_si128((__m128i*)v65.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-992
	r11.s64 = -992;
	// lvx128 v66,r11,r12
	_mm_store_si128((__m128i*)v66.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// lvx128 v67,r11,r12
	_mm_store_si128((__m128i*)v67.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_66"))) PPC_WEAK_FUNC(__restvmx_66);
PPC_FUNC_IMPL(__imp____restvmx_66) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v66{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-992
	r11.s64 = -992;
	// lvx128 v66,r11,r12
	_mm_store_si128((__m128i*)v66.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-976
	r11.s64 = -976;
	// lvx128 v67,r11,r12
	_mm_store_si128((__m128i*)v67.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_67"))) PPC_WEAK_FUNC(__restvmx_67);
PPC_FUNC_IMPL(__imp____restvmx_67) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v67{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-976
	r11.s64 = -976;
	// lvx128 v67,r11,r12
	_mm_store_si128((__m128i*)v67.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_68"))) PPC_WEAK_FUNC(__restvmx_68);
PPC_FUNC_IMPL(__imp____restvmx_68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v68{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-960
	r11.s64 = -960;
	// lvx128 v68,r11,r12
	_mm_store_si128((__m128i*)v68.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_69"))) PPC_WEAK_FUNC(__restvmx_69);
PPC_FUNC_IMPL(__imp____restvmx_69) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v69{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-944
	r11.s64 = -944;
	// lvx128 v69,r11,r12
	_mm_store_si128((__m128i*)v69.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_70"))) PPC_WEAK_FUNC(__restvmx_70);
PPC_FUNC_IMPL(__imp____restvmx_70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v70{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-928
	r11.s64 = -928;
	// lvx128 v70,r11,r12
	_mm_store_si128((__m128i*)v70.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_71"))) PPC_WEAK_FUNC(__restvmx_71);
PPC_FUNC_IMPL(__imp____restvmx_71) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v71{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-912
	r11.s64 = -912;
	// lvx128 v71,r11,r12
	_mm_store_si128((__m128i*)v71.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_72"))) PPC_WEAK_FUNC(__restvmx_72);
PPC_FUNC_IMPL(__imp____restvmx_72) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v72{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-896
	r11.s64 = -896;
	// lvx128 v72,r11,r12
	_mm_store_si128((__m128i*)v72.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_73"))) PPC_WEAK_FUNC(__restvmx_73);
PPC_FUNC_IMPL(__imp____restvmx_73) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v73{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-880
	r11.s64 = -880;
	// lvx128 v73,r11,r12
	_mm_store_si128((__m128i*)v73.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_74"))) PPC_WEAK_FUNC(__restvmx_74);
PPC_FUNC_IMPL(__imp____restvmx_74) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v74{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-864
	r11.s64 = -864;
	// lvx128 v74,r11,r12
	_mm_store_si128((__m128i*)v74.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_75"))) PPC_WEAK_FUNC(__restvmx_75);
PPC_FUNC_IMPL(__imp____restvmx_75) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v75{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-848
	r11.s64 = -848;
	// lvx128 v75,r11,r12
	_mm_store_si128((__m128i*)v75.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_76"))) PPC_WEAK_FUNC(__restvmx_76);
PPC_FUNC_IMPL(__imp____restvmx_76) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v76{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-832
	r11.s64 = -832;
	// lvx128 v76,r11,r12
	_mm_store_si128((__m128i*)v76.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_77"))) PPC_WEAK_FUNC(__restvmx_77);
PPC_FUNC_IMPL(__imp____restvmx_77) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-816
	r11.s64 = -816;
	// lvx128 v77,r11,r12
	_mm_store_si128((__m128i*)v77.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_78"))) PPC_WEAK_FUNC(__restvmx_78);
PPC_FUNC_IMPL(__imp____restvmx_78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v78{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-800
	r11.s64 = -800;
	// lvx128 v78,r11,r12
	_mm_store_si128((__m128i*)v78.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_79"))) PPC_WEAK_FUNC(__restvmx_79);
PPC_FUNC_IMPL(__imp____restvmx_79) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v79{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-784
	r11.s64 = -784;
	// lvx128 v79,r11,r12
	_mm_store_si128((__m128i*)v79.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_80"))) PPC_WEAK_FUNC(__restvmx_80);
PPC_FUNC_IMPL(__imp____restvmx_80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v80{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-768
	r11.s64 = -768;
	// lvx128 v80,r11,r12
	_mm_store_si128((__m128i*)v80.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_81"))) PPC_WEAK_FUNC(__restvmx_81);
PPC_FUNC_IMPL(__imp____restvmx_81) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-752
	r11.s64 = -752;
	// lvx128 v81,r11,r12
	_mm_store_si128((__m128i*)v81.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_82"))) PPC_WEAK_FUNC(__restvmx_82);
PPC_FUNC_IMPL(__imp____restvmx_82) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-736
	r11.s64 = -736;
	// lvx128 v82,r11,r12
	_mm_store_si128((__m128i*)v82.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_83"))) PPC_WEAK_FUNC(__restvmx_83);
PPC_FUNC_IMPL(__imp____restvmx_83) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-720
	r11.s64 = -720;
	// lvx128 v83,r11,r12
	_mm_store_si128((__m128i*)v83.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_84"))) PPC_WEAK_FUNC(__restvmx_84);
PPC_FUNC_IMPL(__imp____restvmx_84) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v84{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-704
	r11.s64 = -704;
	// lvx128 v84,r11,r12
	_mm_store_si128((__m128i*)v84.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_85"))) PPC_WEAK_FUNC(__restvmx_85);
PPC_FUNC_IMPL(__imp____restvmx_85) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v85{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-688
	r11.s64 = -688;
	// lvx128 v85,r11,r12
	_mm_store_si128((__m128i*)v85.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_86"))) PPC_WEAK_FUNC(__restvmx_86);
PPC_FUNC_IMPL(__imp____restvmx_86) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v86{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-672
	r11.s64 = -672;
	// lvx128 v86,r11,r12
	_mm_store_si128((__m128i*)v86.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_87"))) PPC_WEAK_FUNC(__restvmx_87);
PPC_FUNC_IMPL(__imp____restvmx_87) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v87{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-656
	r11.s64 = -656;
	// lvx128 v87,r11,r12
	_mm_store_si128((__m128i*)v87.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_88"))) PPC_WEAK_FUNC(__restvmx_88);
PPC_FUNC_IMPL(__imp____restvmx_88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v88{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-640
	r11.s64 = -640;
	// lvx128 v88,r11,r12
	_mm_store_si128((__m128i*)v88.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_89"))) PPC_WEAK_FUNC(__restvmx_89);
PPC_FUNC_IMPL(__imp____restvmx_89) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v89{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-624
	r11.s64 = -624;
	// lvx128 v89,r11,r12
	_mm_store_si128((__m128i*)v89.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_90"))) PPC_WEAK_FUNC(__restvmx_90);
PPC_FUNC_IMPL(__imp____restvmx_90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-608
	r11.s64 = -608;
	// lvx128 v90,r11,r12
	_mm_store_si128((__m128i*)v90.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_91"))) PPC_WEAK_FUNC(__restvmx_91);
PPC_FUNC_IMPL(__imp____restvmx_91) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-592
	r11.s64 = -592;
	// lvx128 v91,r11,r12
	_mm_store_si128((__m128i*)v91.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_92"))) PPC_WEAK_FUNC(__restvmx_92);
PPC_FUNC_IMPL(__imp____restvmx_92) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-576
	r11.s64 = -576;
	// lvx128 v92,r11,r12
	_mm_store_si128((__m128i*)v92.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_93"))) PPC_WEAK_FUNC(__restvmx_93);
PPC_FUNC_IMPL(__imp____restvmx_93) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-560
	r11.s64 = -560;
	// lvx128 v93,r11,r12
	_mm_store_si128((__m128i*)v93.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_94"))) PPC_WEAK_FUNC(__restvmx_94);
PPC_FUNC_IMPL(__imp____restvmx_94) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-544
	r11.s64 = -544;
	// lvx128 v94,r11,r12
	_mm_store_si128((__m128i*)v94.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_95"))) PPC_WEAK_FUNC(__restvmx_95);
PPC_FUNC_IMPL(__imp____restvmx_95) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v95{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-528
	r11.s64 = -528;
	// lvx128 v95,r11,r12
	_mm_store_si128((__m128i*)v95.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_96"))) PPC_WEAK_FUNC(__restvmx_96);
PPC_FUNC_IMPL(__imp____restvmx_96) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v96{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-512
	r11.s64 = -512;
	// lvx128 v96,r11,r12
	_mm_store_si128((__m128i*)v96.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_97"))) PPC_WEAK_FUNC(__restvmx_97);
PPC_FUNC_IMPL(__imp____restvmx_97) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v97{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-496
	r11.s64 = -496;
	// lvx128 v97,r11,r12
	_mm_store_si128((__m128i*)v97.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_98"))) PPC_WEAK_FUNC(__restvmx_98);
PPC_FUNC_IMPL(__imp____restvmx_98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v98{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-480
	r11.s64 = -480;
	// lvx128 v98,r11,r12
	_mm_store_si128((__m128i*)v98.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_99"))) PPC_WEAK_FUNC(__restvmx_99);
PPC_FUNC_IMPL(__imp____restvmx_99) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v99{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-464
	r11.s64 = -464;
	// lvx128 v99,r11,r12
	_mm_store_si128((__m128i*)v99.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_100"))) PPC_WEAK_FUNC(__restvmx_100);
PPC_FUNC_IMPL(__imp____restvmx_100) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v100{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-448
	r11.s64 = -448;
	// lvx128 v100,r11,r12
	_mm_store_si128((__m128i*)v100.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_101"))) PPC_WEAK_FUNC(__restvmx_101);
PPC_FUNC_IMPL(__imp____restvmx_101) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v101{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-432
	r11.s64 = -432;
	// lvx128 v101,r11,r12
	_mm_store_si128((__m128i*)v101.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_102"))) PPC_WEAK_FUNC(__restvmx_102);
PPC_FUNC_IMPL(__imp____restvmx_102) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v102{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-416
	r11.s64 = -416;
	// lvx128 v102,r11,r12
	_mm_store_si128((__m128i*)v102.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_103"))) PPC_WEAK_FUNC(__restvmx_103);
PPC_FUNC_IMPL(__imp____restvmx_103) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v103{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-400
	r11.s64 = -400;
	// lvx128 v103,r11,r12
	_mm_store_si128((__m128i*)v103.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_104"))) PPC_WEAK_FUNC(__restvmx_104);
PPC_FUNC_IMPL(__imp____restvmx_104) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v104{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-384
	r11.s64 = -384;
	// lvx128 v104,r11,r12
	_mm_store_si128((__m128i*)v104.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_105"))) PPC_WEAK_FUNC(__restvmx_105);
PPC_FUNC_IMPL(__imp____restvmx_105) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v105{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-368
	r11.s64 = -368;
	// lvx128 v105,r11,r12
	_mm_store_si128((__m128i*)v105.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_106"))) PPC_WEAK_FUNC(__restvmx_106);
PPC_FUNC_IMPL(__imp____restvmx_106) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v106{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-352
	r11.s64 = -352;
	// lvx128 v106,r11,r12
	_mm_store_si128((__m128i*)v106.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_107"))) PPC_WEAK_FUNC(__restvmx_107);
PPC_FUNC_IMPL(__imp____restvmx_107) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v107{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-336
	r11.s64 = -336;
	// lvx128 v107,r11,r12
	_mm_store_si128((__m128i*)v107.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_108"))) PPC_WEAK_FUNC(__restvmx_108);
PPC_FUNC_IMPL(__imp____restvmx_108) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v108{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-320
	r11.s64 = -320;
	// lvx128 v108,r11,r12
	_mm_store_si128((__m128i*)v108.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_109"))) PPC_WEAK_FUNC(__restvmx_109);
PPC_FUNC_IMPL(__imp____restvmx_109) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v109{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-304
	r11.s64 = -304;
	// lvx128 v109,r11,r12
	_mm_store_si128((__m128i*)v109.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_110"))) PPC_WEAK_FUNC(__restvmx_110);
PPC_FUNC_IMPL(__imp____restvmx_110) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v110{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-288
	r11.s64 = -288;
	// lvx128 v110,r11,r12
	_mm_store_si128((__m128i*)v110.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_111"))) PPC_WEAK_FUNC(__restvmx_111);
PPC_FUNC_IMPL(__imp____restvmx_111) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v111{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-272
	r11.s64 = -272;
	// lvx128 v111,r11,r12
	_mm_store_si128((__m128i*)v111.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_112"))) PPC_WEAK_FUNC(__restvmx_112);
PPC_FUNC_IMPL(__imp____restvmx_112) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v112{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-256
	r11.s64 = -256;
	// lvx128 v112,r11,r12
	_mm_store_si128((__m128i*)v112.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_113"))) PPC_WEAK_FUNC(__restvmx_113);
PPC_FUNC_IMPL(__imp____restvmx_113) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v113{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-240
	r11.s64 = -240;
	// lvx128 v113,r11,r12
	_mm_store_si128((__m128i*)v113.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_114"))) PPC_WEAK_FUNC(__restvmx_114);
PPC_FUNC_IMPL(__imp____restvmx_114) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v114{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-224
	r11.s64 = -224;
	// lvx128 v114,r11,r12
	_mm_store_si128((__m128i*)v114.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_115"))) PPC_WEAK_FUNC(__restvmx_115);
PPC_FUNC_IMPL(__imp____restvmx_115) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v115{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-208
	r11.s64 = -208;
	// lvx128 v115,r11,r12
	_mm_store_si128((__m128i*)v115.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_116"))) PPC_WEAK_FUNC(__restvmx_116);
PPC_FUNC_IMPL(__imp____restvmx_116) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v116{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-192
	r11.s64 = -192;
	// lvx128 v116,r11,r12
	_mm_store_si128((__m128i*)v116.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_117"))) PPC_WEAK_FUNC(__restvmx_117);
PPC_FUNC_IMPL(__imp____restvmx_117) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v117{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-176
	r11.s64 = -176;
	// lvx128 v117,r11,r12
	_mm_store_si128((__m128i*)v117.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_118"))) PPC_WEAK_FUNC(__restvmx_118);
PPC_FUNC_IMPL(__imp____restvmx_118) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v118{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-160
	r11.s64 = -160;
	// lvx128 v118,r11,r12
	_mm_store_si128((__m128i*)v118.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_119"))) PPC_WEAK_FUNC(__restvmx_119);
PPC_FUNC_IMPL(__imp____restvmx_119) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v119{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-144
	r11.s64 = -144;
	// lvx128 v119,r11,r12
	_mm_store_si128((__m128i*)v119.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_120"))) PPC_WEAK_FUNC(__restvmx_120);
PPC_FUNC_IMPL(__imp____restvmx_120) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v120{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-128
	r11.s64 = -128;
	// lvx128 v120,r11,r12
	_mm_store_si128((__m128i*)v120.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_121"))) PPC_WEAK_FUNC(__restvmx_121);
PPC_FUNC_IMPL(__imp____restvmx_121) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-112
	r11.s64 = -112;
	// lvx128 v121,r11,r12
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_122"))) PPC_WEAK_FUNC(__restvmx_122);
PPC_FUNC_IMPL(__imp____restvmx_122) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-96
	r11.s64 = -96;
	// lvx128 v122,r11,r12
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_123"))) PPC_WEAK_FUNC(__restvmx_123);
PPC_FUNC_IMPL(__imp____restvmx_123) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-80
	r11.s64 = -80;
	// lvx128 v123,r11,r12
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_124"))) PPC_WEAK_FUNC(__restvmx_124);
PPC_FUNC_IMPL(__imp____restvmx_124) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-64
	r11.s64 = -64;
	// lvx128 v124,r11,r12
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_125"))) PPC_WEAK_FUNC(__restvmx_125);
PPC_FUNC_IMPL(__imp____restvmx_125) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-48
	r11.s64 = -48;
	// lvx128 v125,r11,r12
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_126"))) PPC_WEAK_FUNC(__restvmx_126);
PPC_FUNC_IMPL(__imp____restvmx_126) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	// li r11,-32
	r11.s64 = -32;
	// lvx128 v126,r11,r12
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp____restvmx_127"))) PPC_WEAK_FUNC(__restvmx_127);
PPC_FUNC_IMPL(__imp____restvmx_127) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCVRegister v127{};
	// li r11,-16
	r11.s64 = -16;
	// lvx128 v127,r11,r12
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r12.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EF600"))) PPC_WEAK_FUNC(sub_823EF600);
PPC_FUNC_IMPL(__imp__sub_823EF600) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x823f4d98
	sub_823F4D98(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823f4d50
	sub_823F4D50(ctx, base);
	// lis r11,-32015
	r11.s64 = -2098135040;
	// li r3,255
	ctx.r3.s64 = 255;
	// lwz r11,-21440(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -21440);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EF648"))) PPC_WEAK_FUNC(sub_823EF648);
PPC_FUNC_IMPL(__imp__sub_823EF648) {
	PPC_FUNC_PROLOGUE();
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8240f9ec
	__imp__KeBugCheck(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EF650"))) PPC_WEAK_FUNC(sub_823EF650);
PPC_FUNC_IMPL(__imp__sub_823EF650) {
	PPC_FUNC_PROLOGUE();
	// li r3,8
	ctx.r3.s64 = 8;
	// b 0x823f8040
	sub_823F8040(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EF658"))) PPC_WEAK_FUNC(sub_823EF658);
PPC_FUNC_IMPL(__imp__sub_823EF658) {
	PPC_FUNC_PROLOGUE();
	// li r3,8
	ctx.r3.s64 = 8;
	// b 0x823f7ee0
	sub_823F7EE0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EF660"))) PPC_WEAK_FUNC(sub_823EF660);
PPC_FUNC_IMPL(__imp__sub_823EF660) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// b 0x823ef698
	goto loc_823EF698;
loc_823EF680:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x823ef694
	if (cr0.getEQ()) goto loc_823EF694;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_823EF694:
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
loc_823EF698:
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// blt cr6,0x823ef680
	if (cr6.getLT()) goto loc_823EF680;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EF6B8"))) PPC_WEAK_FUNC(sub_823EF6B8);
PPC_FUNC_IMPL(__imp__sub_823EF6B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r16{};
	PPCRegister r18{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// lwz r18,-1044(0)
	r18.u64 = PPC_LOAD_U32(-1044);
	// lwz r16,29416(r7)
	r16.u64 = PPC_LOAD_U32(ctx.r7.u32 + 29416);
	// mflr r12
	// bl 0x823ed138
	// addi r31,r1,-128
	r31.s64 = ctx.r1.s64 + -128;
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// stw r28,164(r31)
	PPC_STORE_U32(r31.u32 + 164, r28.u32);
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x823f8040
	sub_823F8040(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lis r11,-31987
	r11.s64 = -2096300032;
	// lwz r11,-20116(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -20116);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x823ef798
	if (cr6.getEQ()) goto loc_823EF798;
	// lis r10,-31987
	ctx.r10.s64 = -2096300032;
	// lwz r11,-20120(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -20120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x823ef710
	if (!cr6.getEQ()) goto loc_823EF710;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8240f9ec
	__imp__KeBugCheck(ctx, base);
loc_823EF710:
	// li r11,1
	r11.s64 = 1;
	// stw r11,-20120(r10)
	PPC_STORE_U32(ctx.r10.u32 + -20120, r11.u32);
	// lis r10,-31987
	ctx.r10.s64 = -2096300032;
	// stb r28,-20124(r10)
	PPC_STORE_U8(ctx.r10.u32 + -20124, r28.u8);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x823ef784
	if (!cr6.getEQ()) goto loc_823EF784;
	// lis r11,-31987
	r11.s64 = -2096300032;
	// lwz r29,-15560(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + -15560);
	// lis r11,-31987
	r11.s64 = -2096300032;
	// lwz r30,-15564(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + -15564);
	// stw r30,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r30.u32);
	// cmplwi r29,0
	cr0.compare<uint32_t>(r29.u32, 0, xer);
	// beq 0x823ef770
	if (cr0.getEQ()) goto loc_823EF770;
loc_823EF744:
	// addi r30,r30,-4
	r30.s64 = r30.s64 + -4;
	// stw r30,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r30.u32);
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// blt cr6,0x823ef770
	if (cr6.getLT()) goto loc_823EF770;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x823ef768
	if (cr0.getEQ()) goto loc_823EF768;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_823EF768:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x823ef744
	goto loc_823EF744;
loc_823EF770:
	// lis r11,-32190
	r11.s64 = -2109603840;
	// addi r4,r11,536
	ctx.r4.s64 = r11.s64 + 536;
	// lis r11,-32190
	r11.s64 = -2109603840;
	// addi r3,r11,528
	ctx.r3.s64 = r11.s64 + 528;
	// bl 0x823ef660
	sub_823EF660(ctx, base);
loc_823EF784:
	// lis r11,-32190
	r11.s64 = -2109603840;
	// addi r4,r11,544
	ctx.r4.s64 = r11.s64 + 544;
	// lis r11,-32190
	r11.s64 = -2109603840;
	// addi r3,r11,540
	ctx.r3.s64 = r11.s64 + 540;
	// bl 0x823ef660
	sub_823EF660(ctx, base);
loc_823EF798:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r12,r31,128
	r12.s64 = r31.s64 + 128;
	// bl 0x823ef7e0
	sub_823EF7E0(ctx, base);
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x823ef7b8
	if (!cr6.getEQ()) goto loc_823EF7B8;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8240f9ec
	__imp__KeBugCheck(ctx, base);
loc_823EF7B8:
	// addi r1,r31,128
	ctx.r1.s64 = r31.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_823EF6C0"))) PPC_WEAK_FUNC(sub_823EF6C0);
PPC_FUNC_IMPL(__imp__sub_823EF6C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed138
	// addi r31,r1,-128
	r31.s64 = ctx.r1.s64 + -128;
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// stw r28,164(r31)
	PPC_STORE_U32(r31.u32 + 164, r28.u32);
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x823f8040
	sub_823F8040(ctx, base);
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// lis r11,-31987
	r11.s64 = -2096300032;
	// lwz r11,-20116(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -20116);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x823ef798
	if (cr6.getEQ()) goto loc_823EF798;
	// lis r10,-31987
	ctx.r10.s64 = -2096300032;
	// lwz r11,-20120(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -20120);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x823ef710
	if (!cr6.getEQ()) goto loc_823EF710;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8240f9ec
	__imp__KeBugCheck(ctx, base);
loc_823EF710:
	// li r11,1
	r11.s64 = 1;
	// stw r11,-20120(r10)
	PPC_STORE_U32(ctx.r10.u32 + -20120, r11.u32);
	// lis r10,-31987
	ctx.r10.s64 = -2096300032;
	// stb r28,-20124(r10)
	PPC_STORE_U8(ctx.r10.u32 + -20124, r28.u8);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x823ef784
	if (!cr6.getEQ()) goto loc_823EF784;
	// lis r11,-31987
	r11.s64 = -2096300032;
	// lwz r29,-15560(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + -15560);
	// lis r11,-31987
	r11.s64 = -2096300032;
	// lwz r30,-15564(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + -15564);
	// stw r30,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r30.u32);
	// cmplwi r29,0
	cr0.compare<uint32_t>(r29.u32, 0, xer);
	// beq 0x823ef770
	if (cr0.getEQ()) goto loc_823EF770;
loc_823EF744:
	// addi r30,r30,-4
	r30.s64 = r30.s64 + -4;
	// stw r30,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r30.u32);
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// blt cr6,0x823ef770
	if (cr6.getLT()) goto loc_823EF770;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x823ef768
	if (cr0.getEQ()) goto loc_823EF768;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_823EF768:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// b 0x823ef744
	goto loc_823EF744;
loc_823EF770:
	// lis r11,-32190
	r11.s64 = -2109603840;
	// addi r4,r11,536
	ctx.r4.s64 = r11.s64 + 536;
	// lis r11,-32190
	r11.s64 = -2109603840;
	// addi r3,r11,528
	ctx.r3.s64 = r11.s64 + 528;
	// bl 0x823ef660
	sub_823EF660(ctx, base);
loc_823EF784:
	// lis r11,-32190
	r11.s64 = -2109603840;
	// addi r4,r11,544
	ctx.r4.s64 = r11.s64 + 544;
	// lis r11,-32190
	r11.s64 = -2109603840;
	// addi r3,r11,540
	ctx.r3.s64 = r11.s64 + 540;
	// bl 0x823ef660
	sub_823EF660(ctx, base);
loc_823EF798:
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// addi r12,r31,128
	r12.s64 = r31.s64 + 128;
	// bl 0x823ef7e0
	sub_823EF7E0(ctx, base);
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x823ef7b8
	if (!cr6.getEQ()) goto loc_823EF7B8;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8240f9ec
	__imp__KeBugCheck(ctx, base);
loc_823EF7B8:
	// addi r1,r31,128
	ctx.r1.s64 = r31.s64 + 128;
	// b 0x823ed188
	return;
}

__attribute__((alias("__imp__sub_823EF7C0"))) PPC_WEAK_FUNC(sub_823EF7C0);
PPC_FUNC_IMPL(__imp__sub_823EF7C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r28{};
	PPCRegister r31{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// addi r31,r12,-128
	r31.s64 = r12.s64 + -128;
	// std r28,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r28.u64);
	// mflr r12
	// stw r12,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r28,164(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// b 0x823ef7f8
	goto loc_823EF7F8;
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// addi r31,r12,-128
	r31.s64 = r12.s64 + -128;
	// std r28,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r28.u64);
	// mflr r12
	// stw r12,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
loc_823EF7F8:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x823ef808
	if (cr6.getEQ()) goto loc_823EF808;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x823f7ee0
	sub_823F7EE0(ctx, base);
loc_823EF808:
	// lwz r1,0(r1)
	ctx.r1.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// ld r28,-16(r1)
	r28.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// lwz r12,-24(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EF7E0"))) PPC_WEAK_FUNC(sub_823EF7E0);
PPC_FUNC_IMPL(__imp__sub_823EF7E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r28{};
	PPCRegister r31{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// addi r31,r12,-128
	r31.s64 = r12.s64 + -128;
	// std r28,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r28.u64);
	// mflr r12
	// stw r12,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x823ef808
	if (cr6.getEQ()) goto loc_823EF808;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x823f7ee0
	sub_823F7EE0(ctx, base);
loc_823EF808:
	// lwz r1,0(r1)
	ctx.r1.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// ld r28,-16(r1)
	r28.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// lwz r12,-24(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EF820"))) PPC_WEAK_FUNC(sub_823EF820);
PPC_FUNC_IMPL(__imp__sub_823EF820) {
	PPC_FUNC_PROLOGUE();
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// b 0x823ef6c0
	sub_823EF6C0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EF830"))) PPC_WEAK_FUNC(sub_823EF830);
PPC_FUNC_IMPL(__imp__sub_823EF830) {
	PPC_FUNC_PROLOGUE();
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x823ef6c0
	sub_823EF6C0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_823EF840"))) PPC_WEAK_FUNC(sub_823EF840);
PPC_FUNC_IMPL(__imp__sub_823EF840) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f4dc8
	sub_823F4DC8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f8170
	sub_823F8170(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f3c98
	sub_823F3C98(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f7bd8
	sub_823F7BD8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823d22e0
	sub_823D22E0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x823f8158
	sub_823F8158(ctx, base);
	// lis r11,-32193
	r11.s64 = -2109800448;
	// lis r10,-32015
	ctx.r10.s64 = -2098135040;
	// addi r11,r11,-2016
	r11.s64 = r11.s64 + -2016;
	// stw r11,-21440(r10)
	PPC_STORE_U32(ctx.r10.u32 + -21440, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EF8A0"))) PPC_WEAK_FUNC(sub_823EF8A0);
PPC_FUNC_IMPL(__imp__sub_823EF8A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r0{};
	// cmpw r3,r4
	cr0.compare<int32_t>(ctx.r3.s32, ctx.r4.s32, xer);
	// beqlr- 
	if (cr0.getEQ()) return;
	// bge+ 0x823ef8b0
	if (!cr0.getLT()) goto loc_823EF8B0;
	// b 0x823ee010
	sub_823EE010(ctx, base);
	return;
loc_823EF8B0:
	// addi r0,r5,1
	r0.s64 = ctx.r5.s64 + 1;
	// add r3,r3,r5
	ctx.r3.u64 = ctx.r3.u64 + ctx.r5.u64;
	// add r4,r4,r5
	ctx.r4.u64 = ctx.r4.u64 + ctx.r5.u64;
	// mtctr r0
	ctr.u64 = r0.u64;
	// b 0x823ef8d8
	goto loc_823EF8D8;
loc_823EF8C4:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// lbz r0,-1(r4)
	r0.u64 = PPC_LOAD_U8(ctx.r4.u32 + -1);
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// stb r0,-1(r3)
	PPC_STORE_U8(ctx.r3.u32 + -1, r0.u8);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
loc_823EF8D8:
	// andi. r0,r3,3
	r0.u64 = ctx.r3.u64 & 3;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// bdnzf eq,0x823ef8c4
	--ctr.u64;
	if (ctr.u32 != 0 && !cr0.getEQ()) goto loc_823EF8C4;
	// rlwinm. r0,r5,30,2,31
	r0.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// beq- 0x823ef908
	if (cr0.getEQ()) goto loc_823EF908;
	// mtctr r0
	ctr.u64 = r0.u64;
	// andi. r0,r4,3
	r0.u64 = ctx.r4.u64 & 3;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// bne- 0x823ef92c
	if (!cr0.getEQ()) goto loc_823EF92C;
loc_823EF8F4:
	// lwz r7,-4(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + -4);
	// addi r4,r4,-4
	ctx.r4.s64 = ctx.r4.s64 + -4;
	// stw r7,-4(r3)
	PPC_STORE_U32(ctx.r3.u32 + -4, ctx.r7.u32);
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// bdnz+ 0x823ef8f4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EF8F4;
loc_823EF908:
	// andi. r0,r5,3
	r0.u64 = ctx.r5.u64 & 3;
	cr0.compare<int32_t>(r0.s32, 0, xer);
	// mtctr r0
	ctr.u64 = r0.u64;
	// beqlr+ 
	if (cr0.getEQ()) return;
loc_823EF914:
	// lbz r0,-1(r4)
	r0.u64 = PPC_LOAD_U8(ctx.r4.u32 + -1);
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// stb r0,-1(r3)
	PPC_STORE_U8(ctx.r3.u32 + -1, r0.u8);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// bdnz+ 0x823ef914
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EF914;
	// blr 
	return;
loc_823EF92C:
	// lbz r7,-1(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + -1);
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// lbz r8,-2(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + -2);
	// rlwimi r7,r8,8,16,23
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 8) & 0xFF00) | (ctx.r7.u64 & 0xFFFFFFFFFFFF00FF);
	// lbz r9,-3(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + -3);
	// rlwimi r7,r9,16,8,15
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFF0000) | (ctx.r7.u64 & 0xFFFFFFFFFF00FFFF);
	// lbz r10,-4(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + -4);
	// rlwimi r7,r10,24,0,7
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r10.u32, 24) & 0xFF000000) | (ctx.r7.u64 & 0xFFFFFFFF00FFFFFF);
	// addi r4,r4,-4
	ctx.r4.s64 = ctx.r4.s64 + -4;
	// stw r7,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r7.u32);
	// bdnz 0x823ef92c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_823EF92C;
	// b 0x823ef908
	goto loc_823EF908;
}

__attribute__((alias("__imp__sub_823EF960"))) PPC_WEAK_FUNC(sub_823EF960);
PPC_FUNC_IMPL(__imp__sub_823EF960) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed120
	// stwu r1,-416(r1)
	ea = -416 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x823ef9b4
	if (!cr6.getEQ()) goto loc_823EF9B4;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x823ef9b4
	if (cr6.getEQ()) goto loc_823EF9B4;
loc_823EF984:
	// bl 0x823f3de0
	sub_823F3DE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,22
	ctx.r10.s64 = 22;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// bl 0x823f3ca8
	sub_823F3CA8(ctx, base);
loc_823EF9AC:
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// b 0x823ed170
	return;
loc_823EF9B4:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x823ef984
	if (cr6.getEQ()) goto loc_823EF984;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x823ef984
	if (cr6.getEQ()) goto loc_823EF984;
	// cmplwi cr6,r4,2
	cr6.compare<uint32_t>(ctx.r4.u32, 2, xer);
	// blt cr6,0x823ef9ac
	if (cr6.getLT()) goto loc_823EF9AC;
	// addi r11,r4,-1
	r11.s64 = ctx.r4.s64 + -1;
	// li r22,0
	r22.s64 = 0;
	// mullw r11,r11,r27
	r11.s64 = int64_t(r11.s32) * int64_t(r27.s32);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// add r28,r11,r3
	r28.u64 = r11.u64 + ctx.r3.u64;
	// addi r23,r1,80
	r23.s64 = ctx.r1.s64 + 80;
	// addi r24,r1,208
	r24.s64 = ctx.r1.s64 + 208;
loc_823EF9E8:
	// subf r11,r26,r28
	r11.s64 = r28.s64 - r26.s64;
	// twllei r27,0
	// divwu r11,r11,r27
	r11.u32 = r11.u32 / r27.u32;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bgt cr6,0x823efaa8
	if (cr6.getGT()) goto loc_823EFAA8;
	// mr r29,r28
	r29.u64 = r28.u64;
	// cmplw cr6,r28,r26
	cr6.compare<uint32_t>(r28.u32, r26.u32, xer);
	// ble cr6,0x823efa88
	if (!cr6.getGT()) goto loc_823EFA88;
	// add r28,r26,r27
	r28.u64 = r26.u64 + r27.u64;
loc_823EFA10:
	// mr r31,r28
	r31.u64 = r28.u64;
	// mr r30,r26
	r30.u64 = r26.u64;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// bgt cr6,0x823efa48
	if (cr6.getGT()) goto loc_823EFA48;
loc_823EFA20:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r25
	ctr.u64 = r25.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x823efa3c
	if (!cr6.getGT()) goto loc_823EFA3C;
	// mr r30,r31
	r30.u64 = r31.u64;
loc_823EFA3C:
	// add r31,r31,r27
	r31.u64 = r31.u64 + r27.u64;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// ble cr6,0x823efa20
	if (!cr6.getGT()) goto loc_823EFA20;
loc_823EFA48:
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// beq cr6,0x823efa7c
	if (cr6.getEQ()) goto loc_823EFA7C;
	// subf r9,r29,r30
	ctx.r9.s64 = r30.s64 - r29.s64;
loc_823EFA5C:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stbx r7,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r7.u8);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x823efa5c
	if (!cr6.getEQ()) goto loc_823EFA5C;
loc_823EFA7C:
	// subf r29,r27,r29
	r29.s64 = r29.s64 - r27.s64;
	// cmplw cr6,r29,r26
	cr6.compare<uint32_t>(r29.u32, r26.u32, xer);
	// bgt cr6,0x823efa10
	if (cr6.getGT()) goto loc_823EFA10;
loc_823EFA88:
	// addi r22,r22,-1
	r22.s64 = r22.s64 + -1;
	// addi r24,r24,-4
	r24.s64 = r24.s64 + -4;
	// addi r23,r23,-4
	r23.s64 = r23.s64 + -4;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// blt cr6,0x823ef9ac
	if (cr6.getLT()) goto loc_823EF9AC;
	// lwz r26,0(r24)
	r26.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r28,0(r23)
	r28.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// b 0x823ef9e8
	goto loc_823EF9E8;
loc_823EFAA8:
	// rlwinm r11,r11,31,1,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mullw r11,r11,r27
	r11.s64 = int64_t(r11.s32) * int64_t(r27.s32);
	// add r29,r11,r26
	r29.u64 = r11.u64 + r26.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mtctr r25
	ctr.u64 = r25.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x823efb00
	if (!cr6.getGT()) goto loc_823EFB00;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
	// cmplw cr6,r26,r29
	cr6.compare<uint32_t>(r26.u32, r29.u32, xer);
	// beq cr6,0x823efb00
	if (cr6.getEQ()) goto loc_823EFB00;
	// subf r9,r29,r26
	ctx.r9.s64 = r26.s64 - r29.s64;
loc_823EFAE0:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stbx r7,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r7.u8);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x823efae0
	if (!cr6.getEQ()) goto loc_823EFAE0;
loc_823EFB00:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mtctr r25
	ctr.u64 = r25.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x823efb4c
	if (!cr6.getGT()) goto loc_823EFB4C;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// cmplw cr6,r26,r28
	cr6.compare<uint32_t>(r26.u32, r28.u32, xer);
	// beq cr6,0x823efb4c
	if (cr6.getEQ()) goto loc_823EFB4C;
	// subf r9,r28,r26
	ctx.r9.s64 = r26.s64 - r28.s64;
loc_823EFB2C:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stbx r7,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r7.u8);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x823efb2c
	if (!cr6.getEQ()) goto loc_823EFB2C;
loc_823EFB4C:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r25
	ctr.u64 = r25.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x823efb98
	if (!cr6.getGT()) goto loc_823EFB98;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// mr r11,r28
	r11.u64 = r28.u64;
	// cmplw cr6,r29,r28
	cr6.compare<uint32_t>(r29.u32, r28.u32, xer);
	// beq cr6,0x823efb98
	if (cr6.getEQ()) goto loc_823EFB98;
	// subf r9,r28,r29
	ctx.r9.s64 = r29.s64 - r28.s64;
loc_823EFB78:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stbx r7,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r7.u8);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x823efb78
	if (!cr6.getEQ()) goto loc_823EFB78;
loc_823EFB98:
	// mr r30,r26
	r30.u64 = r26.u64;
	// mr r31,r28
	r31.u64 = r28.u64;
loc_823EFBA0:
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// ble cr6,0x823efbd4
	if (!cr6.getGT()) goto loc_823EFBD4;
loc_823EFBA8:
	// add r30,r30,r27
	r30.u64 = r30.u64 + r27.u64;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// bge cr6,0x823efbd4
	if (!cr6.getLT()) goto loc_823EFBD4;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r25
	ctr.u64 = r25.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x823efba8
	if (!cr6.getGT()) goto loc_823EFBA8;
	// cmplw cr6,r29,r30
	cr6.compare<uint32_t>(r29.u32, r30.u32, xer);
	// bgt cr6,0x823efbf8
	if (cr6.getGT()) goto loc_823EFBF8;
loc_823EFBD4:
	// add r30,r30,r27
	r30.u64 = r30.u64 + r27.u64;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// bgt cr6,0x823efbf8
	if (cr6.getGT()) goto loc_823EFBF8;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r25
	ctr.u64 = r25.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x823efbd4
	if (!cr6.getGT()) goto loc_823EFBD4;
loc_823EFBF8:
	// subf r31,r27,r31
	r31.s64 = r31.s64 - r27.s64;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// ble cr6,0x823efc1c
	if (!cr6.getGT()) goto loc_823EFC1C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r25
	ctr.u64 = r25.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bgt cr6,0x823efbf8
	if (cr6.getGT()) goto loc_823EFBF8;
loc_823EFC1C:
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// bgt cr6,0x823efc64
	if (cr6.getGT()) goto loc_823EFC64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// mr r11,r31
	r11.u64 = r31.u64;
	// beq cr6,0x823efc54
	if (cr6.getEQ()) goto loc_823EFC54;
	// subf r9,r31,r30
	ctx.r9.s64 = r30.s64 - r31.s64;
loc_823EFC34:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stbx r7,r9,r11
	PPC_STORE_U8(ctx.r9.u32 + r11.u32, ctx.r7.u8);
	// stb r8,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r8.u8);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// bne cr6,0x823efc34
	if (!cr6.getEQ()) goto loc_823EFC34;
loc_823EFC54:
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// bne cr6,0x823efba0
	if (!cr6.getEQ()) goto loc_823EFBA0;
	// mr r29,r30
	r29.u64 = r30.u64;
	// b 0x823efba0
	goto loc_823EFBA0;
loc_823EFC64:
	// add r31,r31,r27
	r31.u64 = r31.u64 + r27.u64;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// bge cr6,0x823efc9c
	if (!cr6.getLT()) goto loc_823EFC9C;
loc_823EFC70:
	// subf r31,r27,r31
	r31.s64 = r31.s64 - r27.s64;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// ble cr6,0x823efc9c
	if (!cr6.getGT()) goto loc_823EFC9C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r25
	ctr.u64 = r25.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x823efc70
	if (cr6.getEQ()) goto loc_823EFC70;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// blt cr6,0x823efcc0
	if (cr6.getLT()) goto loc_823EFCC0;
loc_823EFC9C:
	// subf r31,r27,r31
	r31.s64 = r31.s64 - r27.s64;
	// cmplw cr6,r31,r26
	cr6.compare<uint32_t>(r31.u32, r26.u32, xer);
	// ble cr6,0x823efcc0
	if (!cr6.getGT()) goto loc_823EFCC0;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r25
	ctr.u64 = r25.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x823efc9c
	if (cr6.getEQ()) goto loc_823EFC9C;
loc_823EFCC0:
	// subf r11,r30,r28
	r11.s64 = r28.s64 - r30.s64;
	// subf r10,r26,r31
	ctx.r10.s64 = r31.s64 - r26.s64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// blt cr6,0x823efcfc
	if (cr6.getLT()) goto loc_823EFCFC;
	// cmplw cr6,r26,r31
	cr6.compare<uint32_t>(r26.u32, r31.u32, xer);
	// bge cr6,0x823efcec
	if (!cr6.getLT()) goto loc_823EFCEC;
	// stw r26,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r26.u32);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// stw r31,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r31.u32);
	// addi r24,r24,4
	r24.s64 = r24.s64 + 4;
	// addi r23,r23,4
	r23.s64 = r23.s64 + 4;
loc_823EFCEC:
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// bge cr6,0x823efa88
	if (!cr6.getLT()) goto loc_823EFA88;
	// mr r26,r30
	r26.u64 = r30.u64;
	// b 0x823ef9e8
	goto loc_823EF9E8;
loc_823EFCFC:
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// bge cr6,0x823efd18
	if (!cr6.getLT()) goto loc_823EFD18;
	// stw r30,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r30.u32);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// stw r28,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r28.u32);
	// addi r24,r24,4
	r24.s64 = r24.s64 + 4;
	// addi r23,r23,4
	r23.s64 = r23.s64 + 4;
loc_823EFD18:
	// cmplw cr6,r26,r31
	cr6.compare<uint32_t>(r26.u32, r31.u32, xer);
	// bge cr6,0x823efa88
	if (!cr6.getLT()) goto loc_823EFA88;
	// mr r28,r31
	r28.u64 = r31.u64;
	// b 0x823ef9e8
	goto loc_823EF9E8;
}

__attribute__((alias("__imp__sub_823EFD28"))) PPC_WEAK_FUNC(sub_823EFD28);
PPC_FUNC_IMPL(__imp__sub_823EFD28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x823ed13c
	// stfd f30,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f30.u64);
	// stfd f31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-16377
	r11.s64 = -1073283072;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// stfd f31,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, f31.u64);
	// ori r29,r11,65279
	r29.u64 = r11.u64 | 65279;
	// li r3,248
	ctx.r3.s64 = 248;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x823f8d20
	sub_823F8D20(ctx, base);
	// lhz r11,160(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 160);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// rlwinm r11,r11,0,17,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7FF0;
	// cmplwi cr6,r11,32752
	cr6.compare<uint32_t>(r11.u32, 32752, xer);
	// bne cr6,0x823efde8
	if (!cr6.getEQ()) goto loc_823EFDE8;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x823f7780
	sub_823F7780(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble 0x823efdb4
	if (!cr0.getGT()) goto loc_823EFDB4;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// ble cr6,0x823efee8
	if (!cr6.getGT()) goto loc_823EFEE8;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// bne cr6,0x823efdb4
	if (!cr6.getEQ()) goto loc_823EFDB4;
	// extsw r11,r31
	r11.s64 = r31.s32;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r3,25
	ctx.r3.s64 = 25;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f2,f0
	ctx.f2.f64 = double(f0.s64);
	// bl 0x823f8ae0
	sub_823F8AE0(ctx, base);
	// b 0x823efef8
	goto loc_823EFEF8;
loc_823EFDB4:
	// extsw r10,r31
	ctx.r10.s64 = r31.s32;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r3,8
	ctx.r3.s64 = 8;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f0,2728(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 2728);
	// fadd f3,f31,f0
	ctx.f3.f64 = f31.f64 + f0.f64;
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
loc_823EFDD0:
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// li r4,25
	ctx.r4.s64 = 25;
	// fcfid f2,f0
	ctx.f2.f64 = double(f0.s64);
	// bl 0x823f8c38
	sub_823F8C38(ctx, base);
	// b 0x823efef8
	goto loc_823EFEF8;
loc_823EFDE8:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f30,2752(r11)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(r11.u32 + 2752);
	// fcmpu cr6,f31,f30
	cr6.compare(f31.f64, f30.f64);
	// beq cr6,0x823efee8
	if (cr6.getEQ()) goto loc_823EFEE8;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x823f7808
	sub_823F7808(ctx, base);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x823efe24
	if (!cr6.getLT()) goto loc_823EFE24;
	// lis r11,-32768
	r11.s64 = -2147483648;
	// subf r10,r31,r11
	ctx.r10.s64 = r11.s64 - r31.s64;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x823efea0
	if (cr6.getLT()) goto loc_823EFEA0;
	// b 0x823efe3c
	goto loc_823EFE3C;
loc_823EFE24:
	// lis r11,32767
	r11.s64 = 2147418112;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// subf r10,r31,r11
	ctx.r10.s64 = r11.s64 - r31.s64;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x823efe48
	if (cr6.getGT()) goto loc_823EFE48;
loc_823EFE3C:
	// add r4,r11,r31
	ctx.r4.u64 = r11.u64 + r31.u64;
	// cmpwi cr6,r4,2560
	cr6.compare<int32_t>(ctx.r4.s32, 2560, xer);
	// ble cr6,0x823efe70
	if (!cr6.getGT()) goto loc_823EFE70;
loc_823EFE48:
	// lis r11,-32015
	r11.s64 = -2098135040;
	// fmr f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = ctx.f1.f64;
	// lfd f1,-19448(r11)
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + -19448);
	// bl 0x823f09b8
	sub_823F09B8(ctx, base);
	// extsw r11,r31
	r11.s64 = r31.s32;
	// fmr f3,f1
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = ctx.f1.f64;
	// li r3,17
	ctx.r3.s64 = 17;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x823efdd0
	goto loc_823EFDD0;
loc_823EFE70:
	// cmpwi cr6,r4,1024
	cr6.compare<int32_t>(ctx.r4.s32, 1024, xer);
	// ble cr6,0x823efe98
	if (!cr6.getGT()) goto loc_823EFE98;
	// addi r4,r4,-1536
	ctx.r4.s64 = ctx.r4.s64 + -1536;
	// bl 0x823f7708
	sub_823F7708(ctx, base);
	// extsw r11,r31
	r11.s64 = r31.s32;
	// fmr f3,f1
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = ctx.f1.f64;
	// li r3,17
	ctx.r3.s64 = 17;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x823efdd0
	goto loc_823EFDD0;
loc_823EFE98:
	// cmpwi cr6,r4,-2557
	cr6.compare<int32_t>(ctx.r4.s32, -2557, xer);
	// bge cr6,0x823efeb8
	if (!cr6.getLT()) goto loc_823EFEB8;
loc_823EFEA0:
	// extsw r11,r31
	r11.s64 = r31.s32;
	// fmul f3,f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = ctx.f1.f64 * f30.f64;
	// li r3,18
	ctx.r3.s64 = 18;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x823efdd0
	goto loc_823EFDD0;
loc_823EFEB8:
	// cmpwi cr6,r4,-1021
	cr6.compare<int32_t>(ctx.r4.s32, -1021, xer);
	// bge cr6,0x823efee0
	if (!cr6.getLT()) goto loc_823EFEE0;
	// addi r4,r4,1536
	ctx.r4.s64 = ctx.r4.s64 + 1536;
	// bl 0x823f7708
	sub_823F7708(ctx, base);
	// extsw r11,r31
	r11.s64 = r31.s32;
	// fmr f3,f1
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = ctx.f1.f64;
	// li r3,18
	ctx.r3.s64 = 18;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x823efdd0
	goto loc_823EFDD0;
loc_823EFEE0:
	// bl 0x823f7708
	sub_823F7708(ctx, base);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
loc_823EFEE8:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823f8d20
	sub_823F8D20(ctx, base);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
loc_823EFEF8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-48(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f31,-40(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x823ed18c
	return;
}

__attribute__((alias("__imp__sub_823EFF08"))) PPC_WEAK_FUNC(sub_823EFF08);
PPC_FUNC_IMPL(__imp__sub_823EFF08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	// stfd f1,16(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + 16, ctx.f1.u64);
	// lhz r11,16(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 16);
	// rlwinm. r11,r11,0,17,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7FF0;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x823eff30
	if (!cr0.getEQ()) goto loc_823EFF30;
	// lwz r11,16(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 16);
	// clrlwi. r11,r11,12
	r11.u64 = r11.u32 & 0xFFFFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x823eff6c
	if (!cr0.getEQ()) goto loc_823EFF6C;
	// lwz r11,20(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x823eff6c
	if (!cr6.getEQ()) goto loc_823EFF6C;
loc_823EFF30:
	// fctid f0,f1
	ctx.fpscr.disableFlushMode();
	f0.s64 = (ctx.f1.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvtsd_si64(_mm_load_sd(&ctx.f1.f64));
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcmpu cr6,f0,f1
	cr6.compare(f0.f64, ctx.f1.f64);
	// bne cr6,0x823eff6c
	if (!cr6.getEQ()) goto loc_823EFF6C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f0,28168(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 28168);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// fctid f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvtsd_si64(_mm_load_sd(&f0.f64));
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bne cr6,0x823eff64
	if (!cr6.getEQ()) goto loc_823EFF64;
	// li r3,2
	ctx.r3.s64 = 2;
	// blr 
	return;
loc_823EFF64:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_823EFF6C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823EFF78"))) PPC_WEAK_FUNC(sub_823EFF78);
PPC_FUNC_IMPL(__imp__sub_823EFF78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f31,f2
	f31.f64 = ctx.f2.f64;
	// stfd f31,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, f31.u64);
	// lis r9,32752
	ctx.r9.s64 = 2146435072;
	// stfd f1,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.f1.u64);
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// fabs f0,f1
	f0.u64 = ctx.f1.u64 & ~0x8000000000000000;
	// li r30,0
	r30.s64 = 0;
	// lis r10,-16
	ctx.r10.s64 = -1048576;
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x823f0000
	if (!cr6.getEQ()) goto loc_823F0000;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x823f003c
	if (!cr6.getEQ()) goto loc_823F003C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f13,2728(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 2728);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x823effe4
	if (!cr6.getGT()) goto loc_823EFFE4;
loc_823EFFD8:
	// lis r11,-32015
	r11.s64 = -2098135040;
	// lfd f0,-19448(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -19448);
	// b 0x823f00dc
	goto loc_823F00DC;
loc_823EFFE4:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x823efff8
	if (!cr6.getLT()) goto loc_823EFFF8;
loc_823EFFEC:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f0,2752(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 2752);
	// b 0x823f00dc
	goto loc_823F00DC;
loc_823EFFF8:
	// stfd f13,0(r31)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r31.u32 + 0, ctx.f13.u64);
	// b 0x823f00e0
	goto loc_823F00E0;
loc_823F0000:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x823f003c
	if (!cr6.getEQ()) goto loc_823F003C;
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x823f003c
	if (!cr6.getEQ()) goto loc_823F003C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f13,2728(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 2728);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bgt cr6,0x823effec
	if (cr6.getGT()) goto loc_823EFFEC;
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// blt cr6,0x823effd8
	if (cr6.getLT()) goto loc_823EFFD8;
	// lis r11,-32015
	r11.s64 = -2098135040;
	// li r30,1
	r30.s64 = 1;
	// lfd f0,-19440(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -19440);
	// b 0x823f00dc
	goto loc_823F00DC;
loc_823F003C:
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x823f0074
	if (!cr6.getEQ()) goto loc_823F0074;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x823f00e0
	if (!cr6.getEQ()) goto loc_823F00E0;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f0,2752(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 2752);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// bgt cr6,0x823effd8
	if (cr6.getGT()) goto loc_823EFFD8;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f13,2728(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 2728);
	// fsel f0,f31,f13,f0
	f0.f64 = f31.f64 >= 0.0 ? ctx.f13.f64 : f0.f64;
	// b 0x823f00dc
	goto loc_823F00DC;
loc_823F0074:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x823f00e0
	if (!cr6.getEQ()) goto loc_823F00E0;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x823f00e0
	if (!cr6.getEQ()) goto loc_823F00E0;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x823eff08
	sub_823EFF08(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f0,2752(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 2752);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// ble cr6,0x823f00b8
	if (!cr6.getGT()) goto loc_823F00B8;
	// lis r11,-32015
	r11.s64 = -2098135040;
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// lfd f0,-19448(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -19448);
	// bne cr6,0x823f00dc
	if (!cr6.getEQ()) goto loc_823F00DC;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
	// b 0x823f00dc
	goto loc_823F00DC;
loc_823F00B8:
	// fcmpu cr6,f31,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, f0.f64);
	// bge cr6,0x823f00d4
	if (!cr6.getLT()) goto loc_823F00D4;
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x823f00dc
	if (!cr6.getEQ()) goto loc_823F00DC;
	// lis r11,-32015
	r11.s64 = -2098135040;
	// lfd f0,-19416(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -19416);
	// b 0x823f00dc
	goto loc_823F00DC;
loc_823F00D4:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f0,2728(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 2728);
loc_823F00DC:
	// stfd f0,0(r31)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(r31.u32 + 0, f0.u64);
loc_823F00E0:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823F0100"))) PPC_WEAK_FUNC(sub_823F0100);
PPC_FUNC_IMPL(__imp__sub_823F0100) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r12,r1,-24
	r12.s64 = ctx.r1.s64 + -24;
	// bl 0x823ed53c
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f30,f2
	ctx.fpscr.disableFlushMode();
	f30.f64 = ctx.f2.f64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmr f28,f1
	f28.f64 = ctx.f1.f64;
	// stfd f30,200(r1)
	PPC_STORE_U64(ctx.r1.u32 + 200, f30.u64);
	// stfd f28,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, f28.u64);
	// lfd f27,2752(r11)
	f27.u64 = PPC_LOAD_U64(r11.u32 + 2752);
	// fcmpu cr6,f30,f27
	cr6.compare(f30.f64, f27.f64);
	// bne cr6,0x823f0148
	if (!cr6.getEQ()) goto loc_823F0148;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f1,2728(r11)
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + 2728);
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F0148:
	// fcmpu cr6,f28,f27
	ctx.fpscr.disableFlushMode();
	cr6.compare(f28.f64, f27.f64);
	// bne cr6,0x823f019c
	if (!cr6.getEQ()) goto loc_823F019C;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// bl 0x823eff08
	sub_823EFF08(ctx, base);
	// fcmpu cr6,f30,f27
	ctx.fpscr.disableFlushMode();
	cr6.compare(f30.f64, f27.f64);
	// bge cr6,0x823f017c
	if (!cr6.getLT()) goto loc_823F017C;
	// lis r11,-32015
	r11.s64 = -2098135040;
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// lfd f1,-19448(r11)
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + -19448);
	// bne cr6,0x823f05dc
	if (!cr6.getEQ()) goto loc_823F05DC;
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// bl 0x823f09b8
	sub_823F09B8(ctx, base);
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F017C:
	// fcmpu cr6,f30,f27
	ctx.fpscr.disableFlushMode();
	cr6.compare(f30.f64, f27.f64);
	// ble cr6,0x823f019c
	if (!cr6.getGT()) goto loc_823F019C;
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x823f0194
	if (!cr6.getEQ()) goto loc_823F0194;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F0194:
	// fmr f1,f27
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f27.f64;
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F019C:
	// lhz r11,192(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 192);
	// lhz r9,200(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 200);
	// rlwinm r10,r11,0,17,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7FF0;
	// cmplwi cr6,r10,32752
	cr6.compare<uint32_t>(ctx.r10.u32, 32752, xer);
	// beq cr6,0x823f0568
	if (cr6.getEQ()) goto loc_823F0568;
	// rlwinm r10,r9,0,17,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x7FF0;
	// cmplwi cr6,r10,32752
	cr6.compare<uint32_t>(ctx.r10.u32, 32752, xer);
	// beq cr6,0x823f0568
	if (cr6.getEQ()) goto loc_823F0568;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fcmpu cr6,f28,f27
	ctx.fpscr.disableFlushMode();
	cr6.compare(f28.f64, f27.f64);
	// lfd f26,2728(r11)
	f26.u64 = PPC_LOAD_U64(r11.u32 + 2728);
	// fmr f25,f26
	f25.f64 = f26.f64;
	// bge cr6,0x823f0200
	if (!cr6.getLT()) goto loc_823F0200;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// bl 0x823eff08
	sub_823EFF08(ctx, base);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// beq cr6,0x823f01f4
	if (cr6.getEQ()) goto loc_823F01F4;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// beq cr6,0x823f01fc
	if (cr6.getEQ()) goto loc_823F01FC;
	// lis r11,-32015
	r11.s64 = -2098135040;
	// lfd f1,-19440(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + -19440);
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F01F4:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfd f25,27216(r11)
	ctx.fpscr.disableFlushMode();
	f25.u64 = PPC_LOAD_U64(r11.u32 + 27216);
loc_823F01FC:
	// fneg f28,f28
	ctx.fpscr.disableFlushMode();
	f28.u64 = f28.u64 ^ 0x8000000000000000;
loc_823F0200:
	// fabs f13,f30
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = f30.u64 & ~0x8000000000000000;
	// lis r11,-32015
	r11.s64 = -2098135040;
	// lfd f0,-21424(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -21424);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// ble cr6,0x823f0250
	if (!cr6.getGT()) goto loc_823F0250;
	// fcmpu cr6,f30,f27
	cr6.compare(f30.f64, f27.f64);
	// bge cr6,0x823f0220
	if (!cr6.getLT()) goto loc_823F0220;
	// fdiv f28,f26,f28
	f28.f64 = f26.f64 / f28.f64;
loc_823F0220:
	// fcmpu cr6,f28,f26
	ctx.fpscr.disableFlushMode();
	cr6.compare(f28.f64, f26.f64);
	// ble cr6,0x823f0238
	if (!cr6.getGT()) goto loc_823F0238;
loc_823F0228:
	// lis r11,-32015
	r11.s64 = -2098135040;
	// lfd f0,-19448(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -19448);
loc_823F0230:
	// fmul f1,f0,f25
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64 * f25.f64;
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F0238:
	// fcmpu cr6,f28,f26
	ctx.fpscr.disableFlushMode();
	cr6.compare(f28.f64, f26.f64);
	// bge cr6,0x823f0248
	if (!cr6.getLT()) goto loc_823F0248;
loc_823F0240:
	// fmul f1,f25,f27
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f25.f64 * f27.f64;
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F0248:
	// fmr f1,f25
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f25.f64;
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F0250:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fmr f1,f28
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f28.f64;
	// bl 0x823f7808
	sub_823F7808(ctx, base);
	// lis r11,-32249
	r11.s64 = -2113470464;
	// fmr f29,f1
	ctx.fpscr.disableFlushMode();
	f29.f64 = ctx.f1.f64;
	// lfd f0,9152(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 9152);
	// fcmpu cr6,f30,f0
	cr6.compare(f30.f64, f0.f64);
	// bgt cr6,0x823f030c
	if (cr6.getGT()) goto loc_823F030C;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x823eff08
	sub_823EFF08(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x823f030c
	if (cr0.getEQ()) goto loc_823F030C;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// bl 0x823eff08
	sub_823EFF08(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x823f030c
	if (cr0.getEQ()) goto loc_823F030C;
	// fcmpu cr6,f30,f27
	ctx.fpscr.disableFlushMode();
	cr6.compare(f30.f64, f27.f64);
	// ble cr6,0x823f030c
	if (!cr6.getGT()) goto loc_823F030C;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// fctiwz f0,f30
	f0.s64 = (f30.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f30.f64));
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// fmr f31,f26
	f31.f64 = f26.f64;
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// mullw r31,r11,r10
	r31.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x823f02d4
	if (cr6.getEQ()) goto loc_823F02D4;
loc_823F02BC:
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x823f02c8
	if (cr0.getEQ()) goto loc_823F02C8;
	// fmul f31,f31,f29
	ctx.fpscr.disableFlushMode();
	f31.f64 = f31.f64 * f29.f64;
loc_823F02C8:
	// fmul f29,f29,f29
	ctx.fpscr.disableFlushMode();
	f29.f64 = f29.f64 * f29.f64;
	// srawi. r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x823f02bc
	if (!cr0.getEQ()) goto loc_823F02BC;
loc_823F02D4:
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x823f7730
	sub_823F7730(ctx, base);
	// add r4,r3,r31
	ctx.r4.u64 = ctx.r3.u64 + r31.u64;
	// cmpwi cr6,r4,2560
	cr6.compare<int32_t>(ctx.r4.s32, 2560, xer);
	// ble cr6,0x823f02f8
	if (!cr6.getGT()) goto loc_823F02F8;
	// lis r11,-32015
	r11.s64 = -2098135040;
	// lfd f0,-19448(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + -19448);
	// fmul f0,f0,f31
	f0.f64 = f0.f64 * f31.f64;
	// b 0x823f0230
	goto loc_823F0230;
loc_823F02F8:
	// cmpwi cr6,r4,-2557
	cr6.compare<int32_t>(ctx.r4.s32, -2557, xer);
	// bge cr6,0x823f0548
	if (!cr6.getLT()) goto loc_823F0548;
	// fmul f0,f31,f25
	ctx.fpscr.disableFlushMode();
	f0.f64 = f31.f64 * f25.f64;
	// fmul f1,f0,f27
	ctx.f1.f64 = f0.f64 * f27.f64;
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F030C:
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// li r11,1
	r11.s64 = 1;
	// addi r31,r10,8816
	r31.s64 = ctx.r10.s64 + 8816;
	// lfd f0,72(r31)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r31.u32 + 72);
	// fcmpu cr6,f29,f0
	cr6.compare(f29.f64, f0.f64);
	// bgt cr6,0x823f0328
	if (cr6.getGT()) goto loc_823F0328;
	// li r11,9
	r11.s64 = 9;
loc_823F0328:
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r9,r31,32
	ctx.r9.s64 = r31.s64 + 32;
	// lfdx f0,r10,r9
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r9.u32);
	// fcmpu cr6,f29,f0
	cr6.compare(f29.f64, f0.f64);
	// bgt cr6,0x823f0340
	if (cr6.getGT()) goto loc_823F0340;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_823F0340:
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r9,r31,16
	ctx.r9.s64 = r31.s64 + 16;
	// lfdx f0,r10,r9
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r9.u32);
	// fcmpu cr6,f29,f0
	cr6.compare(f29.f64, f0.f64);
	// bgt cr6,0x823f0358
	if (cr6.getGT()) goto loc_823F0358;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
loc_823F0358:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// subf. r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// blt 0x823f0370
	if (cr0.getLT()) goto loc_823F0370;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_823F0370:
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfd f31,-18664(r10)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r10.u32 + -18664);
	// bne cr6,0x823f0398
	if (!cr6.getEQ()) goto loc_823F0398;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x823f0f40
	sub_823F0F40(ctx, base);
	// lfd f0,216(r31)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r31.u32 + 216);
	// fmr f13,f27
	ctx.f13.f64 = f27.f64;
	// fmul f12,f1,f0
	ctx.f12.f64 = ctx.f1.f64 * f0.f64;
	// b 0x823f0428
	goto loc_823F0428;
loc_823F0398:
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lfd f11,256(r31)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(r31.u32 + 256);
	// addi r8,r31,8
	ctx.r8.s64 = r31.s64 + 8;
	// lfd f10,216(r31)
	ctx.f10.u64 = PPC_LOAD_U64(r31.u32 + 216);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r7,r31,144
	ctx.r7.s64 = r31.s64 + 144;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// lfdx f0,r10,r8
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r8.u32);
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// fsub f12,f29,f0
	ctx.f12.f64 = f29.f64 - f0.f64;
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// fadd f0,f0,f29
	f0.f64 = f0.f64 + f29.f64;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f9,88(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfdx f13,r11,r7
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + ctx.r7.u32);
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// fsub f13,f12,f13
	ctx.f13.f64 = ctx.f12.f64 - ctx.f13.f64;
	// fmul f12,f9,f31
	ctx.f12.f64 = ctx.f9.f64 * f31.f64;
	// lfd f9,224(r31)
	ctx.f9.u64 = PPC_LOAD_U64(r31.u32 + 224);
	// fdiv f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 / f0.f64;
	// lfd f0,27256(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 27256);
	// fmul f0,f13,f0
	f0.f64 = ctx.f13.f64 * f0.f64;
	// fmul f13,f0,f0
	ctx.f13.f64 = f0.f64 * f0.f64;
	// fmul f8,f0,f9
	ctx.f8.f64 = f0.f64 * ctx.f9.f64;
	// lfd f9,248(r31)
	ctx.f9.u64 = PPC_LOAD_U64(r31.u32 + 248);
	// fmadd f9,f13,f11,f9
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64 + ctx.f9.f64;
	// lfd f11,240(r31)
	ctx.f11.u64 = PPC_LOAD_U64(r31.u32 + 240);
	// fmadd f9,f9,f13,f11
	ctx.f9.f64 = ctx.f9.f64 * ctx.f13.f64 + ctx.f11.f64;
	// lfd f11,232(r31)
	ctx.f11.u64 = PPC_LOAD_U64(r31.u32 + 232);
	// fmadd f11,f9,f13,f11
	ctx.f11.f64 = ctx.f9.f64 * ctx.f13.f64 + ctx.f11.f64;
	// fmul f13,f11,f13
	ctx.f13.f64 = ctx.f11.f64 * ctx.f13.f64;
	// fmul f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 * f0.f64;
	// fmadd f13,f13,f10,f8
	ctx.f13.f64 = ctx.f13.f64 * ctx.f10.f64 + ctx.f8.f64;
	// fadd f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 + f0.f64;
loc_823F0428:
	// lis r11,-32254
	r11.s64 = -2113798144;
	// fmul f10,f13,f30
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = ctx.f13.f64 * f30.f64;
	// lfd f0,-14368(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + -14368);
	// fmul f11,f30,f0
	ctx.f11.f64 = f30.f64 * f0.f64;
	// fctid f13,f11
	ctx.f13.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvtsd_si64(_mm_load_sd(&ctx.f11.f64));
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fmul f13,f13,f31
	ctx.f13.f64 = ctx.f13.f64 * f31.f64;
	// fsub f11,f30,f13
	ctx.f11.f64 = f30.f64 - ctx.f13.f64;
	// fmadd f11,f11,f12,f10
	ctx.f11.f64 = ctx.f11.f64 * ctx.f12.f64 + ctx.f10.f64;
	// fmul f10,f11,f0
	ctx.f10.f64 = ctx.f11.f64 * f0.f64;
	// fctid f10,f10
	ctx.f10.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvtsd_si64(_mm_load_sd(&ctx.f10.f64));
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// fmul f10,f10,f31
	ctx.f10.f64 = ctx.f10.f64 * f31.f64;
	// fmadd f13,f13,f12,f10
	ctx.f13.f64 = ctx.f13.f64 * ctx.f12.f64 + ctx.f10.f64;
	// fsub f11,f11,f10
	ctx.f11.f64 = ctx.f11.f64 - ctx.f10.f64;
	// fmul f12,f13,f0
	ctx.f12.f64 = ctx.f13.f64 * f0.f64;
	// fctid f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvtsd_si64(_mm_load_sd(&ctx.f12.f64));
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// fmul f12,f12,f31
	ctx.f12.f64 = ctx.f12.f64 * f31.f64;
	// fsub f13,f13,f12
	ctx.f13.f64 = ctx.f13.f64 - ctx.f12.f64;
	// fadd f13,f13,f11
	ctx.f13.f64 = ctx.f13.f64 + ctx.f11.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * f0.f64;
	// fctid f11,f11
	ctx.f11.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvtsd_si64(_mm_load_sd(&ctx.f11.f64));
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fmul f11,f11,f31
	ctx.f11.f64 = ctx.f11.f64 * f31.f64;
	// fadd f12,f12,f11
	ctx.f12.f64 = ctx.f12.f64 + ctx.f11.f64;
	// fsub f13,f13,f11
	ctx.f13.f64 = ctx.f13.f64 - ctx.f11.f64;
	// fmul f0,f12,f0
	f0.f64 = ctx.f12.f64 * f0.f64;
	// lfd f12,320(r31)
	ctx.f12.u64 = PPC_LOAD_U64(r31.u32 + 320);
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// bgt cr6,0x823f0228
	if (cr6.getGT()) goto loc_823F0228;
	// lfd f12,328(r31)
	ctx.f12.u64 = PPC_LOAD_U64(r31.u32 + 328);
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// blt cr6,0x823f0240
	if (cr6.getLT()) goto loc_823F0240;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// fctiwz f0,f0
	f0.s64 = (f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&f0.f64));
	// fcmpu cr6,f13,f27
	cr6.compare(ctx.f13.f64, f27.f64);
	// stfiwx f0,0,r11
	PPC_STORE_U32(r11.u32, f0.u32);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// ble cr6,0x823f04d0
	if (!cr6.getGT()) goto loc_823F04D0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// fsub f13,f13,f31
	ctx.f13.f64 = ctx.f13.f64 - f31.f64;
loc_823F04D0:
	// lfd f0,312(r31)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r31.u32 + 312);
	// cntlzw r10,r11
	ctx.r10.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lfd f12,304(r31)
	ctx.f12.u64 = PPC_LOAD_U64(r31.u32 + 304);
	// srawi r9,r11,4
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0xF) != 0);
	ctx.r9.s64 = r11.s32 >> 4;
	// fmadd f12,f13,f0,f12
	ctx.f12.f64 = ctx.f13.f64 * f0.f64 + ctx.f12.f64;
	// lfd f0,296(r31)
	f0.u64 = PPC_LOAD_U64(r31.u32 + 296);
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// add r30,r10,r9
	r30.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r31,8
	ctx.r10.s64 = r31.s64 + 8;
	// rlwinm r9,r30,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// fmadd f12,f12,f13,f0
	ctx.f12.f64 = ctx.f12.f64 * ctx.f13.f64 + f0.f64;
	// lfd f0,288(r31)
	f0.u64 = PPC_LOAD_U64(r31.u32 + 288);
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// fmadd f12,f12,f13,f0
	ctx.f12.f64 = ctx.f12.f64 * ctx.f13.f64 + f0.f64;
	// lfd f0,280(r31)
	f0.u64 = PPC_LOAD_U64(r31.u32 + 280);
	// fmadd f12,f12,f13,f0
	ctx.f12.f64 = ctx.f12.f64 * ctx.f13.f64 + f0.f64;
	// lfd f0,272(r31)
	f0.u64 = PPC_LOAD_U64(r31.u32 + 272);
	// fmadd f12,f12,f13,f0
	ctx.f12.f64 = ctx.f12.f64 * ctx.f13.f64 + f0.f64;
	// lfd f0,264(r31)
	f0.u64 = PPC_LOAD_U64(r31.u32 + 264);
	// fmadd f0,f12,f13,f0
	f0.f64 = ctx.f12.f64 * ctx.f13.f64 + f0.f64;
	// lfdx f12,r11,r10
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + ctx.r10.u32);
	// fmadd f0,f0,f13,f26
	f0.f64 = f0.f64 * ctx.f13.f64 + f26.f64;
	// fmul f31,f0,f12
	f31.f64 = f0.f64 * ctx.f12.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x823f7730
	sub_823F7730(ctx, base);
	// add r4,r3,r30
	ctx.r4.u64 = ctx.r3.u64 + r30.u64;
loc_823F0548:
	// cmpwi cr6,r4,1024
	cr6.compare<int32_t>(ctx.r4.s32, 1024, xer);
	// bgt cr6,0x823f0228
	if (cr6.getGT()) goto loc_823F0228;
	// cmpwi cr6,r4,-1021
	cr6.compare<int32_t>(ctx.r4.s32, -1021, xer);
	// blt cr6,0x823f0240
	if (cr6.getLT()) goto loc_823F0240;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x823f7708
	sub_823F7708(ctx, base);
	// fmul f1,f1,f25
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f1.f64 * f25.f64;
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F0568:
	// rlwinm r10,r11,0,17,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7FF8;
	// cmplwi cr6,r10,32752
	cr6.compare<uint32_t>(ctx.r10.u32, 32752, xer);
	// bne cr6,0x823f058c
	if (!cr6.getEQ()) goto loc_823F058C;
	// lwz r11,192(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// clrlwi. r11,r11,13
	r11.u64 = r11.u32 & 0x7FFFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x823f05d8
	if (!cr0.getEQ()) goto loc_823F05D8;
	// lwz r11,196(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x823f05d8
	if (!cr6.getEQ()) goto loc_823F05D8;
loc_823F058C:
	// rlwinm r11,r9,0,17,28
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x7FF8;
	// cmplwi cr6,r11,32752
	cr6.compare<uint32_t>(r11.u32, 32752, xer);
	// bne cr6,0x823f05b0
	if (!cr6.getEQ()) goto loc_823F05B0;
	// lwz r9,200(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// clrlwi. r9,r9,13
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFF;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x823f05d8
	if (!cr0.getEQ()) goto loc_823F05D8;
	// lwz r9,204(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x823f05d8
	if (!cr6.getEQ()) goto loc_823F05D8;
loc_823F05B0:
	// cmplwi cr6,r10,32760
	cr6.compare<uint32_t>(ctx.r10.u32, 32760, xer);
	// beq cr6,0x823f05d8
	if (cr6.getEQ()) goto loc_823F05D8;
	// cmplwi cr6,r11,32760
	cr6.compare<uint32_t>(r11.u32, 32760, xer);
	// beq cr6,0x823f05d8
	if (cr6.getEQ()) goto loc_823F05D8;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// fmr f2,f30
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f30.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x823eff78
	sub_823EFF78(ctx, base);
	// lfd f1,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x823f05dc
	goto loc_823F05DC;
loc_823F05D8:
	// fadd f1,f28,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f28.f64 + f30.f64;
loc_823F05DC:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// addi r12,r1,-24
	r12.s64 = ctx.r1.s64 + -24;
	// bl 0x823ed588
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

